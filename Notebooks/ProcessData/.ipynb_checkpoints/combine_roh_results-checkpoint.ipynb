{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process ROH Results into one big dataframe\n",
    "Contains cleaning lines (i.e. to remove duplicates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "midway2-0401.rcc.local\n",
      "Midway jnovmbre partition detected.\n",
      "/project2/jnovembre/hringbauer/HAPSBURG\n",
      "CPU Count: 28\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os  # For Saving to Folder\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.colorbar as clb\n",
    "import matplotlib.colors as cls\n",
    "from matplotlib import gridspec\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from mpl_toolkits.basemap import Basemap\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF, WhiteKernel\n",
    "\n",
    "import socket\n",
    "import os as os\n",
    "import sys as sys\n",
    "import multiprocessing as mp\n",
    "\n",
    "socket_name = socket.gethostname()\n",
    "print(socket_name)\n",
    "if socket_name == \"VioletQueen\":\n",
    "    path = \"/home/harald/git/HAPSBURG/\"   # The Path on Harald's machine\n",
    "elif socket_name.startswith(\"midway2\"):\n",
    "    print(\"Midway jnovmbre partition detected.\")\n",
    "    path = \"/project2/jnovembre/hringbauer/HAPSBURG/\"  # The Path on Midway Cluster\n",
    "else: \n",
    "    raise RuntimeWarning(\"Not compatible machine. Check!!\")\n",
    "    \n",
    "os.chdir(path)  # Set the right Path (in line with Atom default)\n",
    "print(os.getcwd()) # Show the current working directory. Should be HAPSBURG/Notebooks/ParallelRuns\n",
    "print(f\"CPU Count: {mp.cpu_count()}\")\n",
    "\n",
    "### Additional Imports from Support Packages\n",
    "sys.path.append(\"./PackagesSupport/\")\n",
    "from pp_individual_roh_csvs import extract_sub_df_geo_kw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions that pre-process Data\n",
    "Add \"region\" Field. Add \"color\" (based on Time) field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process_roman_df(df, age_error=0, remove_sard=False):\n",
    "    \"\"\"Preprocess and return roman df and adds colors\"\"\"\n",
    "    color_dict = {\"Medieval/EarlyModern\":\"yellow\", \"Imperial\":\"red\", \"Iron/Republic\":\"magenta\", \n",
    "                  \"LateAntiquity\":\"orange\", \"Copper Age\":\"aquamarine\", \"Neolithic\":\"dodgerblue\", \n",
    "                  \"Mesolithic\":\"purple\", \"(not included in analyses)\":\"gray\"}\n",
    "    df[\"color\"] = df[\"clst\"].map(color_dict)\n",
    "    if age_error>0:\n",
    "        df[\"age\"]+= np.random.random(len(df))*age_error - age_error/2\n",
    "    \n",
    "    df[\"region\"]=\"Rome\" \n",
    "    ### Modify Sardinians\n",
    "    idx_sar = (df[\"clst\"] == \"(not included in analyses)\")\n",
    "    df.loc[idx_sar,\"region\"] = \"Sardinia\"\n",
    "    return df\n",
    "\n",
    "def pre_process_iberia_df(df, age_error=0):\n",
    "    \"\"\"Preprocess and return roman df and adds colors\"\"\"\n",
    "    df[\"color\"]=\"silver\"\n",
    "\n",
    "    ### WHG Coloring\n",
    "    hg_terms = [\"HG\", \"Meso\", \"ElMiron\"]\n",
    "    idx = df[\"clst\"].str.contains('|'.join(hg_terms))\n",
    "    df.loc[idx, \"color\"]=\"purple\"\n",
    "    df.loc[idx, \"clst\"]=\"Mesolithic\"\n",
    "    \n",
    "    ### EN Coloring\n",
    "    en_terms = [\"Iberia_EN\"]\n",
    "    idx = df[\"clst\"].str.contains('|'.join(en_terms))\n",
    "    df.loc[idx,\"color\"]=\"blue\"\n",
    "    df.loc[idx,\"clst\"]=\"Early Neolithic\"\n",
    "    \n",
    "    ### Middle Late Neoltihic\n",
    "    mn_terms = [\"MN\", \"MLN\", \"MN\", \"LN\"]\n",
    "    idx = df[\"clst\"].str.contains('|'.join(mn_terms))\n",
    "    df.loc[idx,\"color\"]=\"lightblue\"\n",
    "    df.loc[idx,\"clst\"]=\"Middle/Late Neolithic\"\n",
    "    \n",
    "    ### Muslim Burials\n",
    "    en_terms = [\"SE_Iberia_c.10-16CE\"]\n",
    "    idx = df[\"clst\"].str.contains('|'.join(en_terms))\n",
    "    df.loc[idx,\"color\"]=\"red\"\n",
    "    df.loc[idx,\"clst\"]=\"Muslim Period\"\n",
    "        \n",
    "    if age_error>0:\n",
    "        df[\"age\"]+= np.random.random(len(df)) * age_error - age_error/2      \n",
    "    return df\n",
    "\n",
    "def pre_process_reich_df(df, age_error=0, del_strings=[]):\n",
    "    \"\"\"Preprocess and return roman df and adds colors.\n",
    "    del_strings: iid column in df that contains this list of strings\n",
    "    gets deleted\"\"\"\n",
    "    ### Fix Geography\n",
    "    df.loc[df[\"iid\"]==\"I7554\", \"lon\"] = -3.249  # Flip Wrong Latitude Atlantic\n",
    "    df.loc[df[\"iid\"]==\"Aconcagua.SG\", \"lat\"] = -32.65  # Flip Wrong Latitude (32.64 is in Atlantic)\n",
    "    \n",
    "    ### Delete individuals\n",
    "    for ds in del_strings:\n",
    "        df = df[~df[\"iid\"].str.contains(ds)]\n",
    "    \n",
    "    ### WHG Coloring\n",
    "    hg_terms = [\"HG\", \"Meso\", \"ElMiron\", \"Iron Gates\", \"Loschbour\"]\n",
    "    idx = ((df[\"clst\"].str.contains('|'.join(hg_terms))) | (df[\"age\"]>10500)) & (df[\"age\"]>5000)\n",
    "    df.loc[idx,\"color\"]=\"purple\"\n",
    "    df.loc[idx,\"clst\"]=\"Mesolithic\"\n",
    "    \n",
    "    ### EN Coloring\n",
    "    en_terms = [\"EN\", \"Early Neol\", \"Neolithic\", \"Cardial\", \"MN\", \"LN\", \"MLN\", \"Ukraine_N\", \"Peloponnese_N\"]\n",
    "    idx = df[\"clst\"].str.contains('|'.join(en_terms)) & (df[\"age\"]>5500)\n",
    "    df.loc[idx,\"color\"] = \"aqua\"\n",
    "    df.loc[idx,\"clst\"] = \"Neolithic\"\n",
    "    \n",
    "    ### Antatolia Farmers\n",
    "    en_terms = [\"Anatolia_N\", \"Anatolia Farmers\"]\n",
    "    idx = df[\"clst\"].str.contains('|'.join(en_terms))\n",
    "    df.loc[idx,\"color\"]=\"blue\"\n",
    "    df.loc[idx,\"clst\"]=\"Anatolia Farmers\"\n",
    "    \n",
    "    en_terms = [\"Canaanite\"]\n",
    "    idx = df[\"clst\"].str.contains('|'.join(en_terms))\n",
    "    df.loc[idx,\"color\"]=\"red\"\n",
    "    df.loc[idx,\"clst\"]=\"Canaanite\"\n",
    "    \n",
    "    en_terms = [\"Sar-Nur\"]\n",
    "    idx = df[\"clst\"].str.contains('|'.join(en_terms))\n",
    "    df.loc[idx,\"color\"]=\"red\"\n",
    "    df.loc[idx,\"clst\"]=\"Nuragic\"\n",
    "    \n",
    "    en_terms = [\"skythian\", \"Skythian\"]\n",
    "    idx = df[\"clst\"].str.contains('|'.join(en_terms))\n",
    "    df.loc[idx,\"color\"]=\"orange\"\n",
    "    df.loc[idx,\"clst\"]=\"Skythian\"\n",
    "    \n",
    "    if age_error>0:\n",
    "        df[\"age\"]+= np.random.random(len(df)) * age_error - age_error/2\n",
    "    return df\n",
    "\n",
    "############################################################################\n",
    "### Post-Process Regions\n",
    "\n",
    "def set_regions_from_csv(csv_path, df, output=True):\n",
    "    \"\"\"Set Region coumn in df, by loading coordinates from csv_path\"\"\"\n",
    "    df_regions= pd.read_csv(csv_path, sep='\\t')\n",
    "    for index, row in df_regions.iterrows():\n",
    "        kw = row[\"Keywords\"].split(\"|\") # produce list from Keywords\n",
    "        df_t = extract_sub_df_geo_kw(df_all, row[\"Lat_low\"], row[\"Lat_high\"], row[\"Lon_low\"], \n",
    "                                     row[\"Lon_high\"], kw, output=output)\n",
    "        idx = df[\"iid\"].isin(df_t[\"iid\"]) # Get Indices of Sub Dataframe\n",
    "        df.loc[idx,\"region\"] = row[\"Region\"] \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Goal:\n",
    "Have one Master Dataframe. With Region field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Roman Dataframe\n",
    "df_rome = pd.read_csv(\"./Empirical/1240k/Antonio/combined_roh05.csv\", sep=\"\\t\")\n",
    "df_rome = pre_process_roman_df(df_rome, age_error=0, remove_sard=False)\n",
    "df_rome.drop(columns='age_range', inplace=True)\n",
    "cols = df_rome.columns # Remember the Column Names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Reich Dataframe\n",
    "# Define Individuals we want to delete (Duplicates/Neanderthals)\n",
    "del_strings = [\"Loschbour_snpAD.DG\", \"Mezmaiskaya\", \"Ishim_published.DG\", \"Vindija_snpAD\", \n",
    "               \"Kostenki14.SG\", \"Goyet\", \"Spy\", \"Denisova\", \"Altai\", \"Les_Cottes\", \"Anzick.SG\",\n",
    "               \"Russia_Karelia_HG.SG\", \"I0001\", \"I2966_all\", \"I5259_all\", \"I4450_all\",\n",
    "               \"I4105_all\", \"I4106_all\", \"I3921_all\"]\n",
    "df_r = pd.read_csv(\"./Empirical/Eigenstrat/Reichall/combined_roh05.csv\", sep=\"\\t\")\n",
    "df_r = pre_process_iberia_df(df_r, age_error=0)\n",
    "df_r = pre_process_reich_df(df_r, del_strings=del_strings)\n",
    "df_r['region'] = \"all\"   ### Modify this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Sardinians from Marcus et all\n",
    "df_sard = pd.read_csv(\"./Empirical/1240k/MarcusAncs/combined_roh05.csv\", sep=\"\\t\")\n",
    "df_sard = pre_process_reich_df(df_sard)\n",
    "df_sard[\"region\"]=\"Sardinia\"\n",
    "df_sard = df_sard[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Iberia from Olalde19\n",
    "df_ib = pd.read_csv(\"./Empirical/Eigenstrat/Olalde19/combined_roh05.csv\", sep=\"\\t\")\n",
    "df_ib = pre_process_iberia_df(df_ib, age_error=0)\n",
    "df_ib[\"region\"]=\"Iberia\"\n",
    "df_ib.drop(columns='age_range', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Human Origin Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concatenate all the Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = pd.concat([df_rome, df_r, df_sard, df_ib])\n",
    "df_all = pre_process_reich_df(df_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set the Regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 136 Individuals; 120 from Geography\n",
      "Found 162 Individuals; 105 from Geography\n"
     ]
    }
   ],
   "source": [
    "csv_path = \"./Data/RegionDefinition/regions.csv\"\n",
    "df_t=set_regions_from_csv(csv_path, df_all)\n",
    "df_t = pre_process_iberia_df(df_t, age_error=0)  # Hack for having Iberians right"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the Summary Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 1321 Individual ROH to: ./Empirical/roh_all_inds.csv\n"
     ]
    }
   ],
   "source": [
    "savepath=\"./Empirical/roh_all_inds.csv\"\n",
    "if len(savepath)>0:\n",
    "    df_all.to_csv(savepath, sep=\"\\t\")\n",
    "    print(f\"Saved {len(df_all)} Individual ROH to: {savepath}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Area 51"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_region = pd.read_csv(csv_path, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Region</th>\n",
       "      <th>Lat_low</th>\n",
       "      <th>Lat_high</th>\n",
       "      <th>Lon_low</th>\n",
       "      <th>Lon_high</th>\n",
       "      <th>Keywords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Iberia</td>\n",
       "      <td>35.95</td>\n",
       "      <td>44.0</td>\n",
       "      <td>-10.00</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Iberia|Portugal|Spain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Balkans</td>\n",
       "      <td>42.20</td>\n",
       "      <td>46.9</td>\n",
       "      <td>13.05</td>\n",
       "      <td>23.9</td>\n",
       "      <td>Balkans|Serbia|Hungary</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Region  Lat_low  Lat_high  Lon_low  Lon_high                Keywords\n",
       "0   Iberia    35.95      44.0   -10.00       4.0   Iberia|Portugal|Spain\n",
       "1  Balkans    42.20      46.9    13.05      23.9  Balkans|Serbia|Hungary"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_r[df_r.duplicated(subset=[\"lat\", \"lon\", \"age\"], keep=False)].sort_values(by=\"age\", ascending=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
