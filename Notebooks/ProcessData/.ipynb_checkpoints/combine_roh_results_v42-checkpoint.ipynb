{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process ROH Results into one big dataframe\n",
    "Contains cleaning lines (i.e. to remove duplicates), fix flipped coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "midway2-0401.rcc.local\n",
      "Midway jnovmbre partition detected.\n",
      "/project2/jnovembre/hringbauer/HAPSBURG\n",
      "CPU Count: 28\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os  # For Saving to Folder\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.colorbar as clb\n",
    "import matplotlib.colors as cls\n",
    "from matplotlib import gridspec\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from mpl_toolkits.basemap import Basemap\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF, WhiteKernel\n",
    "\n",
    "import socket\n",
    "import os as os\n",
    "import sys as sys\n",
    "import multiprocessing as mp\n",
    "\n",
    "socket_name = socket.gethostname()\n",
    "print(socket_name)\n",
    "if socket_name == \"VioletQueen\":\n",
    "    path = \"/home/harald/git/HAPSBURG/\"   # The Path on Harald's machine\n",
    "elif socket_name.startswith(\"midway2\"):\n",
    "    print(\"Midway jnovmbre partition detected.\")\n",
    "    path = \"/project2/jnovembre/hringbauer/HAPSBURG/\"  # The Path on Midway Cluster\n",
    "else: \n",
    "    raise RuntimeWarning(\"Not compatible machine. Check!!\")\n",
    "    \n",
    "os.chdir(path)  # Set the right Path (in line with Atom default)\n",
    "print(os.getcwd()) # Show the current working directory. Should be HAPSBURG/Notebooks/ParallelRuns\n",
    "print(f\"CPU Count: {mp.cpu_count()}\")\n",
    "\n",
    "### Additional Imports from Support Packages\n",
    "sys.path.append(\"./package/hapsburg/\")\n",
    "from PackagesSupport.pp_individual_roh_csvs import extract_sub_df_geo_kw, give_df_clsts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions that pre-process Data\n",
    "Add \"region\" Field. Add \"color\" (based on Time) field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################################\n",
    "### Post-Process Regions\n",
    "def set_regions_from_csv(df, csv_path = \"./Data/RegionDefinition/regions.csv\", \n",
    "                         output=True, sep=\",\"):\n",
    "    \"\"\"Set Region column in df, by loading coordinates from csv_path\"\"\"\n",
    "    df_regions= pd.read_csv(csv_path, sep=sep)\n",
    "    for index, row in df_regions.iterrows():\n",
    "        region = row[\"Region\"] \n",
    "        if output:\n",
    "            print(f\"Doing {region}...\")\n",
    "        kw = str(row[\"Keywords\"]).split(\"|\") # produce list from Keywords\n",
    "        df_t = extract_sub_df_geo_kw(df, row[\"Lat_low\"], row[\"Lat_high\"], row[\"Lon_low\"], \n",
    "                                     row[\"Lon_high\"], kw, output=output)\n",
    "        idx = df[\"iid\"].isin(df_t[\"iid\"]) # Get Indices of Sub Dataframe\n",
    "        df.loc[idx, \"region\"] = row[\"Region\"] \n",
    "    return df\n",
    "\n",
    "############################################################################\n",
    "### Post-Process Colors\n",
    "def set_colors_from_csv(df, csv_path = \"./Data/RegionDefinition/colors.csv\", \n",
    "                         output=True, sep=\",\"):\n",
    "    \"\"\"Set Color column in df, by loading colors from csv_path\"\"\"\n",
    "    df_colors= pd.read_csv(csv_path, sep=sep)\n",
    "    for index, row in df_colors.iterrows():\n",
    "        color = row[\"Color\"] \n",
    "        ig = row[\"InternalGroup\"]\n",
    "        kw = str(row[\"Keywords\"]).split(\"|\") # produce list from Keywords\n",
    "        df_t = give_df_clsts(df, search=kw, col=\"pop\")\n",
    "        idx = df[\"iid\"].isin(df_t[\"iid\"]) # Get Indices of Sub Dataframe\n",
    "        df.loc[idx, \"color\"] = color\n",
    "        df.loc[idx, \"clst\"] = row[\"clst\"]\n",
    "        \n",
    "        if output:\n",
    "            print(f\"Doing {ig}...\")\n",
    "            print(f\"Found {np.sum(idx)} Inds - set to color: {color}\")\n",
    "        \n",
    "    ### Do old HunterGatherers\n",
    "    return df\n",
    "\n",
    "def set_color_hg_minage(df, color=\"blue\", min_age=10500, output=True):\n",
    "    \"\"\"Set the color for all ancient Huntergatherers.\"\"\"\n",
    "    idx = df[\"age\"] > min_age\n",
    "    df.loc[idx, \"color\"] = color\n",
    "    if output:\n",
    "        print(f\"Found {np.sum(idx)} Inds >{min_age} BP - set to color: {color}\")\n",
    "    return df\n",
    "    \n",
    "def set_color_modern(df, color=\"white\", output=True):\n",
    "    \"\"\"Set color for all Modern Samples\"\"\"\n",
    "    idx = df[\"age\"] == 0\n",
    "    df.loc[idx, \"color\"] = color\n",
    "    df.loc[idx, \"clst\"] = \"Modern\"\n",
    "    if output:\n",
    "        print(f\"Found {np.sum(idx)} Moderns - set to color: {color}\")\n",
    "    return df\n",
    "\n",
    "def remove_ids(df, csv_path = \"./Data/RegionDefinition/remove_ids.csv\", output=True, del_col=\"iid\"):\n",
    "    \"\"\"Remove Individuals whose del_col column contains\n",
    "    string from del_strings (list)\"\"\"\n",
    "    del_list = np.loadtxt(csv_path, dtype=\"str\")\n",
    "    \n",
    "    n=len(df)\n",
    "    for ds in del_list:\n",
    "        df = df[~df[del_col].str.contains(ds)]\n",
    "    if output:\n",
    "        print(f\"Removed {n-len(df)} / {n} Individuals in Deletion List.\")\n",
    "    return df\n",
    "\n",
    "def remove_duplicates(df, cov_col=\"n_cov_snp\", id_col=\"iid\", master_col = \"Master ID\",\n",
    "                      path_master=\"./Data/ReichLabEigenstrat/Raw.v42.4/v42.4.1240K.anno\",\n",
    "                      output=True):\n",
    "    \"\"\"Remove duplicates based on merging with Master Dataframe.\n",
    "    Return Filtered Dataframe\n",
    "    id_col: Column onto which to merge\n",
    "    \"\"\"\n",
    "    n = len(df)\n",
    "    df_meta = pd.read_csv(path_master, sep=\"\\t\")\n",
    "    df_meta[id_col] = df_meta.filter(regex='Instance ID')\n",
    "    \n",
    "    df_meta = df_meta[[id_col, master_col]]  # Only relevant columns\n",
    "    df_merge = pd.merge(df, df_meta, on=id_col, how=\"left\")  # Merge on IID\n",
    "    df_merge = df_merge.sort_values(by=cov_col, ascending=False) # Put IIDs with most SNPs first\n",
    "    ### Fill up NaNs with IDs\n",
    "    idx = df_merge[master_col].isnull()\n",
    "    df_merge.loc[idx, master_col] = df_merge.loc[idx, id_col]\n",
    "    df_merge = df_merge.drop_duplicates(subset=master_col, keep=\"first\")\n",
    "    \n",
    "    df_merge = df_merge.drop(columns=master_col)  #Drop the Master ID Col again\n",
    "\n",
    "    if output:\n",
    "        print(f\"Removed {n- len(df_merge)} / {n} Duplicates\")\n",
    "    return df_merge\n",
    "\n",
    "############################################################################\n",
    "############################################################################\n",
    "\n",
    "def merge_in_economy_iid(df, path_economy=\"\", \n",
    "                         economy_col=\"economy\",\n",
    "                         match_col = \"iid\", \n",
    "                         case=False):\n",
    "    \"\"\"Create/Set Column economy_col into dataframe df. Check for substring matches (to be future proof)\n",
    "    Return modified dataframe.\n",
    "    match_col: What columns to match\n",
    "    economy_col: What column to transfer over\n",
    "    case: Whether IID substring matching is case sensitive\"\"\"\n",
    "    df_match = pd.read_csv(path_economy)  # Load the data\n",
    "    \n",
    "    if not economy_col in df.columns:\n",
    "        df[economy_col] = np.nan\n",
    "    \n",
    "    ### Match all IIDs\n",
    "    for i,m in enumerate(df_match[match_col]):\n",
    "        m = m.rstrip()  # Remove all right whitespaces\n",
    "        idx = df[match_col].str.contains(m, case=case)\n",
    "        df.loc[idx, economy_col] = df_match.loc[i,economy_col]\n",
    "    return df\n",
    "\n",
    "def set_economy_color(df, path_color_df=\"./Data/RegionDefinition/economy_colors.csv\", \n",
    "                      color_col=\"color\", economy_col=\"economy\"):\n",
    "    \"\"\"Set Color Based on Economy.\n",
    "    Assume color column in df exists\"\"\"\n",
    "    df_c = pd.read_csv(path_color_df)\n",
    "    dct = dict(zip(df_c[economy_col], df_c[color_col]))  # Create mapping dictionary\n",
    "    df[color_col] = df[economy_col].map(dct).fillna(df[color_col])  # Only Map hits\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load all varying Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded Reich Data: 1923\n",
      "Loaded Sardinian Data: 40\n",
      "Loaded modern Data: 1941 Individuals\n",
      "Concatenated 3904 Individual ROH Data!\n",
      "Filtered to 3811 Individuals with include_alt>0\n"
     ]
    }
   ],
   "source": [
    "### Reich Dataframe\n",
    "# Define Individuals we want to delete (Duplicates/Neanderthals)\n",
    "df_r = pd.read_csv(\"./Empirical/Eigenstrat/Reichall/combined_roh_v42.csv\", sep=\"\\t\")\n",
    "df_r['region'] = \"all\"   # Place Holder\n",
    "print(f\"Loaded Reich Data: {len(df_r)}\")\n",
    "cols = df_r.columns # Extract key column names in right order\n",
    "\n",
    "### Sardinians from Marcus et all\n",
    "df_sard = pd.read_csv(\"./Empirical/1240k/MarcusAncs/combined_roh05.csv\", sep=\"\\t\")\n",
    "df_sard = df_sard[df_sard[\"pop\"].str.contains(\"Sar-\")]  #Extract Sardinia Data\n",
    "df_sard[\"region\"]=\"Sardinia\"\n",
    "df_sard = df_sard[cols]\n",
    "print(f\"Loaded Sardinian Data: {len(df_sard)}\")\n",
    "\n",
    "### Human Origin Data\n",
    "df_ho = pd.read_csv(\"./Empirical/HO/combined_roh05.csv\", sep=\"\\t\")\n",
    "df_ho[\"region\"] = df_ho[\"pop\"] # Will be later overwritten for Macro Region!\n",
    "df_ho[\"color\"] = \"gray\"\n",
    "df_ho = df_ho[cols]\n",
    "print(f\"Loaded modern Data: {len(df_ho)} Individuals\")\n",
    "\n",
    "### Concatenate the Dataframes\n",
    "df_all = pd.concat([df_r, df_sard, df_ho])\n",
    "print(f\"Concatenated {len(df_all)} Individual ROH Data!\")\n",
    "\n",
    "### Filter to good individuals\n",
    "df_all =df_all[df_all[\"include_alt\"]>0] \n",
    "print(f\"Filtered to {len(df_all)} Individuals with include_alt>0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#idx = df_all[\"clst\"].str.contains(\"India\")\n",
    "#df_all[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove Individuals in Deletion List and also Duplicates \n",
    "(based on master ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed 21 / 3811 Individuals in Deletion List.\n",
      "Removed 58 / 3790 Duplicates\n"
     ]
    }
   ],
   "source": [
    "csv_path = \"./Data/RegionDefinition/remove_ids.csv\"\n",
    "df_all = remove_ids(df_all, csv_path)\n",
    "df_all = remove_duplicates(df_all, \n",
    "                           path_master=\"./Data/ReichLabEigenstrat/Raw.v42.4/v42.4.1240K.anno\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge in Coordinats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fill in missing coordinates from outside source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doing Iberia...\n",
      "Found 230 Individuals; 193 from Geography\n",
      "Doing Balkans...\n",
      "Found 159 Individuals; 111 from Geography\n",
      "Doing Aegan...\n",
      "Found 112 Individuals; 105 from Geography\n",
      "Doing Central Europe...\n",
      "Found 171 Individuals; 171 from Geography\n",
      "Doing Black Sea...\n",
      "Found 45 Individuals; 45 from Geography\n",
      "Doing North Africa...\n",
      "Found 56 Individuals; 55 from Geography\n",
      "Doing Britain...\n",
      "Found 166 Individuals; 162 from Geography\n",
      "Doing Baltic Sea...\n",
      "Found 103 Individuals; 103 from Geography\n",
      "Doing Sardinia...\n",
      "Found 76 Individuals; 76 from Geography\n",
      "Doing Levante...\n",
      "Found 185 Individuals; 184 from Geography\n",
      "Doing Vanuatu...\n",
      "Found 17 Individuals; 17 from Geography\n",
      "Doing Steppe...\n",
      "Found 586 Individuals; 586 from Geography\n",
      "Doing Patagonia...\n",
      "Found 10 Individuals; 10 from Geography\n",
      "Doing Andean...\n",
      "Found 39 Individuals; 39 from Geography\n",
      "Doing Pacific NW...\n",
      "Found 29 Individuals; 29 from Geography\n",
      "Doing Atlantic Coast...\n",
      "Found 21 Individuals; 21 from Geography\n",
      "Doing Rome...\n",
      "Found 135 Individuals; 135 from Geography\n",
      "Doing Vanuatu...\n",
      "Found 16 Individuals; 16 from Geography\n",
      "Doing East Africa...\n",
      "Found 61 Individuals; 61 from Geography\n",
      "Doing South Africa...\n",
      "Found 8 Individuals; 3 from Geography\n",
      "Doing East Steppe...\n",
      "Found 55 Individuals; 42 from Geography\n",
      "Doing Pakistan...\n",
      "Found 276 Individuals; 165 from Geography\n"
     ]
    }
   ],
   "source": [
    "df_geo = pd.read_csv(\"./Data/Coordinates/MittnikNatComm2018_Coordinates.csv\", sep=\"\\t\")\n",
    "df_geo.index = df_geo[\"iid\"]\n",
    "df_all.index = df_all[\"iid\"]\n",
    "df_all.update(df_geo)\n",
    "\n",
    "### Set the regions from .csv\n",
    "csv_path = \"./Data/RegionDefinition/regions.csv\"\n",
    "df_t = set_regions_from_csv(df_all, csv_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set the colors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set the Economies (Mode of Food Production)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1934 Moderns - set to color: yellow\n"
     ]
    }
   ],
   "source": [
    "df_t[\"color\"]= \"silver\" # Make Tabula Rasa\n",
    "csv_path = \"./Data/RegionDefinition/colors.csv\"\n",
    "#df_t = set_colors_from_csv(df_t, csv_path)\n",
    "#df_t = set_color_hg_minage(df_t, color=\"purple\")\n",
    "df_t = set_color_modern(df_t, color=\"yellow\")\n",
    "\n",
    "### Set it based on Food Economy\n",
    "df_t = merge_in_economy_iid(df_t, path_economy=\"./Data/RegionDefinition/economy_clst.csv\", match_col='clst')   # Do the Individual Matches (overwriting)\n",
    "df_t = merge_in_economy_iid(df_t, path_economy=\"./Data/RegionDefinition/economy_iid.csv\", match_col='iid')   # Do the Individual Matches (overwriting)\n",
    "df_t = set_economy_color(df_t, path_color_df=\"./Data/RegionDefinition/economy_colors.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the Summary Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 3732 Individual ROH to: ./Empirical/roh_all_inds_final_v42.csv\n"
     ]
    }
   ],
   "source": [
    "savepath=\"./Empirical/roh_all_inds_final_v42.csv\"\n",
    "if len(savepath)>0:\n",
    "    df_t.to_csv(savepath, sep=\"\\t\", index=False)\n",
    "    print(f\"Saved {len(df_all)} Individual ROH to: {savepath}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Area 51"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ho = pd.read_csv(\"./Empirical/HO/CombinedROH/combined_roh05.csv\", sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'AA',\n",
       " 'Abkhasian',\n",
       " 'Adygei',\n",
       " 'Albanian',\n",
       " 'Aleut',\n",
       " 'Algerian',\n",
       " 'Altaian',\n",
       " 'Ami',\n",
       " 'Armenian',\n",
       " 'Ashkenazi_Jew',\n",
       " 'Atayal',\n",
       " 'Australian',\n",
       " 'Balkar',\n",
       " 'Balochi',\n",
       " 'BantuKenya',\n",
       " 'BantuSA',\n",
       " 'Basque',\n",
       " 'BedouinA',\n",
       " 'BedouinB',\n",
       " 'Belarusian',\n",
       " 'Bengali',\n",
       " 'Bergamo',\n",
       " 'Biaka',\n",
       " 'Bolivian',\n",
       " 'Bougainville',\n",
       " 'Brahui',\n",
       " 'Bulgarian',\n",
       " 'Burusho',\n",
       " 'Cambodian',\n",
       " 'Canary_Islanders',\n",
       " 'Chechen',\n",
       " 'Chukchi',\n",
       " 'Chuvash',\n",
       " 'Cochin_Jew',\n",
       " 'Croatian',\n",
       " 'Cypriot',\n",
       " 'Czech',\n",
       " 'Dai',\n",
       " 'Datog',\n",
       " 'Daur',\n",
       " 'Dolgan',\n",
       " 'Druze',\n",
       " 'Egyptian',\n",
       " 'English',\n",
       " 'Esan',\n",
       " 'Eskimo',\n",
       " 'Estonian',\n",
       " 'Ethiopian_Jew',\n",
       " 'Even',\n",
       " 'Finnish',\n",
       " 'French',\n",
       " 'French_South',\n",
       " 'Gambian',\n",
       " 'Georgian',\n",
       " 'Georgian_Jew',\n",
       " 'Greek',\n",
       " 'GujaratiA',\n",
       " 'GujaratiB',\n",
       " 'GujaratiC',\n",
       " 'GujaratiD',\n",
       " 'Hadza',\n",
       " 'Han',\n",
       " 'Han_NChina',\n",
       " 'Hazara',\n",
       " 'Hezhen',\n",
       " 'Hungarian',\n",
       " 'Icelandic',\n",
       " 'Iranian',\n",
       " 'Iranian_Jew',\n",
       " 'Iraqi_Jew',\n",
       " 'Italian_South',\n",
       " 'Itelmen',\n",
       " 'Japanese',\n",
       " 'Jordanian',\n",
       " 'Ju_hoan_North',\n",
       " 'Kalash',\n",
       " 'Kalmyk',\n",
       " 'Karitiana',\n",
       " 'Khomani',\n",
       " 'Kikuyu',\n",
       " 'Kinh',\n",
       " 'Korean',\n",
       " 'Koryak',\n",
       " 'Kumyk',\n",
       " 'Kusunda',\n",
       " 'Kyrgyz',\n",
       " 'Lahu',\n",
       " 'Lebanese',\n",
       " 'Lezgin',\n",
       " 'Libyan_Jew',\n",
       " 'Lithuanian',\n",
       " 'Luhya',\n",
       " 'Luo',\n",
       " 'Makrani',\n",
       " 'Maltese',\n",
       " 'Mandenka',\n",
       " 'Mansi',\n",
       " 'Masai',\n",
       " 'Mayan',\n",
       " 'Mbuti',\n",
       " 'Mende',\n",
       " 'Miao',\n",
       " 'Mixe',\n",
       " 'Mixtec',\n",
       " 'Mongola',\n",
       " 'Mordovian',\n",
       " 'Moroccan_Jew',\n",
       " 'Mozabite',\n",
       " 'Naxi',\n",
       " 'Nganasan',\n",
       " 'Nogai',\n",
       " 'North_Ossetian',\n",
       " 'Norwegian',\n",
       " 'Orcadian',\n",
       " 'Oroqen',\n",
       " 'Palestinian',\n",
       " 'Papuan',\n",
       " 'Pathan',\n",
       " 'Piapoco',\n",
       " 'Pima',\n",
       " 'Punjabi',\n",
       " 'Quechua',\n",
       " 'Russian',\n",
       " 'Saami_WGA',\n",
       " 'Saharawi',\n",
       " 'Sardinian',\n",
       " 'Saudi',\n",
       " 'Scottish',\n",
       " 'Selkup',\n",
       " 'She',\n",
       " 'Sicilian',\n",
       " 'Sindhi',\n",
       " 'Somali',\n",
       " 'Spanish',\n",
       " 'Spanish_North',\n",
       " 'Surui',\n",
       " 'Syrian',\n",
       " 'Tajik_Pomiri',\n",
       " 'Thai',\n",
       " 'Tlingit',\n",
       " 'Tu',\n",
       " 'Tubalar',\n",
       " 'Tujia',\n",
       " 'Tunisian',\n",
       " 'Tunisian_Jew',\n",
       " 'Turkish',\n",
       " 'Turkish_Jew',\n",
       " 'Turkmen',\n",
       " 'Tuscan',\n",
       " 'Tuvinian',\n",
       " 'Ukrainian',\n",
       " 'Ulchi',\n",
       " 'Uygur',\n",
       " 'Uzbek',\n",
       " 'Xibo',\n",
       " 'Yakut',\n",
       " 'Yemen',\n",
       " 'Yemenite_Jew',\n",
       " 'Yi',\n",
       " 'Yoruba',\n",
       " 'Yukagir',\n",
       " 'Zapotec'}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(df_ho[\"pop\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>iid</th>\n",
       "      <th>pop</th>\n",
       "      <th>max_roh</th>\n",
       "      <th>sum_roh&gt;4</th>\n",
       "      <th>n_roh&gt;4</th>\n",
       "      <th>sum_roh&gt;8</th>\n",
       "      <th>n_roh&gt;8</th>\n",
       "      <th>sum_roh&gt;12</th>\n",
       "      <th>n_roh&gt;12</th>\n",
       "      <th>sum_roh&gt;20</th>\n",
       "      <th>...</th>\n",
       "      <th>study</th>\n",
       "      <th>clst_alt</th>\n",
       "      <th>period_alt</th>\n",
       "      <th>include_alt</th>\n",
       "      <th>clst</th>\n",
       "      <th>mean_cov</th>\n",
       "      <th>med_cov</th>\n",
       "      <th>n_cov_snp_read</th>\n",
       "      <th>full_iid</th>\n",
       "      <th>n_cov_snp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>Hadza_3</td>\n",
       "      <td>Hadza</td>\n",
       "      <td>16.240204</td>\n",
       "      <td>264.950700</td>\n",
       "      <td>31</td>\n",
       "      <td>187.566915</td>\n",
       "      <td>17</td>\n",
       "      <td>68.000203</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>Lazaridis et al. 2014</td>\n",
       "      <td>Hadza</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>Hadza</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>End08</td>\n",
       "      <td>549841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203</th>\n",
       "      <td>Hadza_2</td>\n",
       "      <td>Hadza</td>\n",
       "      <td>18.397104</td>\n",
       "      <td>120.681022</td>\n",
       "      <td>18</td>\n",
       "      <td>38.452709</td>\n",
       "      <td>3</td>\n",
       "      <td>18.397104</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>Lazaridis et al. 2014</td>\n",
       "      <td>Hadza</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>Hadza</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Bar13</td>\n",
       "      <td>540831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>355</th>\n",
       "      <td>Hadza_4</td>\n",
       "      <td>Hadza</td>\n",
       "      <td>12.667095</td>\n",
       "      <td>66.326388</td>\n",
       "      <td>10</td>\n",
       "      <td>29.600291</td>\n",
       "      <td>3</td>\n",
       "      <td>12.667095</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>Lazaridis et al. 2014</td>\n",
       "      <td>Hadza</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>Hadza</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Bar04</td>\n",
       "      <td>545031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>743</th>\n",
       "      <td>Hadza_0</td>\n",
       "      <td>Hadza</td>\n",
       "      <td>8.227205</td>\n",
       "      <td>17.779204</td>\n",
       "      <td>3</td>\n",
       "      <td>8.227205</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>Lazaridis et al. 2014</td>\n",
       "      <td>Hadza</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>Hadza</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Bar08</td>\n",
       "      <td>543902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>802</th>\n",
       "      <td>Hadza_1</td>\n",
       "      <td>Hadza</td>\n",
       "      <td>5.499005</td>\n",
       "      <td>14.886808</td>\n",
       "      <td>3</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>Lazaridis et al. 2014</td>\n",
       "      <td>Hadza</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>Hadza</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Bar10</td>\n",
       "      <td>543967</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         iid    pop    max_roh   sum_roh>4  n_roh>4   sum_roh>8  n_roh>8  \\\n",
       "64   Hadza_3  Hadza  16.240204  264.950700       31  187.566915       17   \n",
       "203  Hadza_2  Hadza  18.397104  120.681022       18   38.452709        3   \n",
       "355  Hadza_4  Hadza  12.667095   66.326388       10   29.600291        3   \n",
       "743  Hadza_0  Hadza   8.227205   17.779204        3    8.227205        1   \n",
       "802  Hadza_1  Hadza   5.499005   14.886808        3    0.000000        0   \n",
       "\n",
       "     sum_roh>12  n_roh>12  sum_roh>20    ...                      study  \\\n",
       "64    68.000203         5         0.0    ...      Lazaridis et al. 2014   \n",
       "203   18.397104         1         0.0    ...      Lazaridis et al. 2014   \n",
       "355   12.667095         1         0.0    ...      Lazaridis et al. 2014   \n",
       "743    0.000000         0         0.0    ...      Lazaridis et al. 2014   \n",
       "802    0.000000         0         0.0    ...      Lazaridis et al. 2014   \n",
       "\n",
       "    clst_alt  period_alt  include_alt   clst  mean_cov  med_cov  \\\n",
       "64     Hadza         NaN            1  Hadza       NaN      NaN   \n",
       "203    Hadza         NaN            1  Hadza       NaN      NaN   \n",
       "355    Hadza         NaN            1  Hadza       NaN      NaN   \n",
       "743    Hadza         NaN            1  Hadza       NaN      NaN   \n",
       "802    Hadza         NaN            1  Hadza       NaN      NaN   \n",
       "\n",
       "    n_cov_snp_read full_iid  n_cov_snp  \n",
       "64             NaN    End08     549841  \n",
       "203            NaN    Bar13     540831  \n",
       "355            NaN    Bar04     545031  \n",
       "743            NaN    Bar08     543902  \n",
       "802            NaN    Bar10     543967  \n",
       "\n",
       "[5 rows x 27 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ho[df_ho[\"pop\"].str.contains(\"Hadza\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Hadza, Khomani, Ju_Huan_North"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
