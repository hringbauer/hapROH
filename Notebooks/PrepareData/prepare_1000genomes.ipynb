{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook to Prepare 1000 Genomes Data\n",
    "This notebooks prepares the downsampled HDF5 for 1240k Data.\n",
    "Requires bcftools binary.\n",
    "Atm, only runs on Harald's local machine (where BCFtools is installed)\n",
    "Runtime on one CPU: Ca. 15 Min for Chr. 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "midway2-0401.rcc.local\n",
      "Midway jnovmbre partition detected.\n",
      "/project/jnovembre/hringbauer/HAPSBURG\n",
      "CPU Count: 28\n"
     ]
    }
   ],
   "source": [
    "import allel\n",
    "import h5py  # Python Package to do the HDF5.\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import socket\n",
    "import os as os\n",
    "import sys as sys\n",
    "import multiprocessing as mp\n",
    "\n",
    "socket_name = socket.gethostname()\n",
    "print(socket_name)\n",
    "if socket_name == \"VioletQueen\":\n",
    "    path = \"/home/harald/git/HAPSBURG/\"   # The Path on Harald's machine\n",
    "elif socket_name.startswith(\"midway2\"):\n",
    "    print(\"Midway jnovmbre partition detected.\")\n",
    "    path = \"/project/jnovembre/hringbauer/HAPSBURG/\"  # The Path on Midway Cluster\n",
    "else: \n",
    "    raise RuntimeWarning(\"Not compatible machine. Check!!\")\n",
    "    \n",
    "os.chdir(path)  # Set the right Path (in line with Atom default)\n",
    "\n",
    "sys.path.append(\"./Python3/\")  # Since now we are in the Root Directory\n",
    "from hmm_inference import HMM_Analyze   # Do not move. Should be after sys.path..\n",
    "#sys.path.append(\"./Python3/create1000G_Mosaic/\")  # Since now we are in the Root Directory\n",
    "#from createMosaicsMulti import Mosaic_1000G_Multi  # Import the object that can create the Multiruns\n",
    "\n",
    "print(os.getcwd()) # Show the current working directory. Should be HAPSBURG/Notebooks/ParallelRuns\n",
    "print(f\"CPU Count: {mp.cpu_count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#path_vcf = \"../Data/1000Genomes/ALL.chrX.phase3_shapeit2_mvncall_integrated_v1b.20130502.genotypes.vcf.gz\" # Path of VCF\n",
    "path_vcf = \"./Data/1000Genomes/1000gX1240kEur.vcf\"\n",
    "#h5_path = \"../Data/1000Genomes/1000Genomes_X.hdf5\"\n",
    "h5_path = \"./Data/1000Genomes/1000gX1240kEur.hdf5\"\n",
    "ind_path = \"./Data/1000Genomes/integrated_call_samples_v2.20130502.ALL.ped\"  # Family Relationships\n",
    "pop_path = \"./Data/1000Genomes/integrated_call_samples_v3.20130502.ALL.panel\" # Population Information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do the Conversion to HDF 5 \n",
    "Comment out / one can skip down for standalone extraction of autosomes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Do the conversion to hdf5 (if not done already)\n",
    "# geno = allel.read_vcf(path_vcf) # Load the VCF # Load the VCF # Needs too much Memory for my laptop\n",
    "#allel.vcf_to_hdf5(input=path_vcf, output=h5_path, compression=\"gzip\") # Do the conversion to hdf5. Takes 10 Minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 47297 variants\n",
      "Loaded 503 individuals\n",
      "['GT']\n",
      "['ALT', 'CHROM', 'FILTER_PASS', 'ID', 'POS', 'QUAL', 'REF']\n"
     ]
    }
   ],
   "source": [
    "## Load HDF5\n",
    "f = h5py.File(h5_path, \"r\") # Load for Sanity Check. See below!\n",
    "print(\"Loaded %i variants\" % np.shape(f[\"calldata/GT\"])[0])\n",
    "print(\"Loaded %i individuals\" % np.shape(f[\"calldata/GT\"])[1])\n",
    "print(list(f[\"calldata\"].keys()))\n",
    "print(list(f[\"variants\"].keys()))\n",
    "#print(list(f[\"samples\"].keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Load the ID of the Individuals\n",
    "ids = np.array(f[\"variants/ID\"])\n",
    "df_s_empirical = pd.DataFrame({'Individual ID' : list(f[\"samples\"])})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_s_empirical[\"Individual ID\"] = df_s_empirical[\"Individual ID\"].str.split(\"_\").str[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and Merge Individual Meta Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 3691 Individuals\n",
      "Loaded 3691 Population Data\n",
      "Merged from 503 to 503 individuals\n"
     ]
    }
   ],
   "source": [
    "df_i = pd.read_csv(ind_path, sep=\"\\t\")\n",
    "print(f\"Loaded {np.shape(df_i)[0]} Individuals\")\n",
    "\n",
    "df_pops = pd.read_csv(pop_path, sep=\"\\t\")\n",
    "print(f\"Loaded {np.shape(df_i)[0]} Population Data\")\n",
    "\n",
    "### Merge with IDs in Genotype File\n",
    "df = pd.merge(df_s_empirical, df_i, on='Individual ID', how='inner')\n",
    "df = pd.merge(df, df_pops, left_on=\"Individual ID\", right_on=\"sample\", how=\"inner\")\n",
    "print(f\"Merged from {len(df_s_empirical)} to {len(df)} individuals\")\n",
    "assert(len(df_s_empirical) == len(df))  # Sanity Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TSI    107\n",
       "IBS    107\n",
       "CEU     99\n",
       "FIN     99\n",
       "GBR     91\n",
       "Name: Population, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"Population\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Table with individual IDs [European, needed later for bcftools]\n",
    "Save table with Individual and Family IDs of all European 1000 Genome Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = \"./Data/1000Genomes/EUR.csv\"\n",
    "\n",
    "eur_inds = df[\"super_pop\"]==\"EUR\"\n",
    "print(f\"Nr of European Samples: {np.sum(eur_inds)}\")\n",
    "\n",
    "df_save = df[eur_inds]\n",
    "df_save = df_save[[\"Family ID\", \"Individual ID\"]] # Extract column of Individual and Family ID\n",
    "df_save.to_csv(save_path, sep=\"\\t\", header=None, index=False)\n",
    "print(f\"Saved to {save_path}. Nr Individuals: {len(df_save)}\")\n",
    "\n",
    "### Do the same, but for only individual ID\n",
    "save_path = \"./Data/1000Genomes/EUR_fam.csv\"\n",
    "\n",
    "eur_inds = df[\"super_pop\"]==\"EUR\"\n",
    "print(f\"Nr of European Samples: {np.sum(eur_inds)}\")\n",
    "\n",
    "df_save = df[eur_inds]   # Extract the European Individuals\n",
    "df_save = df_save[\"Individual ID\"] # Extract column of Individual ID\n",
    "df_save.to_csv(save_path, sep=\"\\t\", header=None, index=False)\n",
    "print(f\"Saved to {save_path}. Nr Individuals: {len(df_save)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check against Sardinian X data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py:2230: FutureWarning: split() requires a non-empty pattern match.\n",
      "  yield pat.split(line.strip())\n",
      "/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py:2232: FutureWarning: split() requires a non-empty pattern match.\n",
      "  yield pat.split(line.strip())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 49711 X SNPs.\n"
     ]
    }
   ],
   "source": [
    "path_snp = \"../ancient-sardinia/data/bed/full230.snp\" # All SNPs found in the 1240k Ancient Panel\n",
    "\n",
    "df_snp = pd.read_csv(path_snp, header=None, sep=r\"\\s*\", engine=\"python\")\n",
    "df_snp.columns = [\"SNP\", \"chr\", \"map\", \"pos\", \"ref\", \"alt\"]\n",
    "df_snp = df_snp[df_snp[\"chr\"]==23]\n",
    "\n",
    "print(f\"Loaded {len(df_snp)} X SNPs.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save  interesection with 1240k Marker \n",
    "Prepare txt List for Plink filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-bdc23b4e82f4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0msave_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"./Data/1000Genomes/variants1240k\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mfound\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"variants/POS\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_snp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"pos\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Intersection: {np.sum(found)} out of {len(found)} SNPS\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mvariants\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"variants/ID\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfound\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "save_path = \"./Data/1000Genomes/variants1240k\"\n",
    "\n",
    "found = np.isin(f[\"variants/POS\"], df_snp[\"pos\"])\n",
    "print(f\"Intersection: {np.sum(found)} out of {len(found)} SNPS\")\n",
    "variants = f[\"variants/ID\"][found]\n",
    "\n",
    "dots = np.where(variants == \".\")[0]\n",
    "print(f\"Found {len(dots)} unnamed SNPs\")\n",
    "variants = np.delete(variants, dots)\n",
    "\n",
    "np.savetxt(save_path, variants, fmt=\"%s\")\n",
    "print(f\"Successfully saved to {save_path}. Length: {len(variants)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare downsampled Autosomal hdf5s\n",
    "Prepare a 1000 Genome autosomal hdf5 file. Include Recombination Map\n",
    "Input: 1000 Genome vcf file, Recombination Map from a 1240k Eigenstrat\n",
    "## Standalone from here onward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Important Parameters and paths\n",
    "ch = 3 # Which Chromosome to use:\n",
    "\n",
    "# Path of the 1000 Genome VCF:\n",
    "p1, p2 = \"\", \"\"\n",
    "file_vcf100g, path_vcf100g = \"\", \"\"\n",
    "out_vcf_path0, out_vcf_path = \"\", \"\"\n",
    "path_hdf5temp, path_hdf5final = \"\", \"\"\n",
    "\n",
    "snp1240k_path, ind_path = \"\", \"\"   # Where to find the 1240k SNPs\n",
    "snp_filter_path = \"\"\n",
    "\n",
    "def prepare_paths(ch = 3):\n",
    "    \"\"\"Prepares all the Paths need for processing Steps.\n",
    "    ch: Which Chromosomes to use\"\"\"\n",
    "    global p1, p2, file_vcf100g, path_vcf100g, out_vcf_path0, out_vcf_path, path_hdf5temp, path_hdf5final\n",
    "    global snp1240k_path, ind_path, snp_filter_path, out_vcf_path_gz, marker_path\n",
    "    # Path of the 1000 Genome VCF:\n",
    "    p1 = \"./Data/1000Genomes/AutosomeVCF/\"\n",
    "    p2 = \".phase3_shapeit2_mvncall_integrated_v5a.20130502.genotypes.vcf.gz\"\n",
    "    file_vcf100g = \"ALL.chr\" + str(ch) + p2\n",
    "    path_vcf100g = p1 + file_vcf100g\n",
    "    print(f\"Full Input path:\\n{path_vcf100g}\")\n",
    "    out_vcf_path0 = \"./Data/1000Genomes/AutosomeVCF/Subset/\" + \"1240EURchr\" + str(ch) # needs no .vcf\n",
    "    out_vcf_path = out_vcf_path0 + \".vcf\"\n",
    "    out_vcf_path_gz = out_vcf_path + \".gz\"\n",
    "    path_hdf5temp = \"./Data/1000Genomes/HDF5/FULLHDF5/cr\" + str(ch) + \".hdf5\"\n",
    "    path_hdf5final = \"./Data/1000Genomes/HDF5/1240kHDF5/Eur1240chr\" + str(ch) + \".hdf5\"\n",
    "    snp1240k_path = \"./Data/1000Genomes/Markers/MinMyc.snp\"   # Where to find the 1240k SNPs\n",
    "    ind_path = \"./Data/1000Genomes/Individuals/EUR_fam.csv\"   # Where to find the individual lists\n",
    "    marker_path = \"./Data/1000Genomes/Markers/1240k/chr\" + str(ch) + \".csv\"\n",
    "\n",
    "    # Path of SNP Filter\n",
    "    snp_filter_path = \"./Data/1000Genomes/Markers/variants1240k\" + str(ch) + \".txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 0: Download the Data\n",
    "### Step 1: Produce hdf5 file for all markers\n",
    "### Step 2: Extract Positions. Match with Eigenstrat File Positions\n",
    "### Step 3: Create new vcf based on subset of Individuals and Markers\n",
    "### Step 4: Transfer to hdf5. \n",
    "### Step 5: Merge in Linkage Map\n",
    "### Step 6: Quality Check? (Control ref/alt against hdf5 we have for Sardinians)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Step 0: Download the Data\n",
    "def download_1kg(cluster=False):\n",
    "    \"\"\"cluster: Whether program is run on cluster\"\"\"\n",
    "    if cluster==False:\n",
    "        path_cl = \"/project/jnovembre/data/external_public/1kg_phase3/haps/\"\n",
    "        path_cluster = \"hringbauer@midway.rcc.uchicago.edu:\" + path_cl + file_vcf100g\n",
    "    \n",
    "    elif cluster==True:\n",
    "        path_cl = \"/project/jnovembre/data/external_public/1kg_phase3/haps/\"\n",
    "        path_cluster = path_cl + file_vcf100g\n",
    "    \n",
    "    p_c = path_cluster + \".tbi\"\n",
    "    p_v = path_vcf100g + \".tbi\"\n",
    "    #!scp $p_c $p_v # Download the tbi\n",
    "    !scp $path_cluster $path_vcf100g # Only Download the .vcf (not the .tbi)\n",
    "    \n",
    "### Step 1: Produce hdf5 file for all markers\n",
    "def vcf_to_hdf5(in_path, out_path):\n",
    "    \"\"\"Transform Full VCF to full HDF5\"\"\"\n",
    "    allel.vcf_to_hdf5(input=in_path, output=out_path, compression=\"gzip\") # Takes 10 Minutes\n",
    "    \n",
    "### Step 2: Extract Positions. Match with Eigenstrat File Positions\n",
    "### Load HDF5\n",
    "\n",
    "def merge_positions():\n",
    "    \"\"\"Creates the Filter File to filter SNPs for\"\"\"\n",
    "    f_full = h5py.File(path_hdf5temp, \"r\") # Load for Sanity Check. See below!\n",
    "    print(\"Loaded %i variants\" % np.shape(f_full[\"calldata/GT\"])[0])\n",
    "    print(\"Loaded %i individuals\" % np.shape(f_full[\"calldata/GT\"])[1])\n",
    "    print(list(f_full[\"calldata\"].keys()))\n",
    "    print(list(f_full[\"variants\"].keys()))\n",
    "    #print(list(f[\"samples\"].keys()))\n",
    "\n",
    "    ### Load Eigenstrat\n",
    "    df_snp = pd.read_csv(snp1240k_path, header=None, sep=r\"\\s+\", engine=\"python\")\n",
    "    df_snp.columns = [\"SNP\", \"chr\", \"map\", \"pos\", \"ref\", \"alt\"]\n",
    "    df_snp = df_snp[df_snp[\"chr\"] == ch]\n",
    "    print(f\"Loaded {len(df_snp)} Chr.{ch} SNPs.\")\n",
    "\n",
    "    ### Prepare SNP File for Eigenstrat filtering \n",
    "    found = np.isin(f_full[\"variants/POS\"], df_snp[\"pos\"])\n",
    "    print(f\"Intersection: {np.sum(found)} out of {len(found)} SNPS\")\n",
    "    variants = f_full[\"variants/ID\"][found]\n",
    "\n",
    "    dots = np.where(variants == \".\")[0]\n",
    "    print(f\"Found {len(dots)} unnamed SNPs\")\n",
    "    variants = np.delete(variants, dots)\n",
    "\n",
    "    np.savetxt(snp_filter_path, variants, fmt=\"%s\")\n",
    "    print(f\"Successfully saved to {snp_filter_path}. Length: {len(variants)}\")\n",
    "    \n",
    "def save_1240kmarkers():\n",
    "    \"\"\"Save all 1240 Markers in csv\"\"\"\n",
    "    df_snp = pd.read_csv(snp1240k_path, header=None, sep=r\"\\s+\", engine=\"python\")\n",
    "    df_snp.columns = [\"SNP\", \"chr\", \"map\", \"pos\", \"ref\", \"alt\"]\n",
    "    df_snp = df_snp[df_snp[\"chr\"] == ch]\n",
    "    print(f\"Loaded {len(df_snp)} Chr.{ch} SNPs.\")\n",
    "\n",
    "    df_save = df_snp[[\"chr\", \"pos\"]]\n",
    "    df_save.to_csv(marker_path, sep=\"\\t\", header=None, index=False)\n",
    "    print(f\"Saved {len(df_save)} 1240k Markers on Chr. {ch} to {marker_path}\")\n",
    "    \n",
    "### Step 3: Create new vcf based on subset of Individuals and Markers\n",
    "def plink_new_vcf():\n",
    "    !plink --vcf $path_vcf100g --extract $snp_filter_path --keep-fam $ind_path --recode vcf --out $out_vcf_path0 --biallelic-only strict --keep-allele-order\n",
    "\n",
    "### Step 3b\n",
    "def bctools_new_vcf0():\n",
    "    \"\"\"Same as PLINK, but with bcftools \n",
    "    [small hack with marker strings, so LEGACY code and replaced by bcftools_new_vcf]\"\"\"\n",
    "    str_ex = \"ID=@\" + snp_filter_path\n",
    "    #!echo bcftools view -Oz -o $out_vcf_path_gz -S $ind_path -i $str_ex -m2 -M2 -v snps $path_vcf100g\n",
    "    !bcftools view -Oz -o $out_vcf_path_gz -S $ind_path -i $str_ex -m2 -M2 -v snps $path_vcf100g\n",
    "    print(\"Finished BCF tools runs.\")\n",
    "    \n",
    "def bctools_new_vcf(filter_iids=True, cluster=False):\n",
    "    \"\"\"Same as PLINK, but with bcftools and directly via Marker Positions.\n",
    "    filter_iids: Whether to use the .csv with Indivdiduals to extract\"\"\"\n",
    "    if filter_iids==True:\n",
    "        if cluster==False:\n",
    "            !bcftools view -Oz -o $out_vcf_path_gz -S $ind_path -T $marker_path -m2 -M2 -v snps $path_vcf100g\n",
    "        elif cluster==True:\n",
    "            !module load bcftools; bcftools view -Oz -o $out_vcf_path_gz -S $ind_path -T $marker_path -m2 -M2 -v snps $path_vcf100g     \n",
    "    elif filter_iids==False:\n",
    "        if cluster==False:\n",
    "            !bcftools view -Oz -o $out_vcf_path_gz -T $marker_path -m2 -M2 -v snps $path_vcf100g\n",
    "        elif cluster==True:\n",
    "            !module load bcftools; bcftools view -Oz -o $out_vcf_path_gz -T $marker_path -m2 -M2 -v snps $path_vcf100g\n",
    "    print(\"Finished BCF tools runs.\")\n",
    "\n",
    "### Step 4: Transfer to hdf5.\n",
    "#allel.vcf_to_hdf5(input=out_vcf_path, output=path_hdf5final, compression=\"gzip\") # Takes 1s\n",
    " \n",
    "### Step 5: Merge in Linkage Map\n",
    "### Load HDF5\n",
    "def merge_in_ld_map():\n",
    "    \"\"\"Merge in ld_map into HDF5!\"\"\"\n",
    "    f = h5py.File(path_hdf5final, \"r\") # Load for Sanity Check. See below!\n",
    "    print(\"Merging in LD Map into HDF5...\")\n",
    "    print(\"Loaded %i variants\" % np.shape(f[\"calldata/GT\"])[0])\n",
    "    print(\"Loaded %i individuals\" % np.shape(f[\"calldata/GT\"])[1])\n",
    "    print(list(f[\"calldata\"].keys()))\n",
    "    print(list(f[\"variants\"].keys()))\n",
    "    #print(list(f[\"samples\"].keys()))\n",
    "\n",
    "    ### Load Eigenstrat\n",
    "    df_snp = pd.read_csv(snp1240k_path, header=None, sep=r\"\\s+\", engine=\"python\")\n",
    "    df_snp.columns = [\"SNP\", \"chr\", \"map\", \"pos\", \"ref\", \"alt\"]\n",
    "    df_snp = df_snp[df_snp[\"chr\"] == ch]\n",
    "    print(f\"Loaded {len(df_snp)} Chr.{ch} SNPs.\")\n",
    "\n",
    "    ### Intersect SNP positions\n",
    "    its, i1, i2 = np.intersect1d(f[\"variants/POS\"], df_snp[\"pos\"], return_indices=True)\n",
    "\n",
    "    l = len(f[\"variants/POS\"])\n",
    "    print(f\"Intersection {len(i2)} out of {l}\")\n",
    "\n",
    "    ### Extract \n",
    "    rec = np.zeros(len(f[\"variants/POS\"]))\n",
    "    rec[i1] = df_snp[\"map\"].values[i2]  # Fill in the values in Recombination map\n",
    "\n",
    "    ids0 = np.where(rec == 0)[0] # The 0 Values\n",
    "    rec[ids0] = (rec[ids0-1] + rec[ids0+1]) / 2.0 # Interpolate\n",
    "\n",
    "    ### Make sure that sorted\n",
    "    assert(np.all(np.diff(rec)>=0))  # Assert the Recombination Map is sorted! (no 0 left and no funky stuff)\n",
    "\n",
    "    f.close()\n",
    "    with h5py.File(path_hdf5final, 'a') as f0:\n",
    "        group = f0[\"variants\"]\n",
    "        group.create_dataset('MAP', (l,), dtype='f')   \n",
    "        f0[\"variants/MAP\"][:] = rec[:]\n",
    "\n",
    "    print(f\"Finished Chromosome {ch}\")\n",
    "    \n",
    "### Step 6: Delete the Data:\n",
    "def del_temp_data():\n",
    "    !rm $path_vcf100g # Delete the full 1000 genome .vcf\n",
    "    !rm $out_vcf_path_gz # Delete the extracted .vcf\n",
    "    #!rm $path_hdf5temp # The originally intermediate hdf5 (for 1240k intersection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Do all steps in one run (function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_1000genomes_full(ch):\n",
    "    \"\"\"ch: Which Chromosome to prepare\"\"\"\n",
    "    prepare_paths(ch = ch)\n",
    "    download_1kg(cluster=False)\n",
    "    print(\"Download Complete\")\n",
    "    #vcf_to_hdf5(in_path=path_vcf100g, out_path=path_hdf5temp) # Takes 10 Minutes\n",
    "    #print(\"Transformation to HDF5 Complete.\")\n",
    "    #merge_positions()\n",
    "    save_1240kmarkers()\n",
    "    #plink_new_vcf()\n",
    "    bctools_new_vcf()\n",
    "    vcf_to_hdf5(in_path=out_vcf_path_gz, out_path=path_hdf5final)\n",
    "    merge_in_ld_map()\n",
    "    #del_temp_data()\n",
    "    print(f\"Finished Preparing HDF5 Chromosome {ch}. GZ!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loop to run all chromosomes\n",
    "\n",
    "for ch in range(1, 23):\n",
    "    print(f\"Preparing Chromosome: {ch}\")\n",
    "    prep_1000genomes_full(ch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare 1240k SNP HDF5 for CHB, CLM, YRI.\n",
    "(Needs functions from downsample autosomal hdf5s but otherwise stand-alone)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the Indivdual .csv for bcftools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_iid_csv(pops_oi=[], save_path = \"../Data/1000Genomes/Individuals/CHB_CLM_YRI_fam.csv\", \n",
    "                   pop_path=\"../Data/1000Genomes/integrated_call_samples_v3.20130502.ALL.panel\", \n",
    "                    output=True):\n",
    "    \"\"\"Prepare .csv with Individuals.\n",
    "    pops_oi: List of Population IIDs to extract. If empty, extract all\n",
    "    save_path: Where to save the .csv File to\n",
    "    pop_path: Input .csv with the Population information\n",
    "    output: Whether to print output\n",
    "    \"\"\"\n",
    "    df_pops = pd.read_csv(pop_path, sep=\"\\t\")\n",
    "    inds = df_pops[\"pop\"].isin(pops_oi)\n",
    "    \n",
    "    if output == True:\n",
    "        print(f\"Loaded {np.shape(df_pops)[0]} Individuals with Population Data\")\n",
    "        print(f\"Found {np.sum(inds)} Individuals from target populations\")\n",
    "    \n",
    "    df_save = df_pops[inds]   # Filter to target Individual rows\n",
    "    df_save = df_save[\"sample\"] # Extract column of IIds\n",
    "    df_save.to_csv(save_path, sep=\"\\t\", header=None, index=False)\n",
    "    \n",
    "    if output == True:\n",
    "        print(f\"Saved to {save_path}. Nr saved IIDs: {len(df_save)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 2504 Individuals with Population Data\n",
      "Found 305 Individuals from target populations\n",
      "Saved to ../Data/1000Genomes/Individuals/CHB_CLM_YRI_fam.csv. Nr saved IIDs: 305\n"
     ]
    }
   ],
   "source": [
    "prepare_iid_csv(pops_oi = [\"CHB\", \"CLM\", \"YRI\"], save_path = \"../Data/1000Genomes/Individuals/CHB_CLM_YRI_fam.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overwrite paths for HDF5 creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Important Parameters and paths\n",
    "ch = 3 # Which Chromosome to use:\n",
    "\n",
    "# Path of the 1000 Genome VCF:\n",
    "p1, p2 = \"\", \"\"\n",
    "file_vcf100g, path_vcf100g = \"\", \"\"\n",
    "out_vcf_path0, out_vcf_path = \"\", \"\"\n",
    "path_hdf5temp, path_hdf5final = \"\", \"\"\n",
    "\n",
    "snp1240k_path, ind_path = \"\", \"\"   # Where to find the 1240k SNPs\n",
    "snp_filter_path = \"\"\n",
    "\n",
    "def prepare_paths(ch = 3):\n",
    "    \"\"\"Prepares all the Paths need for processing Steps.\n",
    "    ch: Which Chromosomes to use\"\"\"\n",
    "    global p1, p2, file_vcf100g, path_vcf100g, out_vcf_path0, out_vcf_path, path_hdf5temp, path_hdf5final\n",
    "    global snp1240k_path, ind_path, snp_filter_path, out_vcf_path_gz, marker_path\n",
    "    # Path of the 1000 Genome VCF:\n",
    "    p1 = \"./Data/1000Genomes/AutosomeVCF/\"\n",
    "    p2 = \".phase3_shapeit2_mvncall_integrated_v5a.20130502.genotypes.vcf.gz\"\n",
    "    file_vcf100g = \"ALL.chr\" + str(ch) + p2\n",
    "    path_vcf100g = p1 + file_vcf100g\n",
    "    print(f\"Full Input path:\\n{path_vcf100g}\")\n",
    "    out_vcf_path0 = \"./Data/1000Genomes/AutosomeVCF/Subset/\" + \"1240NonEURchr\" + str(ch) # needs no .vcf\n",
    "    out_vcf_path = out_vcf_path0 + \".vcf\"\n",
    "    out_vcf_path_gz = out_vcf_path + \".gz\"\n",
    "    path_hdf5temp = \"./Data/1000Genomes/HDF5/FULLHDF5/cr\" + str(ch) + \".hdf5\"\n",
    "    path_hdf5final = \"./Data/1000Genomes/HDF5/1240kHDF5/NonEur1240chr\" + str(ch) + \".hdf5\"\n",
    "    snp1240k_path = \"./Data/1000Genomes/Markers/MinMyc.snp\"   # Where to find the 1240k SNPs\n",
    "    ind_path = \"./Data/1000Genomes/Individuals/CHB_CLM_YRI_fam.csv\"   # Where to find the individual lists\n",
    "    marker_path = \"./Data/1000Genomes/Markers/1240k/chr\" + str(ch) + \".csv\"\n",
    "\n",
    "    ### Path of SNP Filter\n",
    "    snp_filter_path = \"./Data/1000Genomes/Markers/variants1240k\" + str(ch) + \".txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do the Extraction run for the HDF5 \n",
    "(don't forget to have the other functions defined but program would complain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_1000genomes_full(ch):\n",
    "    \"\"\"ch: Which Chromosome to prepare\"\"\"\n",
    "    prepare_paths(ch = ch)\n",
    "    download_1kg()\n",
    "    print(\"Download Complete\")\n",
    "    # Merge not needed here: As interesecting 1240k SNPs are already extracted above\n",
    "    bctools_new_vcf()\n",
    "    vcf_to_hdf5(in_path=out_vcf_path_gz, out_path=path_hdf5final)\n",
    "    merge_in_ld_map()\n",
    "    del_temp_data()\n",
    "    print(\"Finished Preparing HDF5. GZ!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full Input path:\n",
      "../Data/1000Genomes/AutosomeVCF/ALL.chr3.phase3_shapeit2_mvncall_integrated_v5a.20130502.genotypes.vcf.gz\n",
      "ALL.chr3.phase3_shapeit2_mvncall_integrated_v 100% 1055MB  10.9MB/s   01:36    \n",
      "Download Complete\n",
      "Finished BCF tools runs.\n",
      "Loaded 77652 variants\n",
      "Loaded 305 individuals\n",
      "['GT']\n",
      "['ALT', 'CHROM', 'FILTER_PASS', 'ID', 'POS', 'QUAL', 'REF']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py:2230: FutureWarning: split() requires a non-empty pattern match.\n",
      "  yield pat.split(line.strip())\n",
      "/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py:2232: FutureWarning: split() requires a non-empty pattern match.\n",
      "  yield pat.split(line.strip())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 81416 Chr.3 SNPs.\n",
      "Intersection 77652 out of 77652\n",
      "Finished Chromosome 3\n",
      "Finished Preparing HDF5. GZ!\n"
     ]
    }
   ],
   "source": [
    "### Only do Chromosome 3 [suffices for testing]\n",
    "prep_1000genomes_full(ch = 3)   ### Takes about X min"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare 1240k HDF5 with all 1000g reference Indviduals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Important Parameters and paths\n",
    "ch = 3 # Which Chromosome to use:\n",
    "\n",
    "# Path of the 1000 Genome VCF:\n",
    "p1, p2 = \"\", \"\"\n",
    "file_vcf100g, path_vcf100g = \"\", \"\"\n",
    "out_vcf_path0, out_vcf_path = \"\", \"\"\n",
    "path_hdf5temp, path_hdf5final = \"\", \"\"\n",
    "\n",
    "snp1240k_path, ind_path = \"\", \"\"   # Where to find the 1240k SNPs\n",
    "snp_filter_path = \"\"\n",
    "\n",
    "def prepare_paths(ch = 3):\n",
    "    \"\"\"Prepares all the Paths need for processing Steps.\n",
    "    ch: Which Chromosomes to use\"\"\"\n",
    "    global p1, p2, file_vcf100g, path_vcf100g, out_vcf_path0, out_vcf_path, path_hdf5temp, path_hdf5final\n",
    "    global snp1240k_path, ind_path, snp_filter_path, out_vcf_path_gz, marker_path\n",
    "    # Path of the 1000 Genome VCF:\n",
    "    p1 = \"./Data/1000Genomes/AutosomeVCF/\"\n",
    "    p2 = \".phase3_shapeit2_mvncall_integrated_v5a.20130502.genotypes.vcf.gz\"\n",
    "    file_vcf100g = \"ALL.chr\" + str(ch) + p2\n",
    "    path_vcf100g = p1 + file_vcf100g\n",
    "    print(f\"Full Input path:\\n{path_vcf100g}\")\n",
    "    out_vcf_path0 = \"./Data/1000Genomes/AutosomeVCF/Subset/\" + \"1240all/chr\" + str(ch) # needs no .vcf\n",
    "    out_vcf_path = out_vcf_path0 + \".vcf\"\n",
    "    out_vcf_path_gz = out_vcf_path + \".gz\"\n",
    "    path_hdf5temp = \"./Data/1000Genomes/HDF5/FULLHDF5/cr\" + str(ch) + \".hdf5\"\n",
    "    path_hdf5final = \"./Data/1000Genomes/HDF5/1240kHDF5/all1240/chr\" + str(ch) + \".hdf5\"\n",
    "    snp1240k_path = \"./Data/1000Genomes/Markers/MinMyc.snp\"   # Where to find the 1240k SNPs\n",
    "    ind_path = \"./Data/1000Genomes/Individuals/NO_EXIST.csv\"  # non-existing place-holder (sanity check)\n",
    "    marker_path = \"./Data/1000Genomes/Markers/1240k/chr\" + str(ch) + \".csv\"   \n",
    "    \n",
    "    for path in [out_vcf_path, path_hdf5final]:\n",
    "        path_dir = os.path.dirname(path)\n",
    "    \n",
    "        if not os.path.exists(path_dir):\n",
    "            os.makedirs(path_dir)\n",
    "            print(f\"Created new directory: {path_dir}\")\n",
    "    \n",
    "    ### Path of SNP Filter\n",
    "    snp_filter_path = \"../Data/1000Genomes/Markers/variants1240k\" + str(ch) + \".txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_1000genomes_full(ch, cluster):\n",
    "    \"\"\"ch: Which Chromosome to prepare\n",
    "    cluster: Whether Function is run on Cluster\"\"\"\n",
    "    prepare_paths(ch = ch)\n",
    "    download_1kg(cluster=cluster)  # Since we run it on the cluster\n",
    "    print(\"Download Complete\")\n",
    "    ### SNP prep not needed here: Interesecting 1240k SNPs were already extracted above\n",
    "    bctools_new_vcf(filter_iids=False, cluster=cluster)  # Important, turn off filter individuals here!\n",
    "    vcf_to_hdf5(in_path=out_vcf_path_gz, out_path=path_hdf5final)\n",
    "    merge_in_ld_map()\n",
    "    del_temp_data()\n",
    "    print(\"Finished Preparing HDF5. GZ!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full Input path:\n",
      "../Data/1000Genomes/AutosomeVCF/ALL.chr3.phase3_shapeit2_mvncall_integrated_v5a.20130502.genotypes.vcf.gz\n",
      "Created new directory: ../Data/1000Genomes/AutosomeVCF/Subset/1240all\n",
      "Created new directory: ../Data/1000Genomes/HDF5/1240kHDF5/all1240\n",
      "ALL.chr3.phase3_shapeit2_mvncall_integrated_v 100% 1055MB  11.2MB/s   01:34    \n",
      "Download Complete\n",
      "Finished BCF tools runs.\n",
      "Loaded 77652 variants\n",
      "Loaded 2504 individuals\n",
      "['GT']\n",
      "['ALT', 'CHROM', 'FILTER_PASS', 'ID', 'POS', 'QUAL', 'REF']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py:2230: FutureWarning: split() requires a non-empty pattern match.\n",
      "  yield pat.split(line.strip())\n",
      "/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py:2232: FutureWarning: split() requires a non-empty pattern match.\n",
      "  yield pat.split(line.strip())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 81416 Chr.3 SNPs.\n",
      "Intersection 77652 out of 77652\n",
      "Finished Chromosome 3\n",
      "Finished Preparing HDF5. GZ!\n"
     ]
    }
   ],
   "source": [
    "prep_1000genomes_full(ch=3, cluster=True)  ### Test with Chromosome 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full Input path:\n",
      "./Data/1000Genomes/AutosomeVCF/ALL.chr2.phase3_shapeit2_mvncall_integrated_v5a.20130502.genotypes.vcf.gz\n",
      "Download Complete\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for ch in range(1,23):\n",
    "    prep_1000genomes_full(ch, cluster=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare lean Meta File for the new Reference\n",
    "(sample, pop, super_pop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 2504 Rows of Population Data\n",
      "Successfully saved to ../Data/1000Genomes/Individuals/meta_df_all.csv\n"
     ]
    }
   ],
   "source": [
    "pop_path = \"../Data/1000Genomes/integrated_call_samples_v3.20130502.ALL.panel\" # Population Information\n",
    "save_path =\"../Data/1000Genomes/Individuals/meta_df_all.csv\"\n",
    "\n",
    "df_pops = pd.read_csv(pop_path, sep=\"\\t\")\n",
    "print(f\"Loaded {np.shape(df_pops)[0]} Rows of Population Data\")\n",
    "\n",
    "### Merge with IDs in Genotype File\n",
    "df_save = df_pops[[\"sample\", \"pop\", \"super_pop\"]]\n",
    "df_save.to_csv(save_path, index=False, sep=\"\\t\") # Tab seperation!!\n",
    "print(f\"Successfully saved to {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AFR    661\n",
       "EAS    504\n",
       "EUR    503\n",
       "SAS    489\n",
       "AMR    347\n",
       "Name: super_pop, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_save[\"super_pop\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Area51\n",
    "Test Code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Debugging / Trouble Shooting\n",
    "Run specific parts of the pipeline for trouble shooting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_1000genomes_full_error(ch):\n",
    "    \"\"\"ch: Which Chromosome to prepare\"\"\"\n",
    "    prepare_paths(ch = ch)\n",
    "    #download_1kg()\n",
    "    #print(\"Download Complete\")\n",
    "    #vcf_to_hdf5(in_path=path_vcf100g, out_path=path_hdf5temp) # Takes 10 Minutes\n",
    "    #print(\"Transformation to HDF5 Complete.\")\n",
    "    #merge_positions()\n",
    "    #plink_new_vcf()\n",
    "    #bctools_new_vcf()\n",
    "    vcf_to_hdf5(in_path=out_vcf_path_gz, out_path=path_hdf5final)\n",
    "    #merge_in_ld_map()\n",
    "    #del_temp_data()\n",
    "    print(\"Finished Preparing HDF5. GZ!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full Input path:\n",
      "../Data/1000Genomes/AutosomeVCF/ALL.chr21.phase3_shapeit2_mvncall_integrated_v5a.20130502.genotypes.vcf.gz\n",
      "Finished Preparing HDF5. GZ!\n"
     ]
    }
   ],
   "source": [
    "prep_1000genomes_full_error(ch=21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Test that. Possible Solution to warning: s+\n",
    "df_snp = pd.read_csv(snp1240k_path, header=None, sep=r\"\\s*\", engine=\"python\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging in LD Map into HDF5...\n",
      "Loaded 77652 variants\n",
      "Loaded 2504 individuals\n",
      "['GT']\n",
      "['ALT', 'CHROM', 'FILTER_PASS', 'ID', 'MAP', 'POS', 'QUAL', 'REF']\n"
     ]
    }
   ],
   "source": [
    "### Test the Final HDF5 just created\n",
    "f = h5py.File(path_hdf5final, \"r\") # Load for Sanity Check. See below!\n",
    "print(\"Merging in LD Map into HDF5...\")\n",
    "print(\"Loaded %i variants\" % np.shape(f[\"calldata/GT\"])[0])\n",
    "print(\"Loaded %i individuals\" % np.shape(f[\"calldata/GT\"])[1])\n",
    "print(list(f[\"calldata\"].keys()))\n",
    "print(list(f[\"variants\"].keys()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
