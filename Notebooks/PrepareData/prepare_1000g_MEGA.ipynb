{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare MEGA markers for hapROH analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "midway2-0405.rcc.local\n",
      "Midway jnovmbre partition detected.\n",
      "/project2/jnovembre/hringbauer/HAPSBURG\n",
      "CPU Count: 28\n"
     ]
    }
   ],
   "source": [
    "import allel\n",
    "import h5py  # Python Package to do the HDF5.\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import socket\n",
    "import os as os\n",
    "import sys as sys\n",
    "import multiprocessing as mp\n",
    "\n",
    "socket_name = socket.gethostname()\n",
    "print(socket_name)\n",
    "if socket_name == \"VioletQueen\":\n",
    "    path = \"/home/harald/git/HAPSBURG/\"   # The Path on Harald's machine\n",
    "elif socket_name.startswith(\"midway2\"):\n",
    "    print(\"Midway jnovmbre partition detected.\")\n",
    "    path = \"/project2/jnovembre/hringbauer/HAPSBURG/\"  # The Path on Midway Cluster\n",
    "else: \n",
    "    raise RuntimeWarning(\"Not compatible machine. Check!!\")\n",
    "    \n",
    "os.chdir(path)  # Set the right Path (in line with Atom default)\n",
    "sys.path.append(\"./Python3/\")  # Since now we are in the Root Directory\n",
    "\n",
    "from hapsburg.PackagesSupport.h5_python.h5_functions import merge_in_ld_map\n",
    "#from hmm_inference import HMM_Analyze   # Do not move. Should be after sys.path..\n",
    "#sys.path.append(\"./Python3/create1000G_Mosaic/\")  # Since now we are in the Root Directory\n",
    "#from createMosaicsMulti import Mosaic_1000G_Multi  # Import the object that can create the Multiruns\n",
    "\n",
    "print(os.getcwd()) # Show the current working directory. Should be HAPSBURG/Notebooks/ParallelRuns\n",
    "print(f\"CPU Count: {mp.cpu_count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare downsampled Autosomal MEGA hdf5s\n",
    "Prepare a 1000 Genome autosomal hdf5 file. Include Recombination Map   \n",
    "Input: 1000 Genome vcf file, Recombination Map from a 1240k Eigenstrat\n",
    "## Standalone from here onward.\n",
    "### Updated for Mega:\n",
    "prepare_paths function (with Mega paths) and also \n",
    "created save_megamarkers to use specific mega text file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Important Parameters and paths\n",
    "ch = 3 # Which Chromosome to use:\n",
    "\n",
    "# Path of the 1000 Genome VCF:\n",
    "p1, p2 = \"\", \"\"\n",
    "file_vcf100g, path_vcf100g = \"\", \"\"\n",
    "out_vcf_path0, out_vcf_path = \"\", \"\"\n",
    "path_hdf5temp, path_hdf5final = \"\", \"\"\n",
    "\n",
    "snp1240k_path, ind_path = \"\", \"\"   # Where to find the 1240k SNPs\n",
    "snp_filter_path = \"\"\n",
    "\n",
    "def prepare_paths(ch = 3):\n",
    "    \"\"\"Prepares all the Paths need for processing Steps.\n",
    "    ch: Which Chromosomes to use\"\"\"\n",
    "    global p1, p2, file_vcf100g, path_vcf100g, out_vcf_path0, out_vcf_path, path_hdf5temp, path_hdf5final\n",
    "    global snp1240k_path, ind_path, snp_filter_path, out_vcf_path_gz, marker_path\n",
    "    # Path of the 1000 Genome VCF:\n",
    "    p1 = \"./Data/1000Genomes/AutosomeVCF/\"\n",
    "    p2 = \".phase3_shapeit2_mvncall_integrated_v5a.20130502.genotypes.vcf.gz\"\n",
    "    file_vcf100g = \"ALL.chr\" + str(ch) + p2\n",
    "    path_vcf100g = p1 + file_vcf100g\n",
    "    print(f\"Full Input path:\\n{path_vcf100g}\")\n",
    "    out_vcf_path0 = \"./Data/1000Genomes/AutosomeVCF/Subset/\" + \"1240all/chr\" + str(ch) # needs no .vcf\n",
    "    out_vcf_path = out_vcf_path0 + \".vcf\"\n",
    "    out_vcf_path_gz = out_vcf_path + \".gz\"\n",
    "    path_hdf5temp = \"./Data/1000Genomes/HDF5/FULLHDF5/cr\" + str(ch) + \".hdf5\"\n",
    "    path_hdf5final = \"./Data/1000Genomes/HDF5/mega/all/chr\" + str(ch) + \".hdf5\"\n",
    "    snp1240k_path = \"./Data/1000Genomes/Markers/MinMyc.snp\"   # Where to find the 1240k SNPs\n",
    "    ind_path = \"./Data/1000Genomes/Individuals/NO_EXIST.csv\"  # non-existing place-holder (sanity check)\n",
    "    marker_path = \"./Data/1000Genomes/Markers/mega/chr\" + str(ch) + \".csv\"   \n",
    "    \n",
    "    for path in [out_vcf_path, path_hdf5final]:\n",
    "        path_dir = os.path.dirname(path)\n",
    "    \n",
    "        if not os.path.exists(path_dir):\n",
    "            os.makedirs(path_dir)\n",
    "            print(f\"Created new directory: {path_dir}\")\n",
    "    \n",
    "    ### Path of SNP Filter\n",
    "    snp_filter_path = \"../Data/1000Genomes/Markers/variants1240k\" + str(ch) + \".txt\"\n",
    "    \n",
    "### Step 0: Download the Data\n",
    "def download_1kg(cluster=False):\n",
    "    \"\"\"cluster: Whether program is run on cluster\"\"\"\n",
    "    if cluster==False:\n",
    "        path_cl = \"/project2/jnovembre/data/external_public/1kg_phase3/haps/\"\n",
    "        path_cluster = \"hringbauer@midway.rcc.uchicago.edu:\" + path_cl + file_vcf100g\n",
    "    \n",
    "    elif cluster==True:\n",
    "        path_cl = \"/project2/jnovembre/data/external_public/1kg_phase3/haps/\"\n",
    "        path_cluster = path_cl + file_vcf100g\n",
    "    \n",
    "    p_c = path_cluster + \".tbi\"\n",
    "    p_v = path_vcf100g + \".tbi\"\n",
    "    #!scp $p_c $p_v # Download the tbi\n",
    "    !scp $path_cluster $path_vcf100g # Only Download the .vcf (not the .tbi)\n",
    "    \n",
    "### Step 1: Produce hdf5 file for all markers\n",
    "def vcf_to_hdf5(in_path, out_path):\n",
    "    \"\"\"Transform Full VCF to full HDF5\"\"\"\n",
    "    allel.vcf_to_hdf5(input=in_path, output=out_path, compression=\"gzip\") # Takes 10 Minutes\n",
    "    \n",
    "### Step 2: Extract Positions. Match with Eigenstrat File Positions\n",
    "### Load HDF5\n",
    "\n",
    "def merge_positions():\n",
    "    \"\"\"Creates the Filter File to filter SNPs for\"\"\"\n",
    "    f_full = h5py.File(path_hdf5temp, \"r\") # Load for Sanity Check. See below!\n",
    "    print(\"Loaded %i variants\" % np.shape(f_full[\"calldata/GT\"])[0])\n",
    "    print(\"Loaded %i individuals\" % np.shape(f_full[\"calldata/GT\"])[1])\n",
    "    print(list(f_full[\"calldata\"].keys()))\n",
    "    print(list(f_full[\"variants\"].keys()))\n",
    "    #print(list(f[\"samples\"].keys()))\n",
    "\n",
    "    ### Load Eigenstrat\n",
    "    df_snp = pd.read_csv(snp1240k_path, header=None, sep=r\"\\s+\", engine=\"python\")\n",
    "    df_snp.columns = [\"SNP\", \"chr\", \"map\", \"pos\", \"ref\", \"alt\"]\n",
    "    df_snp = df_snp[df_snp[\"chr\"] == ch]\n",
    "    print(f\"Loaded {len(df_snp)} Chr.{ch} SNPs.\")\n",
    "\n",
    "    ### Prepare SNP File for Eigenstrat filtering \n",
    "    found = np.isin(f_full[\"variants/POS\"], df_snp[\"pos\"])\n",
    "    print(f\"Intersection: {np.sum(found)} out of {len(found)} SNPS\")\n",
    "    variants = f_full[\"variants/ID\"][found]\n",
    "\n",
    "    dots = np.where(variants == \".\")[0]\n",
    "    print(f\"Found {len(dots)} unnamed SNPs\")\n",
    "    variants = np.delete(variants, dots)\n",
    "\n",
    "    np.savetxt(snp_filter_path, variants, fmt=\"%s\")\n",
    "    print(f\"Successfully saved to {snp_filter_path}. Length: {len(variants)}\")\n",
    "    \n",
    "def save_megamarkers():\n",
    "    \"\"\"Save all MEGA markers of chromosome\n",
    "    chr (globally defined) in csv\n",
    "    (readable by bcftools)\"\"\"\n",
    "    df_snp = pd.read_csv(\"./Data/mega/MEGAex.txt\", \n",
    "                 low_memory=False, sep=\"\\t\")\n",
    "    df_snp.columns=[\"name\", \"chr\", \"pos\", \"map\"]\n",
    "    df_snp  = df_snp[df_snp[\"chr\"] == str(ch)]\n",
    "    df_snp = df_snp.sort_values(by=\"pos\")\n",
    "    print(f\"Loaded {len(df_snp)} Chr.{ch} SNPs.\")\n",
    "    df_save = df_snp[[\"chr\", \"pos\"]]\n",
    "    df_save.to_csv(marker_path, sep=\"\\t\", header=None, index=False)\n",
    "    print(f\"Saved {len(df_save)} Mega Markers on Chr. {ch} to {marker_path}\")\n",
    "    \n",
    "### Step 3: Create new vcf based on subset of Individuals and Markers\n",
    "def plink_new_vcf():\n",
    "    !plink --vcf $path_vcf100g --extract $snp_filter_path --keep-fam $ind_path --recode vcf --out $out_vcf_path0 --biallelic-only strict --keep-allele-order\n",
    "\n",
    "### Step 3b\n",
    "def bctools_new_vcf0():\n",
    "    \"\"\"Same as PLINK, but with bcftools \n",
    "    [small hack with marker strings, so LEGACY code and replaced by bcftools_new_vcf]\"\"\"\n",
    "    str_ex = \"ID=@\" + snp_filter_path\n",
    "    #!echo bcftools view -Oz -o $out_vcf_path_gz -S $ind_path -i $str_ex -m2 -M2 -v snps $path_vcf100g\n",
    "    !bcftools view -Oz -o $out_vcf_path_gz -S $ind_path -i $str_ex -m2 -M2 -v snps $path_vcf100g\n",
    "    print(\"Finished BCF tools runs.\")\n",
    "    \n",
    "def bctools_new_vcf(filter_iids=True, cluster=False):\n",
    "    \"\"\"Same as PLINK, but with bcftools and directly via Marker Positions.\n",
    "    filter_iids: Whether to use the .csv with Indivdiduals to extract\"\"\"\n",
    "    if filter_iids==True:\n",
    "        if cluster==False:\n",
    "            !bcftools view -Oz -o $out_vcf_path_gz -S $ind_path -T $marker_path -m2 -M2 -v snps $path_vcf100g\n",
    "        elif cluster==True:\n",
    "            !module load bcftools; bcftools view -Oz -o $out_vcf_path_gz -S $ind_path -T $marker_path -m2 -M2 -v snps $path_vcf100g     \n",
    "    elif filter_iids==False:\n",
    "        if cluster==False:\n",
    "            !bcftools view -Oz -o $out_vcf_path_gz -T $marker_path -m2 -M2 -v snps $path_vcf100g\n",
    "        elif cluster==True:\n",
    "            !module load bcftools; bcftools view -Oz -o $out_vcf_path_gz -T $marker_path -m2 -M2 -v snps $path_vcf100g\n",
    "    print(\"Finished BCF tools runs.\")\n",
    "\n",
    "### Step 4: Transfer to hdf5.\n",
    "#allel.vcf_to_hdf5(input=out_vcf_path, output=path_hdf5final, compression=\"gzip\") # Takes 1s\n",
    " \n",
    "### Step 5: Merge in Linkage Map\n",
    "    \n",
    "### Step 6: Delete the Data:\n",
    "def del_temp_data():\n",
    "    print(f\"Removing temporary vcf files...\")\n",
    "    !rm $path_vcf100g # Delete the full 1000 genome .vcf\n",
    "    !rm $out_vcf_path_gz # Delete the extracted .vcf\n",
    "    #!rm $path_hdf5temp # The originally intermediate hdf5 (for 1240k intersection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run the whole Procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_mega_from1000G_full(ch, cluster=True, delete=True):\n",
    "    \"\"\"ch: Which Chromosome to prepare\n",
    "    cluster: Whether Function is run on Cluster\"\"\"\n",
    "    prepare_paths(ch = ch)\n",
    "    download_1kg(cluster=cluster)  # Since we run it on the cluster\n",
    "    save_megamarkers()\n",
    "    print(\"Download Full Data Complete\")\n",
    "\n",
    "    bctools_new_vcf(filter_iids=False, cluster=cluster)  # Important, turn off filter individuals here!\n",
    "    vcf_to_hdf5(in_path=out_vcf_path_gz, out_path=path_hdf5final)\n",
    "    merge_in_ld_map(path_h5=path_hdf5final,\n",
    "                    path_snp1240k=snp1240k_path, chs=[ch])\n",
    "    \n",
    "    if delete:\n",
    "        del_temp_data()\n",
    "    print(\"Finished Preparing HDF5. GZ!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full Input path:\n",
      "./Data/1000Genomes/AutosomeVCF/ALL.chr1.phase3_shapeit2_mvncall_integrated_v5a.20130502.genotypes.vcf.gz\n",
      "Lifting LD Map from eigenstrat to HDF5...\n",
      "Loaded 114727 variants.\n",
      "Loaded 2504 individuals.\n",
      "Loaded 93166 Chr.1 1240K SNPs.\n",
      "Intersection 19489 out of 114727 HDF5 SNPs\n",
      "Interpolating 95238 variants.\n",
      "Finished Chromosome 1.\n",
      "Adding map to HDF5...\n",
      "We did it. Finished.\n",
      "CPU times: user 7.51 s, sys: 495 ms, total: 8 s\n",
      "Wall time: 9.53 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for ch in range(1,23):\n",
    "    prep_mega_from1000G_full(ch=ch, cluster=True, delete=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Area 51"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 103146 variants\n",
      "Loaded 2504 individuals\n",
      "['GT']\n",
      "['ALT', 'CHROM', 'FILTER_PASS', 'ID', 'MAP', 'POS', 'QUAL', 'REF']\n",
      "Found 103146/120485 of Meta positions in HDF5 file with 103146 markers\n"
     ]
    }
   ],
   "source": [
    "### Test the Final HDF5 just created\n",
    "ch=3\n",
    "#h5_path = \"./Data/mega/chr1.hdf5\"\n",
    "#h5_path = f\"./Data/1000Genomes/HDF5/FULLHDF5/maf02_chr{ch}.hdf5\"\n",
    "h5_path = f\"./Data/1000Genomes/HDF5/mega/all/chr{ch}.hdf5\"\n",
    "path_mega = \"./Data/mega/MEGAex.txt\"\n",
    "\n",
    "with h5py.File(h5_path, \"r\") as f: # Load for Sanity Check. See below!\n",
    "    print(\"Loaded %i variants\" % np.shape(f[\"calldata/GT\"])[0])\n",
    "    print(\"Loaded %i individuals\" % np.shape(f[\"calldata/GT\"])[1])\n",
    "    print(list(f[\"calldata\"].keys()))\n",
    "    print(list(f[\"variants\"].keys()))\n",
    "    pos = f[\"variants/POS\"][:]\n",
    "f.close()\n",
    "\n",
    "df = pd.read_csv(path_mega, \n",
    "                 low_memory=False, sep=\"\\t\")\n",
    "df1 = df[df[\"Chr\"]==str(ch)]\n",
    "df1 = df1.sort_values(by=\"MapInfo\")\n",
    "\n",
    "found = np.isin(df1[\"MapInfo\"].values, pos)\n",
    "print(f\"Found {np.sum(found)}/{len(found)} of Meta positions in HDF5 file with {len(pos)} markers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
