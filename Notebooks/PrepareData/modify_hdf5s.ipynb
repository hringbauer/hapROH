{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pilot for downsampling / throwing in errors\n",
    "Plan: Input is a a HDF5, Output is a HDF5. Eventually make class that can downsample, and/or throw errors on the reads.\n",
    "\n",
    "Class is a wrapper of an HDF5, and applies the operations to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "midway2-0402.rcc.local\n",
      "Midway jnovmbre partition detected.\n",
      "/project2/jnovembre/hringbauer/HAPSBURG\n"
     ]
    }
   ],
   "source": [
    "import allel\n",
    "import h5py  # Python Package to do the HDF5.\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import socket\n",
    "import os\n",
    "\n",
    "socket_name = socket.gethostname()\n",
    "print(socket_name)\n",
    "if socket_name == \"VioletQueen\":\n",
    "    path = \"/home/harald/git/HAPSBURG/\"   # The Path on Harald's machine\n",
    "elif socket_name[:7] == \"midway2\":\n",
    "    print(\"Midway jnovmbre partition detected.\")\n",
    "    path = \"/project2/jnovembre/hringbauer/HAPSBURG/\"  # The Path on Midway Cluster\n",
    "else: \n",
    "    raise RuntimeWarning(\"Not compatible machine. Check!!\")\n",
    "    \n",
    "os.chdir(path)  # Set the right Path (in line with Atom default)\n",
    "print(os.getcwd()) # Show the current working directory. Should be HAPSBURG/Notebooks/ParallelRuns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModifyHDF5Genotypes(object):\n",
    "    \"\"\"Class for Modifying HDF5 genotypes and\n",
    "    saving new HDF5s. Can downsample/throw down error/Create Readcound.\n",
    "    Plan: Also do contamination\"\"\"\n",
    "\n",
    "    f = 0    # The hdf5 object to modify\n",
    "    original_path = \"\" # Where to find the original HDF5\n",
    "    save_path = \"\"  # Where to save the modified HDF5 to\n",
    "    output = True # Whether to print any output\n",
    "    gt_new = []\n",
    "\n",
    "    def __init__(self, original_path=\"\", save_path=\"\", output=True):\n",
    "        \"\"\"pop_path: Where to load a HDF5 from\n",
    "           save_path: Where to save the new HDF5 to\"\"\"\n",
    "        self.output = output\n",
    "        self.save_path = save_path\n",
    "        \n",
    "        if output == True:\n",
    "            print(\"Heyho back old friend. I started running\")\n",
    "        \n",
    "        if len(original_path)>0:\n",
    "            self.original_path = original_path\n",
    "            self.load_data()\n",
    "        else:\n",
    "            print(\"No HDF5 Loaded! Alarm. Alarm. Alarm.\")\n",
    "\n",
    "    def load_data(self, path=\"\"):\n",
    "        \"\"\"Load the HDF5 Data\"\"\"\n",
    "        if len(path)==0:\n",
    "            path = self.original_path\n",
    "        self.f = h5py.File(path, \"r\") # Load for Sanity Check. See below!\n",
    "        \n",
    "        if self.output == True:\n",
    "            print(\"Loaded HDF5\")\n",
    "            print(\"Loaded %i variants\" % np.shape(self.f[\"calldata/GT\"])[0])\n",
    "            print(\"Loaded %i individuals\" % np.shape(self.f[\"calldata/GT\"])[1])\n",
    "            print(list(self.f[\"calldata\"].keys()))\n",
    "            print(list(self.f[\"variants\"].keys()))\n",
    "            #self.f[\"samples\"] # Samples Vector\n",
    "        \n",
    "        ### Sanity Check whether both Genotypes are there and nothing else\n",
    "        assert(np.min(self.f[\"calldata/GT\"]) == 0)\n",
    "        assert(np.max(self.f[\"calldata/GT\"]) == 1)\n",
    "\n",
    "    def save_data(self, gt, ad, ref, alt, pos, \n",
    "                  rec, samples, path, compression=\"gzip\", ad_group=True,\n",
    "                  gt_type=\"int8\"):\n",
    "        \"\"\"Create a new HDF5 File with Input Data.\n",
    "        gt: Genotype data [l,k,2]\n",
    "        ad: Allele depth [l,k,2]\n",
    "        ref: Reference Allele [l]\n",
    "        alt: Alternate Allele [l]\n",
    "        pos: Position  [l]\n",
    "        m: Map position [l]\n",
    "        samples: Sample IDs [k].\n",
    "        Save genotype data as int8, readcount data as int16.\n",
    "        ad: whether to save allele depth\n",
    "        gt_type: What genotype data type save\"\"\"\n",
    "\n",
    "        l, k, _ = np.shape(gt)  # Nr loci and Nr of Individuals\n",
    "\n",
    "        if os.path.exists(path):  # Do a Deletion of existing File there\n",
    "            os.remove(path)\n",
    "\n",
    "        dt = h5py.special_dtype(vlen=str)  # To have no problem with saving\n",
    "\n",
    "        with h5py.File(path, 'w') as f0:\n",
    "            ### Create all the Groups\n",
    "            f_map = f0.create_dataset(\"variants/MAP\", (l,), dtype='f')\n",
    "            if ad_group:\n",
    "                f_ad = f0.create_dataset(\"calldata/AD\", (l, k, 2), dtype='int8', compression=compression)\n",
    "            f_ref = f0.create_dataset(\"variants/REF\", (l,), dtype=dt)\n",
    "            f_alt = f0.create_dataset(\"variants/ALT\", (l,), dtype=dt)\n",
    "            f_pos = f0.create_dataset(\"variants/POS\", (l,), dtype='int32')\n",
    "            f_gt = f0.create_dataset(\"calldata/GT\", (l, k, 2), dtype=gt_type, compression=compression)\n",
    "            f_samples = f0.create_dataset(\"samples\", (k,), dtype=dt)\n",
    "\n",
    "            ### Save the Data\n",
    "            f_map[:] = rec\n",
    "            if ad_group:\n",
    "                f_ad[:] = ad\n",
    "            f_ref[:] = ref.astype(\"S1\")\n",
    "            f_alt[:] = alt.astype(\"S1\")\n",
    "            f_pos[:] = pos\n",
    "            f_gt[:] = gt\n",
    "            f_samples[:] = np.array(samples).astype(\"S10\")\n",
    "\n",
    "        if self.output == True:\n",
    "            print(f\"Successfully saved {k} individuals to: {path}\")\n",
    "\n",
    "    def create_error_gt(self, freq_flips=0.01):\n",
    "        \"\"\"Create Error on the HDF5 of genotypes.\n",
    "        freq_flips: How often to do flip of genotyps\"\"\"\n",
    "        f = self.f\n",
    "        gt = f[\"calldata/GT\"]\n",
    "        \n",
    "        switch = np.random.random(np.shape(gt)) < freq_flips\n",
    "        \n",
    "        if self.output == True:\n",
    "            print(f\"Swapping frac of SNPs: {np.mean(switch):.6f}\")\n",
    "\n",
    "        ### Switch the Genotypes\n",
    "        gt_new = (gt + switch) %2\n",
    "\n",
    "        self.save_data(gt_new, f[\"calldata/AD\"], f[\"variants/REF\"][:], f[\"variants/ALT\"][:], f[\"variants/POS\"], \n",
    "                       f[\"variants/MAP\"], f[\"samples\"][:], self.save_path)\n",
    "            \n",
    "    def downsample_gt(self, frac=0.9, ad=True, mult_alt=False, \n",
    "                      gt_type=\"int8\", compression=None):\n",
    "        \"\"\"Downsample the HDF5 to fewer reads.\n",
    "        Update also the recombination and position map if needed to remove missing values\n",
    "        frac: To what fraction of markers one downsamples\n",
    "        ad: Whether original HDF5 has AD field\n",
    "        mult_alt: Whether there are multiple alternative Allelels in the original HDF5\"\"\"\n",
    "        f = self.f\n",
    "        gt = f[\"calldata/GT\"]\n",
    "        \n",
    "        ### Decide on SNPs\n",
    "        l, n, _ = np.shape(gt)\n",
    "        survive = np.random.random(l) <= frac\n",
    "        print(f\"Fraction Loci surviving {np.mean(survive):.6f}\")\n",
    "        \n",
    "        ### Downsample\n",
    "        gt_new = gt[survive,:,:].astype(gt_type)\n",
    "        r_map_new = f[\"variants/MAP\"][survive]\n",
    "        if ad:\n",
    "            ad_new = f[\"calldata/AD\"][survive,:,:]\n",
    "        else:\n",
    "            ad_new = np.zeros(np.shape(gt_new), dtype=\"int8\")\n",
    "        \n",
    "        ref_new = f[\"variants/REF\"][survive]\n",
    "        \n",
    "        if mult_alt:\n",
    "            alt_new = f[\"variants/ALT\"][survive,0]   \n",
    "        else:\n",
    "            alt_new = f[\"variants/ALT\"][survive]\n",
    "        \n",
    "        pos_new = f[\"variants/POS\"][survive]\n",
    "        \n",
    "        ### Downsample where needed  \n",
    "        self.save_data(gt_new, ad_new, ref_new, alt_new, pos_new, r_map_new, \n",
    "                       f[\"samples\"], self.save_path, \n",
    "                       ad_group=ad, gt_type=gt_type, compression=compression)\n",
    "        \n",
    "    def generate_binomial_rc(self, mean_rc=1):\n",
    "        \"\"\"Generate Readcount Data from GT data.\n",
    "        mean_rc: The Mean total Readcount per site\"\"\"\n",
    "        \n",
    "        f = self.f\n",
    "        gt = f[\"calldata/GT\"]\n",
    "        \n",
    "        ### Create the Poisson Readcounts with the right mean\n",
    "        rc_full = poisson_readcounts(gt, mean_rc, output=self.output) \n",
    "        \n",
    "        self.save_data(gt, rc_full, f[\"variants/REF\"][:], f[\"variants/ALT\"][:], f[\"variants/POS\"], \n",
    "               f[\"variants/MAP\"], f[\"samples\"][:], self.save_path)\n",
    "        \n",
    "    def generate_lambda_rc(self, mean_rc = 1, norm_counts=True,\n",
    "                           lambda_path = \"./Data/1000Genomes/Coverage/mean_cov1240k_Marcus.csv\"):\n",
    "        \"\"\"Generate Readcount Data from GT data.\n",
    "        Use Table found at lambda_path for Lambdas \n",
    "        (relative. mean coverages, normed to 1 genome-wide)\n",
    "        norm_counts: Whether to normalize on overlapping Readcounts\"\"\"\n",
    "        \n",
    "        df_lambda = load_lambda(lambda_path, output=self.output)  ### Load the Lambda Data\n",
    "        \n",
    "        f = self.f\n",
    "        gt = f[\"calldata/GT\"]\n",
    "        l, n, _ = np.shape(gt)\n",
    "        \n",
    "        pos_f = f[\"variants/POS\"][:]  # The Position of the Original \n",
    "        _, i1, i2 = np.intersect1d(pos_f, df_lambda[\"Pos\"], return_indices=True)\n",
    "        \n",
    "        if self.output==True:\n",
    "            print(f\"Found {len(i1)} / {l} Loci in Lambda Table\")\n",
    "        \n",
    "        lambdas = df_lambda[\"Lambda\"].values[i2]\n",
    "        if norm_counts == True: # Normalize to extracted lambdas\n",
    "            lambdas = lambdas / np.mean(lambdas)\n",
    "            \n",
    "        mean_cov = lambdas * mean_rc  # Extract the Means that Intersect\n",
    "        gt = gt[i1,:,:]  # Downsample to Loci intersecting the Lambda Table \n",
    "        \n",
    "        ### Do the Binomial Readcount Sampling\n",
    "        rc_full = poisson_readcounts(gt, mean_cov[:,None], output=self.output) \n",
    "        \n",
    "        i1 = list(i1)  # So that it works with HDF5\n",
    "        self.save_data(gt, rc_full, f[\"variants/REF\"][i1], f[\"variants/ALT\"][i1], f[\"variants/POS\"][i1], \n",
    "               f[\"variants/MAP\"][i1], f[\"samples\"][:], self.save_path)\n",
    "           \n",
    "    def generate_ph(self, coverage = 1.0, error = 0.0):\n",
    "        \"\"\"Generate Pseudo-Haploid Data with fraction coverage sites covered,\n",
    "        and then error thrown down.\n",
    "        coverage: Fraction of sites covered\n",
    "        error: Fraction of sites with error. If >0, flip error added at random\"\"\"\n",
    "        \n",
    "        f = self.f\n",
    "        gt = f[\"calldata/GT\"]\n",
    "        l, _, _ = np.shape(gt)\n",
    "        \n",
    "        idx = np.random.random(l)<=coverage  # Which sites are covered\n",
    "        gt = gt[idx, :, :]  # Extract downsampled SNPs\n",
    "        \n",
    "        switch = [0,]\n",
    "        if error>0:\n",
    "            switch = (np.random.random(np.shape(gt)) < error) & (gt >= 0)\n",
    "            gt = (gt + switch) %2 # Switch the Genotypes\n",
    "                \n",
    "        if self.output:\n",
    "            print(f\"{np.sum(idx)} / {len(idx)} SNPs pseudohaploidized.\")\n",
    "            print(f\"Added fraction errors to SNPs: {np.mean(switch):.6f}\")\n",
    "            print(f\"Added sum errors: {np.sum(switch):.0f}\")\n",
    "        \n",
    "        rc = pseudo_haploid(gt) # Generate Pseudo-Haploid Readcounts\n",
    "        \n",
    "        idx = np.array(idx)  # So that it works with HDF5 (Boolean Indexing)\n",
    "        self.save_data(gt, rc, f[\"variants/REF\"][idx], f[\"variants/ALT\"][idx], f[\"variants/POS\"][idx], \n",
    "                       f[\"variants/MAP\"][idx], f[\"samples\"][:], self.save_path)\n",
    "        \n",
    "    def copy_rohinfo(self, load_path=\"\", save_path=\"\", file=\"roh_info.csv\"):\n",
    "        \"\"\"Copy in the ROH Info from folder of load path into folder of save_path.\n",
    "        file: Which file to copy (roh_info by default)\"\"\"\n",
    "        if len(load_path) == 0:\n",
    "            load_path = self.original_path\n",
    "            \n",
    "        if len(save_path) ==0 :\n",
    "            save_path = self.save_path\n",
    "            \n",
    "        save_path = os.path.dirname(save_path) + \"/\" + file\n",
    "        load_path = os.path.dirname(load_path) + \"/\" + file\n",
    "        \n",
    "        ### Copy the file\n",
    "        !cp $load_path $save_path  \n",
    "        \n",
    "##########################################\n",
    "#### Some Small Helper Functions\n",
    "\n",
    "def load_lambda(loadpath, ch=3, output=True):\n",
    "    \"\"\"Load and return the Lambda Vector\n",
    "    for Chromosome ch, and from path loadpath\"\"\"\n",
    "    df_lambda = pd.read_csv(loadpath)\n",
    "    mean = np.mean(df_lambda[\"Lambda\"])\n",
    "    assert(np.isclose(mean, 1))  # Sanity Check if Valid Lambda Vector\n",
    "    l=len(df_lambda)\n",
    "    df_lambda = df_lambda[df_lambda[\"Ch\"]==ch]\n",
    "    if output==True:\n",
    "        print(f\"Extracted {len(df_lambda)} / {l} Loci on Chr.{ch}\")\n",
    "    return df_lambda\n",
    "\n",
    "def poisson_readcounts(gt, mean_rc, output=True):\n",
    "    \"\"\"Create and return Poisson Readcount array.\n",
    "    gt: Underlying Genotype Matrix [l, n, 2]\n",
    "    Return readcound array: [l, n, 2]\"\"\"\n",
    "    l, n, _ = np.shape(gt)\n",
    "    rc_tot = np.random.poisson(lam=mean_rc, size = (l,n))  # Draw Full Readcounts\n",
    "\n",
    "    p = np.mean(gt, axis=2) # Get the Mean Allele Frequency per locus and individual\n",
    "    assert(np.max(p)<=1) ### Sanity Check whether allele freqs are right\n",
    "    assert(np.min(p)>=0)\n",
    "\n",
    "    rc_der = np.random.binomial(n=rc_tot, p=p)  # The derived Readcount (Binomial Sampling)\n",
    "    rc_ref = rc_tot - rc_der  # The Ref Readcount\n",
    "\n",
    "    rc_full = np.stack([rc_ref, rc_der], axis=2)\n",
    "    assert(np.shape(rc_full) == np.shape(gt))  # Check whether data was created properly\n",
    "\n",
    "    if output == True:\n",
    "        print(f\"Mean Readcount: {np.mean(rc_tot):.4f}\")\n",
    "    \n",
    "    return rc_full\n",
    "\n",
    "def pseudo_haploid(gt):\n",
    "    \"\"\"Create and return Pseudo-Haploid Readcount array\n",
    "    gt: Underlying Genotype Matrix [l, n, 2]\n",
    "    Return readcound array: [l, n, 2]\"\"\"\n",
    "    \n",
    "    p = np.mean(gt, axis=2) # Get the Mean Allele Frequency per locus and individual\n",
    "    \n",
    "    rc_der = np.random.binomial(n=1, p=p)  # The derived Readcount (Binomial Sampling)\n",
    "    rc_ref = 1 - rc_der  # The Ref Readcount\n",
    "    rc_full = np.stack([rc_ref, rc_der], axis=2)\n",
    "    \n",
    "    assert(np.max(rc_full)<=1)\n",
    "    assert(np.min(rc_full)==0)\n",
    "    assert(np.shape(rc_full) == np.shape(gt))  # Check whether data was created properly\n",
    "    return rc_full"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Pseudohaploid data hdf5s that are used for parameter setting\n",
    ".) Simulate 0.5x coverage as well as 0.01 error rate into from TSI5 (perfect genotypes) into TSI6  \n",
    ".) Simulate 1x coverage with 0.001 error rate from TSI5 into TSI7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "org_folder = \"./Simulated/1000G_Mosaic/TSI5/\"\n",
    "out_folder = \"./Simulated/1000G_Mosaic/TSI7/\"\n",
    "\n",
    "snps_cov = 1.0 # 0.5 for TSI6\n",
    "e_rate = 0.001\n",
    "lengths = [0, 2, 4, 6, 8, 10]\n",
    "\n",
    "for l in lengths:\n",
    "    org_folder1 = org_folder + \"ch3_\" + str(l) + \"cm/\"\n",
    "    load_path = org_folder1 + \"data.h5\" \n",
    "    save_path = out_folder + \"ch3_\" + str(l) + \"cm/data.h5\" \n",
    "\n",
    "    # Make Directory if not already there\n",
    "    if not os.path.exists(os.path.dirname(save_path)):   \n",
    "        os.makedirs(os.path.dirname(save_path))\n",
    "\n",
    "    #os.remove(save_path)  # For previous whoopsie\n",
    "    m = ModifyHDF5Genotypes(original_path=load_path, save_path=save_path)\n",
    "    \n",
    "    m.generate_ph(coverage = snps_cov, error = e_rate)\n",
    "    m.copy_rohinfo()   # Copy the ROH Info!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save the reference with int8 Genotype data. Faster to read!\n",
    "2020 Version: Include gzip and don't save Allele Depths\n",
    "gt_type: Specifiy dtype of Genotype File (int8 or np.bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "org_folder = \"./Data/1000Genomes/HDF5/1240kHDF5/all1240/chr\"\n",
    "out_folder = \"./Data/1000Genomes/HDF5/1240kHDF5/all1240bool0/chr\"\n",
    "\n",
    "chs = range(1, 23)\n",
    "\n",
    "for ch in chs:\n",
    "    load_path = org_folder + str(ch) + \".hdf5\"\n",
    "    save_path = out_folder + str(ch) + \".hdf5\" \n",
    "\n",
    "    # Make Directory if not already there\n",
    "    if not os.path.exists(os.path.dirname(save_path)):   \n",
    "        os.makedirs(os.path.dirname(save_path))\n",
    "\n",
    "    #os.remove(save_path)  # For previous whoopsie\n",
    "    m = ModifyHDF5Genotypes(original_path=load_path, save_path=save_path)\n",
    "    m.downsample_gt(frac=1.0, ad=False, mult_alt=True, gt_type=\"int8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data hdf5s with various levels of errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_vec = np.logspace(-3,-1, 8)\n",
    "org_folder = \"./Simulated/1000G_Mosaic/TSI5/\"\n",
    "lengths = [0, 2, 4, 6, 8, 10]\n",
    "\n",
    "for l in lengths:\n",
    "    for e in error_vec:\n",
    "        print(f\"Doing Error Frac: {e}\")\n",
    "        org_folder1 = org_folder + \"ch3_\" + str(l) + \"cm/\"\n",
    "        load_path = org_folder1 + \"data.h5\" \n",
    "        \n",
    "        e_print = str(round(e, 4)).split(\".\")[1] # Extract four digits after decimal\n",
    "        \n",
    "        ### To do save 4 digits error data file\n",
    "        save_path = org_folder1 + \"error/\" + e_print + \"/data.h5\"   \n",
    "        \n",
    "        # Make Directory if not already there\n",
    "        if not os.path.exists(os.path.dirname(save_path)):   \n",
    "            os.makedirs(os.path.dirname(save_path))\n",
    "        \n",
    "        #os.remove(save_path)  # For previous whoopsie\n",
    "        m = ModifyHDF5Genotypes(original_path=load_path, save_path=save_path)\n",
    "            \n",
    "        m.create_error_gt(freq_flips=e)\n",
    "        m.copy_rohinfo()   # Copy the ROH Info!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data hdf5s with various levels of missingness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "missing_vec = np.linspace(0.1, 1.0, 10)\n",
    "#missing_vec = np.linspace(0.3, 1.0, 8)\n",
    "#missing_vec = np.array([0.1, 0.2])\n",
    "org_folder = \"./Simulated/1000G_Mosaic/CHB/\"\n",
    "#lengths = [2, 4, 6, 8, 10]\n",
    "lengths = [0,]\n",
    "\n",
    "for l in lengths:\n",
    "    for m in missing_vec:\n",
    "        print(f\"Doing Missing Fraction: {m}\")\n",
    "        org_folder1 = org_folder + \"ch3_\" + str(l) + \"cm/\"\n",
    "        load_path = org_folder1 + \"data.h5\" \n",
    "        \n",
    "        m_print = str(round(m, 4)).split(\".\")[1] # Extract four digits after decimal\n",
    "        \n",
    "        ### To do save 4 digits error data file\n",
    "        save_path = org_folder1 + \"missing/\" + m_print + \"/data.h5\"   \n",
    "        \n",
    "        # Make Directory if not already there\n",
    "        if not os.path.exists(os.path.dirname(save_path)):   \n",
    "            os.makedirs(os.path.dirname(save_path))\n",
    "        \n",
    "        #os.remove(save_path)  # For previous whoopsie\n",
    "        modh5 = ModifyHDF5Genotypes(original_path=load_path, save_path=save_path)\n",
    "            \n",
    "        modh5.downsample_gt(frac=m)\n",
    "        modh5.copy_rohinfo()   # Copy the ROH Info!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create data hdf5s with Lambda Readcounts around Poisson mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_poisson_mean_hdf5s(lengths, mean_rcs, org_folder=\"./Simulated/1000G_Mosaic/TSI5/\", \n",
    "                              lambda_rc_string=\"lambda_rc\", output=False):\n",
    "    \"\"\"Create Downsampled HDF5s with Poisson Mean Readcount Data. \n",
    "    And save into the same folder \n",
    "    but with lambda_rcX.X prefix in file\n",
    "    lengths: Block Lengths Array [in CM]\n",
    "    mean_rcs: Mean Readcount Array\n",
    "    org_folder: The original Folder\"\"\" \n",
    "\n",
    "    for mean_rc in mean_rcs:\n",
    "        print(f\"Simulating Mean RC: {mean_rc}x \")\n",
    "        for l in lengths:\n",
    "            print(f\"Doing block length: {l} cM\")\n",
    "            original_path = org_folder + \"ch3_\" + str(l) + \"cm/data.h5\"\n",
    "            #save_path = org_folder + \"rc\" + str(mean_rc) + \"/ch3_\" + str(l) + \"cm/data.h5\"\n",
    "            save_path = org_folder + lambda_rc_string + f\"{mean_rc:.1f}\" + \"/ch3_\" + str(l) + \"cm/data.h5\"\n",
    "\n",
    "            if not os.path.exists(os.path.dirname(save_path)):   \n",
    "                print(f\"Creating DIR: {save_path}\")\n",
    "                os.makedirs(os.path.dirname(save_path))\n",
    "\n",
    "            m = ModifyHDF5Genotypes(original_path = original_path, save_path = save_path, output=output)\n",
    "            #m.generate_binomial_rc(mean_rc = mean_rc)\n",
    "            m.generate_lambda_rc(mean_rc = mean_rc)\n",
    "            m.copy_rohinfo()\n",
    "\n",
    "    print(\"Once more: Finished the Job.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lengths = [0, 2, 4, 6, 8, 10] # The Block Lengths to simulate\n",
    "mean_rcs = np.linspace(0.1, 1, 10)\n",
    "#mean_rcs = np.linspace(2, 6, 5)\n",
    "#lengths =[8]\n",
    "#mean_rcs = [1.]\n",
    "\n",
    "org_folder = \"./Simulated/1000G_Mosaic/CHB/\"\n",
    "create_poisson_mean_hdf5s(lengths=lengths, mean_rcs=mean_rcs, org_folder=org_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Downsampled Readcount File from TSI6 (the test case with 0.5x PH and 0.01 error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simulating Mean RC: 1.0x \n",
      "Doing block length: 0 cM\n",
      "Creating DIR: ./Simulated/1000G_Mosaic/TSI6/lambda_rc1.0/ch3_0cm/data.h5\n",
      "Doing block length: 4 cM\n",
      "Creating DIR: ./Simulated/1000G_Mosaic/TSI6/lambda_rc1.0/ch3_4cm/data.h5\n",
      "Once more: Finished the Job.\n"
     ]
    }
   ],
   "source": [
    "create_poisson_mean_hdf5s(lengths=[0,4,], mean_rcs=[1.0,], \n",
    "                          org_folder=\"./Simulated/1000G_Mosaic/TSI6/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare data hdf5s with downsampled Pseudohaploid Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pseudohaploid_hdf5s(lengths, coverages, org_folder=\"./Simulated/1000G_Mosaic/TSI5/\", \n",
    "                              lambda_rc_string=\"ph\", output=False):\n",
    "    \"\"\"Create Downsampled HDF5s with Poisson Mean Readcount Data. And save into the same folder \n",
    "    but with lambda_rcX prefix (X coverage)\n",
    "    lengths: Block Lengths Array [in CM]\n",
    "    mean_rcs: Mean Readcount Array\n",
    "    org_folder: The original Folder\"\"\" \n",
    "    \n",
    "    for cov in coverages:\n",
    "        print(f\"Simulating Coverage: {cov}x \")\n",
    "        for l in lengths:\n",
    "            print(f\"Doing block length: {l} cM\")\n",
    "            original_path = org_folder + \"ch3_\" + str(l) + \"cm/data.h5\"  # Hardcoded Path to Original Data\n",
    "            save_path = org_folder + lambda_rc_string + f\"{cov:.1f}\" + \"/ch3_\" + str(l) + \"cm/data.h5\"\n",
    "\n",
    "            if not os.path.exists(os.path.dirname(save_path)):   \n",
    "                print(f\"Creating DIR: {save_path}\")\n",
    "                os.makedirs(os.path.dirname(save_path))\n",
    "\n",
    "            m = ModifyHDF5Genotypes(original_path = original_path, save_path = save_path, output=output)\n",
    "            m.generate_ph(coverage=cov)\n",
    "            m.copy_rohinfo()\n",
    "    print(\"Great Job: Finished Successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simulating Coverage: 1.0x \n",
      "Doing block length: 0 cM\n",
      "Doing block length: 2 cM\n",
      "Creating DIR: ./Simulated/1000G_Mosaic/TSI5/ph1.0/ch3_2cm/data.h5\n",
      "Doing block length: 4 cM\n",
      "Creating DIR: ./Simulated/1000G_Mosaic/TSI5/ph1.0/ch3_4cm/data.h5\n",
      "Doing block length: 6 cM\n",
      "Creating DIR: ./Simulated/1000G_Mosaic/TSI5/ph1.0/ch3_6cm/data.h5\n",
      "Doing block length: 8 cM\n",
      "Creating DIR: ./Simulated/1000G_Mosaic/TSI5/ph1.0/ch3_8cm/data.h5\n",
      "Doing block length: 10 cM\n",
      "Creating DIR: ./Simulated/1000G_Mosaic/TSI5/ph1.0/ch3_10cm/data.h5\n",
      "Great Job: Finished Successfullyb.\n"
     ]
    }
   ],
   "source": [
    "lengths = [0, 2, 4, 6, 8, 10] # The Block Lengths to simulate\n",
    "coverages = [1.0,]\n",
    "\n",
    "org_folder = \"./Simulated/1000G_Mosaic/TSI5/\"\n",
    "create_pseudohaploid_hdf5s(lengths=lengths, coverages=coverages, lambda_rc_string=\"ph\", org_folder=org_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Area 51"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Heyho back old friend. I started running\n",
      "Loaded HDF5\n",
      "Loaded 77652 variants\n",
      "Loaded 100 individuals\n",
      "['AD', 'GT']\n",
      "['ALT', 'MAP', 'POS', 'REF']\n",
      "./Simulated/1000G_Mosaic/TSI5/ch3_2cm/roh_info.csv\n",
      "./Simulated/1000G_Mosaic/TSI5/ch3_2cm/roh_info.csv\n"
     ]
    }
   ],
   "source": [
    "m = ModifyHDF5Genotypes(original_path=\"./Simulated/1000G_Mosaic/TSI5/ch3_2cm/data.h5\", save_path=\"./Simulated/1000G_Mosaic/TSI5/ch3_2cm/data_shitty.h5\")\n",
    "#m.downsample_gt(frac=0.8)\n",
    "#m.create_error_gt(freq_flips=0.01)\n",
    "m.copy_rohinfo()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Readcount Data from Genotype Data: Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating DIR: ./Simulated/1000G_Mosaic/TSI5/rc/ch3_4cm/data.h5\n",
      "Heyho back old friend. I started running\n",
      "Loaded HDF5\n",
      "Loaded 77652 variants\n",
      "Loaded 100 individuals\n",
      "['AD', 'GT']\n",
      "['ALT', 'MAP', 'POS', 'REF']\n",
      "Mean Readcount: 2.0003\n",
      "Successfully saved 100 individuals to: ./Simulated/1000G_Mosaic/TSI5/rc/ch3_4cm/data.h5\n"
     ]
    }
   ],
   "source": [
    "original_path=\"./Simulated/1000G_Mosaic/TSI5/ch3_4cm/data.h5\"\n",
    "save_path=\"./Simulated/1000G_Mosaic/TSI5/rc/ch3_4cm/data.h5\"\n",
    "\n",
    "\n",
    "if not os.path.exists(os.path.dirname(save_path)):   \n",
    "    print(f\"Creating DIR: {save_path}\")\n",
    "    os.makedirs(os.path.dirname(save_path))\n",
    "\n",
    "m = ModifyHDF5Genotypes(original_path = original_path, save_path = save_path)\n",
    "m.generate_binomial_rc(mean_rc=2)\n",
    "m.copy_rohinfo()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Lambda Readcount Data from Genotypes Data: Test Case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Heyho back old friend. I started running\n",
      "Loaded HDF5\n",
      "Loaded 77652 variants\n",
      "Loaded 100 individuals\n",
      "['AD', 'GT']\n",
      "['ALT', 'MAP', 'POS', 'REF']\n",
      "Extracted 81079 / 1145647 Loci on Chr.3\n",
      "Found 77650 / 77652 Loci in Lambda Table\n",
      "Mean Readcount: 0.1999\n",
      "Successfully saved 100 individuals to: ./Simulated/1000G_Mosaic/TSI5/lambda_rc0.2/ch3_4cm/data.h5\n"
     ]
    }
   ],
   "source": [
    "original_path=\"./Simulated/1000G_Mosaic/TSI5/ch3_4cm/data.h5\"\n",
    "save_path=\"./Simulated/1000G_Mosaic/TSI5/lambda_rc0.3/ch3_4cm/data.h5\"\n",
    "\n",
    "if not os.path.exists(os.path.dirname(save_path)):   \n",
    "    print(f\"Creating DIR: {save_path}\")\n",
    "    os.makedirs(os.path.dirname(save_path))\n",
    "\n",
    "m = ModifyHDF5Genotypes(original_path = original_path, save_path = save_path)\n",
    "m.generate_lambda_rc(mean_rc = 0.3)\n",
    "m.copy_rohinfo()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing HDF5s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded HDF5\n",
      "Loaded 77652 variants\n",
      "Loaded 100 individuals\n",
      "['AD', 'GT']\n",
      "['ALT', 'MAP', 'POS', 'REF']\n"
     ]
    }
   ],
   "source": [
    "#path = \"./Simulated/1000G_Mosaic/TSI5/rc0.2/ch3_4cm/data.h5\"\n",
    "#path = \"./Simulated/1000G_Mosaic/TSI5/ch3_10cm/data.h5\"\n",
    "#path = \"./Simulated/1000G_Mosaic/TSI5/lambda_rc0.2/ch3_4cm/data.h5\"\n",
    "path = \"./Simulated/1000G_Mosaic/CHB/ch3_4cm/data.h5\"\n",
    "\n",
    "f = h5py.File(path, \"r\") # Load for Sanity Check. See below!\n",
    "        \n",
    "print(\"Loaded HDF5\")\n",
    "print(\"Loaded %i variants\" % np.shape(f[\"calldata/GT\"])[0])\n",
    "print(\"Loaded %i individuals\" % np.shape(f[\"calldata/GT\"])[1])\n",
    "print(list(f[\"calldata\"].keys()))\n",
    "print(list(f[\"variants\"].keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6.51 ms, sys: 22.8 ms, total: 29.3 ms\n",
      "Wall time: 688 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1, 1],\n",
       "       [1, 0],\n",
       "       [0, 0],\n",
       "       ...,\n",
       "       [0, 0],\n",
       "       [1, 1],\n",
       "       [0, 0]], dtype=int32)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "f[\"calldata/GT\"][:,0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.33 ms, sys: 24.2 ms, total: 27.6 ms\n",
      "Wall time: 648 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1, 1],\n",
       "       [1, 0],\n",
       "       [0, 0],\n",
       "       ...,\n",
       "       [0, 0],\n",
       "       [1, 1],\n",
       "       [0, 0]], dtype=int32)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "f[\"calldata/AD\"][:,0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Coverage: 0.39964\n"
     ]
    }
   ],
   "source": [
    "tot_cov = np.sum(f[\"calldata/AD\"], axis=2)\n",
    "print(f\"Mean Coverage: {np.mean(tot_cov):.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5753332670021388"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.var(tot_cov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7180924661944623"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(tot_cov==0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the Lambda RCs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ch=3\n",
    "loadpath = \"./Data/1000Genomes/Coverage/mean_cov1240k_Marcus.csv\"\n",
    "\n",
    "df_lambda = load_lambda(loadpath, ch=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_f = f[\"variants/POS\"][:]\n",
    "_, i1, i2 = np.intersect1d(pos_f, df_lambda[\"Pos\"], return_indices=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded HDF5\n",
      "Loaded 77652 variants\n",
      "Loaded 100 individuals\n",
      "['AD', 'GT']\n",
      "['ALT', 'MAP', 'POS', 'REF']\n"
     ]
    }
   ],
   "source": [
    "path = \"./Simulated/1000G_Mosaic/YRI/ch3_4cm/data.h5\"\n",
    "\n",
    "f = h5py.File(path, \"r\") # Load for Sanity Check. See below!\n",
    "        \n",
    "print(\"Loaded HDF5\")\n",
    "print(\"Loaded %i variants\" % np.shape(f[\"calldata/AD\"])[0])\n",
    "print(\"Loaded %i individuals\" % np.shape(f[\"calldata/AD\"])[1])\n",
    "print(list(f[\"calldata\"].keys()))\n",
    "print(list(f[\"variants\"].keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['iid0', 'iid1', 'iid2', 'iid3', 'iid4', 'iid5', 'iid6', 'iid7',\n",
       "       'iid8', 'iid9', 'iid10', 'iid11', 'iid12', 'iid13', 'iid14',\n",
       "       'iid15', 'iid16', 'iid17', 'iid18', 'iid19', 'iid20', 'iid21',\n",
       "       'iid22', 'iid23', 'iid24', 'iid25', 'iid26', 'iid27', 'iid28',\n",
       "       'iid29', 'iid30', 'iid31', 'iid32', 'iid33', 'iid34', 'iid35',\n",
       "       'iid36', 'iid37', 'iid38', 'iid39', 'iid40', 'iid41', 'iid42',\n",
       "       'iid43', 'iid44', 'iid45', 'iid46', 'iid47', 'iid48', 'iid49',\n",
       "       'iid50', 'iid51', 'iid52', 'iid53', 'iid54', 'iid55', 'iid56',\n",
       "       'iid57', 'iid58', 'iid59', 'iid60', 'iid61', 'iid62', 'iid63',\n",
       "       'iid64', 'iid65', 'iid66', 'iid67', 'iid68', 'iid69', 'iid70',\n",
       "       'iid71', 'iid72', 'iid73', 'iid74', 'iid75', 'iid76', 'iid77',\n",
       "       'iid78', 'iid79', 'iid80', 'iid81', 'iid82', 'iid83', 'iid84',\n",
       "       'iid85', 'iid86', 'iid87', 'iid88', 'iid89', 'iid90', 'iid91',\n",
       "       'iid92', 'iid93', 'iid94', 'iid95', 'iid96', 'iid97', 'iid98',\n",
       "       'iid99'], dtype=object)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f[\"samples\"][:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.375369456536567"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tot_cov = np.sum(ad, axis=2)\n",
    "np.var(tot_cov[:, 15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
