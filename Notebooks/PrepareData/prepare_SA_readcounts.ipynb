{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare the Readcount Data I got from David for the South Americans into a HDF5 compatible with HAPSBURG\n",
    "Additonally: Also prepare the downsampled RC/PH HDF5 versions of high coverage SA Individuals\n",
    "@ Author: Harald Ringbauer, 2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "midway2-0401.rcc.local\n",
      "Midway jnovmbre partition detected.\n",
      "/project2/jnovembre/hringbauer/HAPSBURG\n",
      "CPU Count: 28\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os  # For Saving to Folder\n",
    "import pandas as pd\n",
    "import h5py  # Python Package to do the HDF5.\n",
    "\n",
    "import socket\n",
    "import os as os\n",
    "import sys as sys\n",
    "import multiprocessing as mp\n",
    "\n",
    "socket_name = socket.gethostname()\n",
    "print(socket_name)\n",
    "if socket_name == \"VioletQueen\":\n",
    "    path = \"/home/harald/git/HAPSBURG/\"   # The Path on Harald's machine\n",
    "elif socket_name.startswith(\"midway2\"):\n",
    "    print(\"Midway jnovmbre partition detected.\")\n",
    "    path = \"/project2/jnovembre/hringbauer/HAPSBURG/\"  # The Path on Midway Cluster\n",
    "else: \n",
    "    raise RuntimeWarning(\"Not compatible machine. Check!!\")\n",
    "    \n",
    "os.chdir(path)  # Set the right Path (in line with Atom default)\n",
    "sys.path.append(\"./package/hapsburg/\")  # Since now we are in the Root Directory\n",
    "#from hmm_inference import HMM_Analyze   # Do not move. Should be after sys.path..\n",
    "\n",
    "\n",
    "print(os.getcwd()) # Show the current working directory. Should be HAPSBURG/Notebooks/ParallelRuns\n",
    "print(f\"CPU Count: {mp.cpu_count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_df_from_rc(path_ind = \"../fromDavid/MA577_1240k_all.cnts\"):\n",
    "    df_rc = pd.read_csv(path_ind, header=None, sep=\" \")\n",
    "    df_rc.columns=[\"chr\",\"pos\", \"ref\", \"alt\", \"A\", \"G\", \"C\", \"T\"]\n",
    "    print(f\"Loaded {len(df_rc)} Markers\")\n",
    "    return df_rc\n",
    "\n",
    "def creat_count_col(df_rc):\n",
    "    \"\"\"Add Fields for ref and alt Count\"\"\"\n",
    "    df_rc[\"ref_count\"]=0\n",
    "    df_rc[\"alt_count\"]=0\n",
    "\n",
    "    for a in [\"A\", \"G\", \"C\", \"T\"]:\n",
    "        idcs = df_rc[\"ref\"]==a\n",
    "        df_rc.loc[idcs,\"ref_count\"]=df_rc[a]\n",
    "\n",
    "        idcs = df_rc[\"alt\"]==a\n",
    "        df_rc.loc[idcs,\"alt_count\"]=df_rc[a]\n",
    "    mean_depth = np.mean(df_rc[\"alt_count\"]+df_rc[\"ref_count\"])\n",
    "    print(f\"Mean Depth: {mean_depth:.4f}\")\n",
    "    return df_rc\n",
    "\n",
    "def save_hdf5(gt, ad, ref, alt, pos, ch, samples, path):\n",
    "    \"\"\"Create a new HDF5 File with Input Data.\n",
    "    gt: Genotype data [l,k,2]\n",
    "    ad: Allele depth [l,k,2]\n",
    "    ref: Reference Allele [l]\n",
    "    alt: Alternate Allele [l]\n",
    "    pos: Position  [l]\n",
    "    m: Map position [l]\n",
    "    ch: Which chromosome [l]\n",
    "    samples: Sample IDs [k]\"\"\"\n",
    "\n",
    "    l, k, _ = np.shape(gt)  # Nr loci and Nr of Individuals\n",
    "\n",
    "    if os.path.exists(path):  ### Do a Deletion of existing File there\n",
    "        os.remove(path)\n",
    "\n",
    "    dt = h5py.special_dtype(vlen=str)  # To have no problem with saving\n",
    "\n",
    "    with h5py.File(path, 'w') as f0:\n",
    "        ### Create all the Groups\n",
    "        #f_map = f0.create_dataset(\"variants/MAP\", (l,), dtype='f')\n",
    "        f_ch = f0.create_dataset(\"variants/CHROM\", (l,), dtype='i')\n",
    "        f_ad = f0.create_dataset(\"calldata/AD\", (l, k, 2), dtype='i')\n",
    "        f_ref = f0.create_dataset(\"variants/REF\", (l,), dtype=dt)\n",
    "        f_alt = f0.create_dataset(\"variants/ALT\", (l,), dtype=dt)\n",
    "        f_pos = f0.create_dataset(\"variants/POS\", (l,), dtype='i')\n",
    "        f_gt = f0.create_dataset(\"calldata/GT\", (l, k, 2), dtype='i')\n",
    "        f_samples = f0.create_dataset(\"samples\", (k,), dtype=dt)\n",
    "\n",
    "        ### Save the Data\n",
    "        #f_map[:] = rec\n",
    "        f_ch[:] = ch\n",
    "        f_ad[:] = ad\n",
    "        f_ref[:] = ref.astype(\"S1\")\n",
    "        f_alt[:] = alt.astype(\"S1\")\n",
    "        f_pos[:] = pos\n",
    "        f_gt[:] = gt\n",
    "        f_samples[:] = np.array(samples).astype(\"S10\")\n",
    "\n",
    "    print(f\"Successfully saved {k} individuals to: {path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do single Indivudal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rc_to_hdf_1ind(path_ind, path_h5=\"./Data/SA_1240kHDF5/MA577_1240k.h5\", iid=\"MA577_1240k\"):\n",
    "    \"\"\"Produce HDF5 File from Readcount Data\"\"\"\n",
    "    df_rc= get_df_from_rc(path_ind)\n",
    "    df_rc = creat_count_col(df_rc)\n",
    "    l = len(df_rc)\n",
    "    k = 1\n",
    "\n",
    "    ###\n",
    "    gt = -np.ones((l,k,2), dtype=\"int8\") # No genotypes\n",
    "    ad = df_rc[[\"ref_count\", \"alt_count\"]].values[:,None,:] # None for n=1 axis\n",
    "    ref = df_rc[\"ref\"].values\n",
    "    alt = df_rc[\"alt\"].values\n",
    "    pos = df_rc[\"pos\"].values\n",
    "    ch = df_rc[\"chr\"].values\n",
    "    samples=[iid,]\n",
    "\n",
    "    save_hdf5(gt, ad, ref, alt, pos, ch, samples, path_h5)\n",
    "    print(f\"Successfully saved to {path_h5}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1115283 Markers\n",
      "Mean Depth: 19.5486\n",
      "Successfully saved 1 individuals to: ./Data/SA_1240kHDF5/Loschbour.h5\n",
      "Successfully saved to ./Data/SA_1240kHDF5/Loschbour.h5\n",
      "Loaded 1115215 Markers\n",
      "Mean Depth: 18.2127\n",
      "Successfully saved 1 individuals to: ./Data/SA_1240kHDF5/Stuttgart.h5\n",
      "Successfully saved to ./Data/SA_1240kHDF5/Stuttgart.h5\n",
      "Loaded 1115315 Markers\n",
      "Mean Depth: 40.7515\n",
      "Successfully saved 1 individuals to: ./Data/SA_1240kHDF5/Ust_Ishim.h5\n",
      "Successfully saved to ./Data/SA_1240kHDF5/Ust_Ishim.h5\n"
     ]
    }
   ],
   "source": [
    "base_folder=\"../fromDavid/\"\n",
    "out_folder=\"./Data/SA_1240kHDF5/\"\n",
    "\n",
    "#iids=[\"IPY10\", \"IPK12\", \"MA577\", \"894\", \"895\"]\n",
    "iids=[\"Loschbour\", \"Stuttgart\", \"Ust_Ishim\"]\n",
    "\n",
    "paths = [os.path.join(base_folder, iid + \"_1240k_all.cnts\") for iid in iids]\n",
    "paths_h5 = [os.path.join(out_folder,iid + \".h5\") for iid in iids]\n",
    "\n",
    "for i in range(len(paths)):\n",
    "    rc_to_hdf_1ind(paths[i], path_h5=paths_h5[i], iid=iids[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do Individuals from Moreno-Mayer\n",
    "Do the Individuals where David W. downloaded and processed the RC data. Only iids and the suffix are different"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1115101 Markers\n",
      "Mean Depth: 12.9466\n",
      "Successfully saved 1 individuals to: ./Data/SA_1240kHDF5/A460.h5\n",
      "Successfully saved to ./Data/SA_1240kHDF5/A460.h5\n",
      "Loaded 1102888 Markers\n",
      "Mean Depth: 22.4121\n",
      "Successfully saved 1 individuals to: ./Data/SA_1240kHDF5/Andaman.h5\n",
      "Successfully saved to ./Data/SA_1240kHDF5/Andaman.h5\n",
      "Loaded 912911 Markers\n",
      "Mean Depth: 2.2065\n",
      "Successfully saved 1 individuals to: ./Data/SA_1240kHDF5/5832.h5\n",
      "Successfully saved to ./Data/SA_1240kHDF5/5832.h5\n",
      "Loaded 543174 Markers\n",
      "Mean Depth: 1.3750\n",
      "Successfully saved 1 individuals to: ./Data/SA_1240kHDF5/Lovelock4.h5\n",
      "Successfully saved to ./Data/SA_1240kHDF5/Lovelock4.h5\n",
      "Loaded 696732 Markers\n",
      "Mean Depth: 1.6262\n",
      "Successfully saved 1 individuals to: ./Data/SA_1240kHDF5/Sumidouro4.h5\n",
      "Successfully saved to ./Data/SA_1240kHDF5/Sumidouro4.h5\n",
      "Loaded 1114912 Markers\n",
      "Mean Depth: 18.8313\n",
      "Successfully saved 1 individuals to: ./Data/SA_1240kHDF5/Sumidouro5.h5\n",
      "Successfully saved to ./Data/SA_1240kHDF5/Sumidouro5.h5\n",
      "Loaded 1008433 Markers\n",
      "Mean Depth: 2.7937\n",
      "Successfully saved 1 individuals to: ./Data/SA_1240kHDF5/Sumidouro6.h5\n",
      "Successfully saved to ./Data/SA_1240kHDF5/Sumidouro6.h5\n",
      "Loaded 476784 Markers\n",
      "Mean Depth: 1.3412\n",
      "Successfully saved 1 individuals to: ./Data/SA_1240kHDF5/Sumidouro7.h5\n",
      "Successfully saved to ./Data/SA_1240kHDF5/Sumidouro7.h5\n",
      "Loaded 862253 Markers\n",
      "Mean Depth: 1.9418\n",
      "Successfully saved 1 individuals to: ./Data/SA_1240kHDF5/19651.h5\n",
      "Successfully saved to ./Data/SA_1240kHDF5/19651.h5\n",
      "Loaded 1115133 Markers\n",
      "Mean Depth: 23.7814\n",
      "Successfully saved 1 individuals to: ./Data/SA_1240kHDF5/AHUR_2064.h5\n",
      "Successfully saved to ./Data/SA_1240kHDF5/AHUR_2064.h5\n",
      "Loaded 923233 Markers\n",
      "Mean Depth: 2.1474\n",
      "Successfully saved 1 individuals to: ./Data/SA_1240kHDF5/Lovelock1.h5\n",
      "Successfully saved to ./Data/SA_1240kHDF5/Lovelock1.h5\n",
      "Loaded 1115141 Markers\n",
      "Mean Depth: 18.0848\n",
      "Successfully saved 1 individuals to: ./Data/SA_1240kHDF5/Lovelock2.h5\n",
      "Successfully saved to ./Data/SA_1240kHDF5/Lovelock2.h5\n",
      "Loaded 1115211 Markers\n",
      "Mean Depth: 22.5161\n",
      "Successfully saved 1 individuals to: ./Data/SA_1240kHDF5/Lovelock3.h5\n",
      "Successfully saved to ./Data/SA_1240kHDF5/Lovelock3.h5\n",
      "Loaded 1034873 Markers\n",
      "Mean Depth: 3.4896\n",
      "Successfully saved 1 individuals to: ./Data/SA_1240kHDF5/Aconcagua.h5\n",
      "Successfully saved to ./Data/SA_1240kHDF5/Aconcagua.h5\n"
     ]
    }
   ],
   "source": [
    "base_folder=\"../fromDavid/\"\n",
    "out_folder=\"./Data/SA_1240kHDF5/\"\n",
    "\n",
    "iids = [\"A460\", \"Andaman\", \"5832\", \"Lovelock4\", \"Sumidouro4\",\"Sumidouro5\", \n",
    "        \"Sumidouro6\", \"Sumidouro7\", \"19651\", \"AHUR_2064\", \"Lovelock1\", \"Lovelock2\", \"Lovelock3\", \"Aconcagua\"]\n",
    "\n",
    "paths = [os.path.join(base_folder, iid + \"_final.bam.mpileup.cnts\") for iid in iids]\n",
    "paths_h5 = [os.path.join(out_folder, iid + \".h5\") for iid in iids]\n",
    "\n",
    "for i in range(len(paths)):\n",
    "    rc_to_hdf_1ind(paths[i], path_h5=paths_h5[i], iid=iids[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Do multiple individuals\n",
    "Did not work because different number of SNPs! Left here as a reminder!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Downsampled HDF5\n",
    "Idea: Have HDF5s with several individuals that are downsampled from one master Individual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def downsample_ad(ad, cov_frac):\n",
    "    \"\"\"Return downsampled Allele Depth\n",
    "    ad: [l,2] Array of Allele Depths\n",
    "    cov_frac: To which fraction to downsample\"\"\"\n",
    "    assert(np.min(ad)>=0)   # Sanity Check\n",
    "    ad = np.random.binomial(ad, cov_frac)\n",
    "    return ad\n",
    "    \n",
    "def rc_to_hdf_cov_ind(base_folder, iid, path_hdf5, down_sampling_covs):\n",
    "    \"\"\"Create HDF5 from one Individual, downsampled to various levels.\n",
    "    down_sampling_covs: Vector of coverages to downsamples to\"\"\"\n",
    "    ### Produce the Sample Names\n",
    "    samples = [f\"{c:.4f}\" for c in down_sampling_covs]\n",
    "    \n",
    "    ### Same as for 1 Individual, but as a Loop\n",
    "    path = os.path.join(base_folder, iid + \"_1240k_all.cnts\")\n",
    "    df_rc = get_df_from_rc(path)\n",
    "    df_rc = creat_count_col(df_rc)\n",
    "\n",
    "    ### Get Fields that are same for all Individuals (i.e. Array)\n",
    "    ref = df_rc[\"ref\"].values\n",
    "    alt = df_rc[\"alt\"].values\n",
    "    pos = df_rc[\"pos\"].values\n",
    "    ch = df_rc[\"chr\"].values\n",
    "\n",
    "    ### Get Fields that are a matrix\n",
    "    l = len(df_rc)\n",
    "    k = len(down_sampling_covs)\n",
    "    gt = -np.ones((l, k, 2), dtype=\"int8\")            # No genotypes filled in!\n",
    "\n",
    "    ad = df_rc[[\"ref_count\", \"alt_count\"]].values     # Extract Allele Depth Field\n",
    "    ads= [downsample_ad(ad, cov_frac) for cov_frac in down_sampling_covs]\n",
    "    ad = np.stack(ads, axis=1)          # Combine the allele Depths (along axis 1 for individuals)\n",
    "    \n",
    "    assert(np.shape(ad)==np.shape(gt)) # Sanity Check\n",
    "    save_hdf5(gt, ad, ref, alt, pos, ch, samples, path_hdf5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downsample IPK12/Stuttgart/Loschbour/Ust_Ishim Readcounts\n",
    "Comment out what is needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1115283 Markers\n",
      "Mean Depth: 19.5486\n",
      "Successfully saved 10 individuals to: ./Data/SA_1240kHDF5/Loschbour_downsample.h5\n"
     ]
    }
   ],
   "source": [
    "### IPK12\n",
    "#down_sampling_covs = np.geomspace(0.04, 1.0, 10)\n",
    "#rc_to_hdf_cov_ind(\"../fromDavid/\", \"IPK12\", path_hdf5=\"./Data/SA_1240kHDF5/IPK12_downsample.h5\", \n",
    "#                  down_sampling_covs=down_sampling_covs)\n",
    "\n",
    "#down_sampling_covs=np.geomspace(0.01, 1.0, 10)\n",
    "#rc_to_hdf_cov_ind(\"../fromDavid/\", \"Stuttgart\", path_hdf5=\"./Data/SA_1240kHDF5/Stuttgart_downsample.h5\", \n",
    "#                  down_sampling_covs=down_sampling_covs)\n",
    "\n",
    "down_sampling_covs=np.geomspace(0.01, 1.0, 10)\n",
    "rc_to_hdf_cov_ind(\"../fromDavid/\", \"Loschbour\", path_hdf5=\"./Data/SA_1240kHDF5/Loschbour_downsample.h5\", \n",
    "                  down_sampling_covs=down_sampling_covs)\n",
    "\n",
    "#down_sampling_covs=np.geomspace(0.005, 1.0, 10)\n",
    "#rc_to_hdf_cov_ind(\"../fromDavid/\", \"Ust_Ishim\", path_hdf5=\"./Data/SA_1240kHDF5/Ust_Ishim_downsample.h5\", \n",
    "#                  down_sampling_covs=down_sampling_covs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Downsampled HDF5 with Pseudohaploid Genotypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def downsample_ph(ad, cov_frac=1.0, mis_val=-1):\n",
    "    \"\"\"Downsample to pseudohaploid Genotypes\n",
    "    Input: Allele Depths [l,2], cov_frac: Target Coverage (1.0x nothing set to 0)\"\"\"\n",
    "    tot_cov = np.sum(ad, axis=1)\n",
    "    cov0 = (tot_cov ==0)\n",
    "    cov_mean = np.mean(tot_cov>0) # How many markers are covered at all\n",
    "    #print(cov_mean)\n",
    "    tot_cov[cov0]=1    # Fill up with fake value for now, will be replaced later!!\n",
    "    p = ad[:,1] / tot_cov  # Fraction of derived Read per Locus\n",
    "    \n",
    "    gt = np.random.binomial(n=1, p=p) # The Pseudohaploid Genotypes\n",
    "    gt = gt.astype(\"int8\")\n",
    "    \n",
    "    ### Do the downsampling (So that coverage + missing is right)\n",
    "    not_cov = np.random.random(size=len(gt)) > (cov_frac / cov_mean)  # Create missing mask\n",
    "    gt[not_cov] = mis_val # Set missing\n",
    "    gt[cov0] = mis_val # Set missing where no coverage\n",
    "    \n",
    "    ### Do the Duplication\n",
    "    gt = np.tile(gt, (2,1)).T # Create (l,2) Array\n",
    "    return gt\n",
    "\n",
    "def rc_to_hdf_cov_ind_ph(base_folder, iid, path_hdf5, \n",
    "                         down_sampling_covs, suffix=\"\"):\n",
    "    \"\"\"Create HDF5 from one Individual, downsampled to various levels.\n",
    "    down_sampling_covs: Vector of coverages to downsamples to.\n",
    "    suffix: Suffix to append to Individuals\"\"\"\n",
    "    ### Produce the Sample Names\n",
    "    samples = [f\"{c:.4f}{suffix}\" for c in down_sampling_covs]\n",
    "    \n",
    "    ### Same as for 1 Individual, but as a Loop\n",
    "    path = os.path.join(base_folder, iid + \"_1240k_all.cnts\")\n",
    "    df_rc = get_df_from_rc(path)\n",
    "    df_rc = creat_count_col(df_rc)\n",
    "\n",
    "    ### Get Fields that are same for all Individuals (i.e. Array)\n",
    "    ref = df_rc[\"ref\"].values\n",
    "    alt = df_rc[\"alt\"].values\n",
    "    pos = df_rc[\"pos\"].values\n",
    "    ch = df_rc[\"chr\"].values\n",
    "\n",
    "    ### Get Fields that are a matrix\n",
    "    l = len(df_rc)\n",
    "    k = len(down_sampling_covs)\n",
    "    \n",
    "    #gts = np.zeros((l, k, 2), dtype=\"int8\")\n",
    "    #gt = -np.ones((l, k, 2), dtype=\"int8\")           # No genotypes filled in!\n",
    "\n",
    "    ad = df_rc[[\"ref_count\", \"alt_count\"]].values     # Extract Allele Depth Field\n",
    "    \n",
    "    gts= [downsample_ph(ad, cov_frac) for cov_frac in down_sampling_covs]\n",
    "    gts = np.stack(gts, axis=1)            # Combine the allele Depths (along axis 1 for individuals)\n",
    "    ads = np.tile(ad[:,None,:], (1, k, 1)) # Tile the Allele depths as well\n",
    "    \n",
    "    assert(np.shape(ads)==np.shape(gts)) # Sanity Check\n",
    "    save_hdf5(gts, ads, ref, alt, pos, ch, samples, path_hdf5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1115283 Markers\n",
      "Mean Depth: 19.5486\n",
      "Successfully saved 8 individuals to: ./Data/SA_1240kHDF5/Loschbour_downsample_ph.h5\n"
     ]
    }
   ],
   "source": [
    "down_sampling_covs = np.linspace(0.3, 1.0, 8)\n",
    "rc_to_hdf_cov_ind_ph(\"../fromDavid/\", \"Loschbour\", path_hdf5=\"./Data/SA_1240kHDF5/Loschbour_downsample_ph.h5\", \n",
    "                     down_sampling_covs=down_sampling_covs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1115215 Markers\n",
      "Mean Depth: 18.2127\n",
      "Successfully saved 8 individuals to: ./Data/SA_1240kHDF5/Stuttgart_downsample_ph.h5\n"
     ]
    }
   ],
   "source": [
    "down_sampling_covs = np.linspace(0.3, 1.0, 8)\n",
    "rc_to_hdf_cov_ind_ph(\"../fromDavid/\", \"Stuttgart\", path_hdf5=\"./Data/SA_1240kHDF5/Stuttgart_downsample_ph.h5\", \n",
    "                     down_sampling_covs=down_sampling_covs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1115315 Markers\n",
      "Mean Depth: 40.7515\n",
      "Successfully saved 8 individuals to: ./Data/SA_1240kHDF5/Ust_Ishim_downsample_ph.h5\n"
     ]
    }
   ],
   "source": [
    "down_sampling_covs = np.linspace(0.2, 1.0, 9)\n",
    "rc_to_hdf_cov_ind_ph(\"../fromDavid/\", \"Ust_Ishim\", path_hdf5=\"./Data/SA_1240kHDF5/Ust_Ishim_downsample_ph.h5\", \n",
    "                     down_sampling_covs=down_sampling_covs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1114250 Markers\n",
      "Mean Depth: 10.4390\n",
      "Successfully saved 8 individuals to: ./Data/SA_1240kHDF5/IPK12_downsample_ph0.h5\n"
     ]
    }
   ],
   "source": [
    "down_sampling_covs = np.linspace(0.3, 1.0, 8)\n",
    "rc_to_hdf_cov_ind_ph(\"../fromDavid/\", \"IPK12\", path_hdf5=\"./Data/SA_1240kHDF5/IPK12_downsample_ph0.h5\", \n",
    "                     down_sampling_covs=down_sampling_covs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Do 10 Replicate of Downsampling Ush Ishim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reps = 20\n",
    "down_sampling_covs = np.linspace(0.2, 1.0, 9)\n",
    "\n",
    "for r in range(reps):  \n",
    "    path_save = \"./Data/SA_1240kHDF5/Ust_Ishim_rep/downsample_ph_r\" + str(r) + \".h5\"\n",
    "    \n",
    "    rc_to_hdf_cov_ind_ph(\"../fromDavid/\", \"Ust_Ishim\", path_hdf5=path_save, \n",
    "                         down_sampling_covs=down_sampling_covs, suffix=f\"_r{r}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Area 51"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "down_sampling_covs = np.linspace(0.3, 1.0, 8)\n",
    "down_sampling_covs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 2],\n",
       "       [0, 2],\n",
       "       [2, 2]])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ad = np.array([[0,2],[0,2],[2,2]])\n",
    "ad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 2)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(ad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 5, 2)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = np.tile(ad[:,None,:], (1,5,1))\n",
    "np.shape(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 2],\n",
       "       [0, 2],\n",
       "       [2, 2]])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b[:,2,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test whether Loading works properly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded HDF5\n",
      "Loaded 1115283 variants\n",
      "Loaded 10 individuals\n",
      "['AD', 'GT']\n",
      "['ALT', 'CHROM', 'POS', 'REF']\n"
     ]
    }
   ],
   "source": [
    "path_load = \"./Data/SA_1240kHDF5/Loschbour_downsample.h5\"\n",
    "#path_load = \"./Data/SA_1240kHDF5/IPK12_downsample_ph.h5\"\n",
    "f = h5py.File(path_load, \"r\") # Load for Sanity Check. See below!\n",
    "        \n",
    "print(\"Loaded HDF5\")\n",
    "print(\"Loaded %i variants\" % np.shape(f[\"calldata/GT\"])[0])\n",
    "print(\"Loaded %i individuals\" % np.shape(f[\"calldata/GT\"])[1])\n",
    "print(list(f[\"calldata\"].keys()))\n",
    "print(list(f[\"variants\"].keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['0.0100', '0.0167', '0.0278', '0.0464', '0.0774', '0.1292',\n",
       "       '0.2154', '0.3594', '0.5995', '1.0000'], dtype=object)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f[\"samples\"][:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1114250, 10, 2)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(f[\"calldata/GT\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "covs_gt = [np.mean(f[\"calldata/GT\"][:,i,:]>-1) for i in range(len(f[\"samples\"]))]\n",
    "covs_gt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.value_counts(f[\"variants/CHROM\"][:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=int32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f[\"variants/CHROM\"][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "covs_ad = [np.mean(f[\"calldata/AD\"][:,i,:]) * 2 for i in range(len(f[\"samples\"]))]\n",
    "covs_ad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4175595781916087"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "10.438989454790217 * 0.04"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f[\"calldata/AD\"][:10,1,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f[\"calldata/GT\"][:10,5,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.value_counts(f[\"variants\"][\"CHROM\"][:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([], dtype=int64)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(f[\"samples\"][:]==\"MA577_1240\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "iid=\"MA577_1240\"\n",
    "\n",
    "samples = f[\"samples\"][:]\n",
    "assert(len(samples) == np.shape(f[\"calldata/GT\"])[1])  # Sanity Check\n",
    "\n",
    "id_obs = np.where(samples == iid)[0]\n",
    "if len(id_obs) == 0:\n",
    "    raise RuntimeError(f\"Individual {iid} not found in Samples Field\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id_obs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['IPY10', 'IPK12', 'MA577', '894', '895'], dtype=object)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f[\"samples\"][:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'IPY10', 'IPK12', 'MA577', '894', '895'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1114799 Markers\n"
     ]
    }
   ],
   "source": [
    "df_rc= get_df_from_rc(path_ind = \"../fromDavid/IPY10_1240k_all.cnts\")\n",
    "df_rc = creat_count_col(df_rc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rc[df_rc[\"chr\"]==10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1114250 Markers\n",
      "Mean Depth: 10.4390\n"
     ]
    }
   ],
   "source": [
    "df_t = get_df_from_rc(path_ind='../fromDavid/IPK12_1240k_all.cnts')\n",
    "df_t = creat_count_col(df_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_cov = (df_t[\"ref_count\"] + df_t[\"alt_count\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_cov.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1115315 Markers\n"
     ]
    }
   ],
   "source": [
    "df_t=get_df_from_rc(\"../fromDavid/Ust_Ishim_1240k_all.cnts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2     95427\n",
       "1     90211\n",
       "3     78609\n",
       "6     76778\n",
       "5     70297\n",
       "4     69671\n",
       "8     62024\n",
       "7     60650\n",
       "10    59583\n",
       "11    55558\n",
       "12    54584\n",
       "9     51354\n",
       "13    39532\n",
       "14    36785\n",
       "16    34996\n",
       "15    34886\n",
       "18    34327\n",
       "17    29665\n",
       "20    29439\n",
       "19    18697\n",
       "21    16250\n",
       "22    15992\n",
       "Name: chr, dtype: int64"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_t[\"chr\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>chr</th>\n",
       "      <th>pos</th>\n",
       "      <th>ref</th>\n",
       "      <th>alt</th>\n",
       "      <th>A</th>\n",
       "      <th>G</th>\n",
       "      <th>C</th>\n",
       "      <th>T</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>752566</td>\n",
       "      <td>G</td>\n",
       "      <td>A</td>\n",
       "      <td>36</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>776546</td>\n",
       "      <td>A</td>\n",
       "      <td>G</td>\n",
       "      <td>43</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>832918</td>\n",
       "      <td>T</td>\n",
       "      <td>C</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>842013</td>\n",
       "      <td>T</td>\n",
       "      <td>G</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>846864</td>\n",
       "      <td>G</td>\n",
       "      <td>C</td>\n",
       "      <td>0</td>\n",
       "      <td>26</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>869303</td>\n",
       "      <td>C</td>\n",
       "      <td>T</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>891021</td>\n",
       "      <td>G</td>\n",
       "      <td>A</td>\n",
       "      <td>39</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>896271</td>\n",
       "      <td>C</td>\n",
       "      <td>T</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>903426</td>\n",
       "      <td>C</td>\n",
       "      <td>T</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>914852</td>\n",
       "      <td>G</td>\n",
       "      <td>C</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   chr     pos ref alt   A   G   C   T\n",
       "0    1  752566   G   A  36  23   0   0\n",
       "1    1  776546   A   G  43   0   0   0\n",
       "2    1  832918   T   C   0   0   0  54\n",
       "3    1  842013   T   G   0   0   0  32\n",
       "4    1  846864   G   C   0  26   0   0\n",
       "5    1  869303   C   T   0   0  20   3\n",
       "6    1  891021   G   A  39   0   0   0\n",
       "7    1  896271   C   T   0   0   5   3\n",
       "8    1  903426   C   T   0   0  32   0\n",
       "9    1  914852   G   C   0   8  17   0"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_t.head(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
