{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modify the Sardinia HDF5 (in particular, the HO samples)\n",
    "Add updated IIDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "midway2-0406.rcc.local\n",
      "Midway jnovmbre partition detected.\n",
      "/project2/jnovembre/hringbauer/HAPSBURG\n",
      "CPU Count: 28\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os as os\n",
    "import sys as sys\n",
    "import multiprocessing as mp\n",
    "import pandas as pd\n",
    "import socket\n",
    "import h5py\n",
    "\n",
    "### Pick the right path (whether on cluster or at home)\n",
    "socket_name = socket.gethostname()\n",
    "print(socket_name)\n",
    "if socket_name == \"VioletQueen\":\n",
    "    path = \"/home/harald/git/HAPSBURG/\"   # The Path on Harald's machine\n",
    "elif socket_name.startswith(\"midway2\"):\n",
    "    print(\"Midway jnovmbre partition detected.\")\n",
    "    path = \"/project2/jnovembre/hringbauer/HAPSBURG/\"  # The Path on Midway Cluster\n",
    "else: \n",
    "    raise RuntimeWarning(\"Not compatible machine. Check!!\")\n",
    "    \n",
    "os.chdir(path)  # Set the right Path (in line with Atom default)\n",
    "print(os.getcwd()) # Show the current working directory. Should be HAPSBURG/Notebooks/ParallelRuns\n",
    "print(f\"CPU Count: {mp.cpu_count()}\")\n",
    "\n",
    "\n",
    "sys.path.insert(0,\"./package/\")  # hack to get local package first in path\n",
    "from hapsburg.PackagesSupport.hapsburg_run import hapsb_chrom, hapsb_ind\n",
    "#from hapsburg.PackagesSupport.parallel_runs.helper_functions import prepare_path, multi_run, combine_individual_data\n",
    "from hapsburg.PackagesSupport.pp_individual_roh_csvs import create_combined_ROH_df, give_iid_paths, pp_individual_roh\n",
    "from hapsburg.PackagesSupport.h5_python.h5_functions import save_data_h5\n",
    "\n",
    "#from createMosaicsMulti import Mosaic_1000G_Multi  # Import the object that can create the Multiruns\n",
    "\n",
    "### Load the Meta File\n",
    "meta_path = \"./Data/Marcus2019_1240k/meta_rev_unique_ids.csv\"\n",
    "meta_df = pd.read_csv(meta_path)\n",
    "mod_df = meta_df[1098:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_h5(path=\"./Data/Marcus2019_1240k/mod_reich_sardinia_ancients_rev_mrg_dedup_3trm_anno.h5\"):\n",
    "    \"\"\"Load and return the HDF5 File from Path\"\"\"\n",
    "    f = h5py.File(path, \"r\")  # Load for Sanity Check. See below!\n",
    "    print(\"\\nLoaded %i variants\" % np.shape(f[\"calldata/GT\"])[0])\n",
    "    print(\"Loaded %i individuals\" % np.shape(f[\"calldata/GT\"])[1])\n",
    "    # print(list(f[\"calldata\"].keys()))\n",
    "    # print(list(f[\"variants\"].keys()))\n",
    "    print(f\"HDF5 loaded from {path}\")\n",
    "    return f\n",
    "\n",
    "def load_iid_df(meta_path_targets = \"./Data/Marcus2019_1240k/meta_rev_unique_ids.csv\"):\n",
    "    \"\"\"Load IID df as well dictionary to translate\"\"\"\n",
    "    df = pd.read_csv(meta_path_targets)\n",
    "    dct_iids = dict(zip(df[\"iid\"], df[\"full_iid\"]))\n",
    "    return df, dct_iids\n",
    "\n",
    "### Loading f for checking\n",
    "#f = load_h5(path=\"./Data/Marcus2019_1240k/mod_reich_sardinia_ancients_rev_mrg_dedup_3trm_anno.h5\")\n",
    "#len(set(f[\"samples\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First Copy over the original \n",
    "(runs shell command)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_h5 = \"./Data/Marcus2019_1240k/mod_reich_sardinia_ancients_rev_mrg_dedup_3trm_anno.h5\"\n",
    "path_h5_mod = \"./Data/Marcus2019_1240k/sardinia_hapsburg.h5\"\n",
    "!cp $path_h5 $path_h5_mod"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now fill in new sample names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loaded 1145647 variants\n",
      "Loaded 4616 individuals\n",
      "HDF5 loaded from ./Data/Marcus2019_1240k/mod_reich_sardinia_ancients_rev_mrg_dedup_3trm_anno.h5\n",
      "Successfully written to ./Data/Marcus2019_1240k/sardinia_hapsburg.h5\n"
     ]
    }
   ],
   "source": [
    "### Load the Metadata\n",
    "df, dct_iid = load_iid_df()\n",
    "\n",
    "### Sanity Check\n",
    "f = load_h5(path=path_h5)\n",
    "assert((f[\"samples\"][:] == df[\"full_iid\"]).all())\n",
    "f.close()\n",
    "\n",
    "### Now write it over\n",
    "f1 = h5py.File(path_h5_mod, 'r+')     # open the file\n",
    "data = f1['samples']                  # load the data\n",
    "data[...] = df[\"iid\"]                 # assign new values to data\n",
    "f1.close()                            # close the file\n",
    "\n",
    "print(f\"Successfully written to {path_h5_mod}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Delete modern Sardinians"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded HDF5\n",
      "Loaded 1145647 variants\n",
      "Loaded 4616 individuals\n",
      "['calldata', 'samples', 'variants']\n",
      "['AD', 'GT']\n",
      "['AA', 'AF', 'AFR_AF', 'ALT', 'AMR_AF', 'CHROM', 'EAS_AF', 'EUR_AF', 'ID', 'POS', 'REF', 'SAS_AF']\n",
      "(1145647, 4616, 2)\n"
     ]
    }
   ],
   "source": [
    "path = \"./Data/Marcus2019_1240k/sardinia_hapsburg.h5\"\n",
    "\n",
    "with h5py.File(path, \"r\") as f: # Load for Sanity Check. See below!     \n",
    "    print(\"Loaded HDF5\")\n",
    "    print(\"Loaded %i variants\" % np.shape(f[\"calldata/AD\"])[0])\n",
    "    print(\"Loaded %i individuals\" % np.shape(f[\"calldata/AD\"])[1])\n",
    "    print(list(f))\n",
    "    print(list(f[\"calldata\"].keys()))\n",
    "    print(list(f[\"variants\"].keys()))\n",
    "    print(np.shape(f[\"calldata/GT\"]))\n",
    "    samples = f[\"samples\"][:]\n",
    "    \n",
    "df = pd.read_csv(\"./Data/Marcus2019_1240k/meta_rev_final.csv\")\n",
    "assert(len(df)==len(samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered to 3039/4616 non modern Sardinians\n"
     ]
    }
   ],
   "source": [
    "idx = df[\"study\"]==\"Chiang et al. 2016\"\n",
    "df1 = df[~idx].copy()\n",
    "print(f\"Filtered to {len(df1)}/{len(df)} non modern Sardinians\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save downsampled Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded HDF5\n",
      "Loaded 1145647 variants\n",
      "Loaded 4616 individuals\n",
      "['calldata', 'samples', 'variants']\n",
      "['AD', 'GT']\n",
      "['AA', 'AF', 'AFR_AF', 'ALT', 'AMR_AF', 'CHROM', 'EAS_AF', 'EUR_AF', 'ID', 'POS', 'REF', 'SAS_AF']\n",
      "Saving 3039/4616 Samples\n",
      "Successfully saved 3039 individuals to: ./Data/Marcus2019_1240k/sardinia_hapsburg_nomodsards.h5\n",
      "CPU times: user 14min 59s, sys: 1min 18s, total: 16min 17s\n",
      "Wall time: 16min 20s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "df = pd.read_csv(\"./Data/Marcus2019_1240k/meta_rev_final.csv\")\n",
    "\n",
    "with h5py.File(\"./Data/Marcus2019_1240k/sardinia_hapsburg.h5\", \"r\") as f: # Load for Sanity Check. See below!     \n",
    "    print(\"Loaded HDF5\")\n",
    "    print(\"Loaded %i variants\" % np.shape(f[\"calldata/AD\"])[0])\n",
    "    print(\"Loaded %i individuals\" % np.shape(f[\"calldata/AD\"])[1])\n",
    "    print(list(f))\n",
    "    print(list(f[\"calldata\"].keys()))\n",
    "    print(list(f[\"variants\"].keys()))\n",
    "    samples = f[\"samples\"][:]\n",
    "    assert(len(df)==len(samples))\n",
    "    ### Downsample to no modern Sardinians\n",
    "    idx = ~(df[\"study\"]==\"Chiang et al. 2016\")\n",
    "    print(f\"Saving {np.sum(idx)}/{len(idx)} Samples\")\n",
    "\n",
    "    save_data_h5(gt=f[\"calldata/GT\"][:][:,idx,:].astype(\"int8\"),\n",
    "                 ad=f[\"calldata/AD\"][:][:,idx,:].astype(\"int8\"),\n",
    "                 ref=f[\"variants/REF\"][:],\n",
    "                 alt=f[\"variants/ALT\"][:],\n",
    "                 pos=f[\"variants/POS\"][:],\n",
    "                 rec=[],\n",
    "                 samples=f[\"samples\"][:][idx],\n",
    "                 path=\"./Data/Marcus2019_1240k/sardinia_hapsburg_nomodsards.h5\",\n",
    "                 gp=[],\n",
    "                 af=f[\"variants/AF\"][:],\n",
    "                 compression='gzip',\n",
    "                 ad_group=True,\n",
    "                 gt_type='int8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "svmem(total=67099480064, available=57126854656, percent=14.9, used=6342938624, free=57400274944, active=4076462080, inactive=434561024, buffers=0, cached=3356266496, shared=3220881408, slab=285446144)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import psutil\n",
    "psutil.virtual_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "awk: cmd. line:1: (FILENAME=- FNR=3) fatal: division by zero attempted\n"
     ]
    }
   ],
   "source": [
    "!free | awk 'FNR == 3 {print $3/($3+$4)*100}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Area 51"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loaded 1145647 variants\n",
      "Loaded 4616 individuals\n",
      "HDF5 loaded from ./Data/Marcus2019_1240k/sardinia_hapsburg.h5\n"
     ]
    }
   ],
   "source": [
    "f = load_h5(path=path_h5_mod)\n",
    "samples = f[\"samples\"][:]\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hadza_0', 'Hadza_1', 'Hadza_2', 'Hadza_3', 'Hadza_4']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[s for s in samples if \"Hadza\" in s]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test no modern Sardinian hdf5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded HDF5\n",
      "Loaded 1145647 variants\n",
      "Loaded 3039 individuals\n",
      "['calldata', 'samples', 'variants']\n",
      "['AD', 'GT']\n",
      "['AF_ALL', 'ALT', 'POS', 'REF']\n",
      "(1145647, 3039, 2)\n"
     ]
    }
   ],
   "source": [
    "path = \"./Data/Marcus2019_1240k/sardinia_hapsburg_nomodsards.h5\"\n",
    "\n",
    "with h5py.File(path, \"r\") as f: # Load for Sanity Check. See below!     \n",
    "    print(\"Loaded HDF5\")\n",
    "    print(\"Loaded %i variants\" % np.shape(f[\"calldata/AD\"])[0])\n",
    "    print(\"Loaded %i individuals\" % np.shape(f[\"calldata/AD\"])[1])\n",
    "    print(list(f))\n",
    "    print(list(f[\"calldata\"].keys()))\n",
    "    print(list(f[\"variants\"].keys()))\n",
    "    print(np.shape(f[\"calldata/GT\"]))\n",
    "    samples = f[\"samples\"][:]\n",
    "    gt = f[\"calldata/GT\"][:10]\n",
    "    ad = f[\"calldata/AD\"][:10]\n",
    "    \n",
    "df = pd.read_csv(\"./Data/Marcus2019_1240k/meta_rev_final_nomod.tsv\", sep=\"\\t\")\n",
    "assert(len(df)==len(samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([b'', b'', b'', ..., b'', b'', b''], dtype=object)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
