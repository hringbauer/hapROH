{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook to call ROH in parallel within HO origin Individuals\n",
    "Notebooks that import the code for the calling ROHs on diploid genotype individuals, and then a function to parallelize it.\n",
    "\n",
    "Very similar to parallel_mosaic_callroh.ipynb\n",
    "\n",
    "\n",
    "@Author: Harald Ringbauer, September 2019\n",
    "All rights reserved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "midway2-0401.rcc.local\n",
      "Midway jnovmbre partition detected.\n",
      "/project2/jnovembre/hringbauer/HAPSBURG\n",
      "CPU Count: 28\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os as os\n",
    "import sys as sys\n",
    "import multiprocessing as mp\n",
    "import pandas as pd\n",
    "import socket\n",
    "\n",
    "### Pick the right path (whether on cluster or at home)\n",
    "socket_name = socket.gethostname()\n",
    "print(socket_name)\n",
    "if socket_name == \"VioletQueen\":\n",
    "    path = \"/home/harald/git/HAPSBURG/\"   # The Path on Harald's machine\n",
    "elif socket_name.startswith(\"midway2\"):\n",
    "    print(\"Midway jnovmbre partition detected.\")\n",
    "    path = \"/project2/jnovembre/hringbauer/HAPSBURG/\"  # The Path on Midway Cluster\n",
    "else: \n",
    "    raise RuntimeWarning(\"Not compatible machine. Check!!\")\n",
    "    \n",
    "os.chdir(path)  # Set the right Path (in line with Atom default)\n",
    "print(os.getcwd()) # Show the current working directory. Should be HAPSBURG/Notebooks/ParallelRuns\n",
    "print(f\"CPU Count: {mp.cpu_count()}\")\n",
    "\n",
    "\n",
    "sys.path.append(\"./package/hapsburg/\")  # Since now we are in the Root Directory\n",
    "from hmm_inference import HMM_Analyze   # Do not move. Should be after sys.path..\n",
    "from PackagesSupport.pp_individual_roh_csvs import pp_individual_roh\n",
    "#from createMosaicsMulti import Mosaic_1000G_Multi  # Import the object that can create the Multiruns\n",
    "\n",
    "### Load the Meta File\n",
    "meta_path = \"./Data/Marcus2019_1240k/meta_rev_unique_ids.csv\"\n",
    "meta_df = pd.read_csv(meta_path)\n",
    "mod_df = meta_df[1098:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_path(path_output, iid, ch, prefix_out, logfile=True, output=False):\n",
    "    \"\"\"Prepare the path and pipe printing for one Individual\n",
    "    logfile: Whether to pipe output to log-file\"\"\"   \n",
    "    #if not os.path.exists(path_output):\n",
    "    #        raise RuntimeError(f\"Path {path_output} not Found. Check!\")\n",
    "    path_log = os.path.join(path_output, str(iid), \"chr\"+str(ch), prefix_out, \"\")      \n",
    "    #path_log =  path_output + str(iid) + \"/chr\" + str(ch) + \"/\" + prefix_out\n",
    "    \n",
    "    if not os.path.exists(path_log):\n",
    "        if output==True:\n",
    "            print(f\"Creating {path_log}...\")\n",
    "        os.makedirs(path_log)\n",
    "    \n",
    "    if logfile == True:\n",
    "        path_log = path_log + \"hmm_run_log.txt\"\n",
    "        if output==True:\n",
    "            print(f\"Set Output Log path: {path_log}\")\n",
    "        sys.stdout = open(path_log, 'w') \n",
    "    \n",
    "def analyze_chromosome_gt(iid, ch=3, n_ref=503, save=True, save_fp=False, exclude_pops=[\"TSI\", ], \n",
    "                          base_out_folder=\"./Empirical/HO/\", prefix_out=\"gt/\",\n",
    "                          roh_in=100, roh_out=100, roh_jump=385, e_rate=0.01, e_rate_ref=0.001, \n",
    "                          max_gap=0, logfile=True):\n",
    "    \"\"\"Run the analysis for one individual and chromosome on readcount data\n",
    "    Wrapper for HMM Class. Takes 13 Parameters\"\"\"\n",
    "    \n",
    "    ### The folder on what to run the Data on (Permanently set here to fixed loaction)\n",
    "    h5_path_targets = \"./Data/Marcus2019_1240k/mod_reich_sardinia_ancients_rev_mrg_dedup_3trm_anno.h5\"\n",
    "    meta_path_targets = \"./Data/Marcus2019_1240k/meta_rev_unique_ids.csv\"  ### Path with the unique IDs per Modern Group\n",
    "    \n",
    "    ### Create Folder if needed, and pipe output if wanted\n",
    "    prepare_path(base_out_folder, iid, ch, prefix_out, logfile=logfile)\n",
    "    \n",
    "    hmm = HMM_Analyze(cython=2, p_model=\"SardHDF5\", e_model=\"diploid_gt\", post_model=\"Standard\",\n",
    "                      manual_load=True, save=save, save_fp=save_fp)\n",
    "\n",
    "    # Load and prepare the pre-processing Model\n",
    "    hmm.load_preprocessing_model()              # Load the preprocessing Model\n",
    "    hmm.p_obj.set_params(readcounts = False, destroy_phase=False,\n",
    "                prefix_out_data=prefix_out, excluded=exclude_pops, base_out_folder=base_out_folder,\n",
    "                h5_path_targets = h5_path_targets, meta_path_targets=meta_path_targets)    \n",
    "    \n",
    "    ### DELETE when run for with European Reference!!\n",
    "    hmm.p_obj.set_params(h5_path1000g = \"./Data/1000Genomes/HDF5/1240kHDF5/all1240/chr\", \n",
    "                         meta_path_ref = \"./Data/1000Genomes/Individuals/meta_df_all.csv\")\n",
    "    \n",
    "    hmm.load_data(iid=iid, ch=ch, n_ref=n_ref)  # Load the actual Data\n",
    "    hmm.load_secondary_objects()\n",
    "\n",
    "    ### Set the Parameters\n",
    "    hmm.e_obj.set_params(e_rate = e_rate, e_rate_ref = e_rate_ref)\n",
    "    hmm.t_obj.set_params(roh_in=roh_in, roh_out=roh_out, roh_jump=roh_jump)\n",
    "    hmm.post_obj.set_params(max_gap=max_gap)\n",
    "    \n",
    "    #hmm.calc_viterbi_path(save=save)           # Calculate the Viterbi Path.\n",
    "    hmm.calc_posterior(save=save)              # Calculate the Posterior.\n",
    "    hmm.post_processing(save=save)             # Do the Post-Processing.\n",
    "\n",
    "    \n",
    "#########################################################\n",
    "def combine_individual_data(base_path, iid, delete=False, chs=range(1,23), prefix_out=\"\"):\n",
    "    \"\"\"Function to merge data from one Individual Analysis (all Chromosome)\n",
    "    chs: Which Chromosomes to combine\"\n",
    "    delete: Whether to delete individual folder and contents after combining.\"\"\"\n",
    "    \n",
    "    full_df_vec =[]  # The full dataframe of inferred ROH blocks\n",
    "    \n",
    "    ### Walk through Chromosomes and combine the Dataframes\n",
    "    for ch in chs:\n",
    "        path_roh = os.path.join(base_path, str(iid), \"chr\"+str(ch), prefix_out, \"roh.csv\") \n",
    "        df_temp = pd.read_csv(path_roh, sep=\",\")\n",
    "        full_df_vec.append(df_temp)\n",
    "        \n",
    "    full_df = pd.concat(full_df_vec)\n",
    "        \n",
    "    ### Save to Path:\n",
    "    path_save = os.path.join(base_path, str(iid) + \"_roh_full.csv\")\n",
    "    full_df.to_csv(path_save, index=False)\n",
    "    \n",
    "    ### Delete files in folder if need\n",
    "    if delete == True:\n",
    "        for ch in chs:\n",
    "            path_folder = os.path.join(base_path, str(iid), \"chr\"+str(ch), prefix_out, \"\") \n",
    "            \n",
    "            for root, _, files in os.walk(path_folder):\n",
    "                for file in files:\n",
    "                    os.remove(os.path.join(root, file))\n",
    "            os.rmdir(path_folder) # Remove the Chromosome Folders\n",
    "        os.rmdir(os.path.join(base_path, str(iid), \"\"))  # Remove the Individual Folder\n",
    "    \n",
    "    return full_df\n",
    "                             \n",
    "#########################################################\n",
    "def analyze_individual_ho(iid, chs=range(1,23), n_ref=2504, save=True, save_fp=False, exclude_pops=[], \n",
    "                          base_out_folder=\"./Empirical/HO/\", prefix_out=\"\",\n",
    "                          roh_in=100, roh_out=100, roh_jump=300, e_rate=0.001, \n",
    "                          e_rate_ref=0.001, max_gap=0, logfile=True, output=True, processes=5, delete=True):\n",
    "    \"\"\"Analyze a full single individual in a parallelized fasion. Run all Chromosome analyses in parallel\n",
    "    Wrapper for analyze_chromosome_gt.\n",
    "    logfile: Whether to use a logfile\n",
    "    output: Whether to print general Output\"\"\"\n",
    "                            \n",
    "    if output == True:\n",
    "        print(f\"Doing Individual {iid}...\")\n",
    "    \n",
    "    ### Prepare the Parameters for that Indivdiual\n",
    "    prms = [[iid, ch, n_ref, save, save_fp, exclude_pops, base_out_folder, prefix_out,\n",
    "         roh_in, roh_out, roh_jump, e_rate, e_rate_ref, max_gap, logfile] for ch in chs] \n",
    "                            \n",
    "    ### Run the analysis in parallel\n",
    "    multi_run(analyze_chromosome_gt, prms, processes = processes)\n",
    "                            \n",
    "    ### Merge results for that Individual\n",
    "    combine_individual_data(base_out_folder, iid=iid, delete=delete, chs=chs)\n",
    "                            \n",
    "    return\n",
    "        \n",
    "#########################################################\n",
    "#########################################################\n",
    "    \n",
    "def multi_run(fun, prms, processes = 4):\n",
    "    \"\"\"Implementation of running in Parallel.\n",
    "    fun: Function\n",
    "    prms: The Parameter Files\n",
    "    processes: How many Processes to use\"\"\"\n",
    "    print(f\"Running {len(prms)} jobs in parallel.\")\n",
    "    \n",
    "    with mp.Pool(processes = processes) as pool:\n",
    "            results = pool.starmap(fun, prms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyze HO Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze a single Individual\n",
    "For reanalysis with delete=True to plot that indivdual / further analysis of posterior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doing Individual Croatian_5...\n",
      "Running 22 jobs in parallel.\n",
      "CPU times: user 409 ms, sys: 116 ms, total: 525 ms\n",
      "Wall time: 5min 46s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "analyze_individual_ho(iid=\"Croatian_5\", chs=range(1,23), processes=6, delete=False, logfile=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run a whole HO Population or Range of HO individuals\n",
    "TODO: Update to newer function for the trun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Write the Command for iid\n",
    "def give_iids_populations_ho(pop):\n",
    "    \"\"\"Return all IIDs of Population pop in meta_df (in Lazaridis HO paper)\"\"\"\n",
    "    ho_df = meta_df[meta_df[\"study\"]==\"Lazaridis et al. 2014\"]\n",
    "    iids = ho_df[\"iid\"][meta_df[\"clst\"]==pop]\n",
    "    assert(len(iids)>0)\n",
    "    return iids.values\n",
    "\n",
    "def give_ho_iids_all():\n",
    "    \"\"\"Return individual IIDs of all HO samples\"\"\"\n",
    "    ho_df = meta_df[meta_df[\"study\"]==\"Lazaridis et al. 2014\"]\n",
    "    iids = ho_df[\"iid\"].values  # Extract Individuals\n",
    "    return iids\n",
    "\n",
    "def run_ho_pops(pops, chs=range(1,23), delete=True, processes=5, base_out_folder=\"./Empirical/HO/\"):\n",
    "    \"\"\"Run HAPSBURG on all Individuals of HO pops\"\"\"\n",
    "    for pop in pops:\n",
    "        iids = give_iids_populations_ho(pop)\n",
    "        for iid in iids:\n",
    "            analyze_individual_ho(iid=iid, chs=chs, processes=processes, delete=delete)\n",
    "                   \n",
    "def run_ho_inds(ind_range=[], chs=range(1,23), delete=True, processes=5, base_out_folder=\"./Empirical/HO/\"):\n",
    "    \"\"\"Run batches of HO Individuals, 1 Individual a time (parallelized)\"\"\"\n",
    "    iids = give_ho_iids_all()\n",
    "    iids = iids[ind_range]\n",
    "    for iid in iids:\n",
    "        analyze_individual_ho(iid=iid, chs=chs, processes=processes, delete=delete, base_out_folder=base_out_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doing Individual Yi_0...\n",
      "Running 1 jobs in parallel.\n"
     ]
    }
   ],
   "source": [
    "run_ho_pops(pops=[\"Yi\",], chs=range(1,2), delete=False, processes=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create HO Analysis Data. Run in batches of ind_range (to not submit everything at once)\n",
    "This is the cell that does the final data analysis\n",
    "\n",
    "Analysis counter: Done until 130 (Python Indexing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "run_ho_inds(ind_range=range(30,130), chs=range(1,23), delete=True, processes=6, base_out_folder=\"./Empirical/HO/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Postprocess all Individuals into one Dataframe\n",
    "1) Get all Individual IIDs\n",
    "2) Combine results into one Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1941 HO Individuals\n"
     ]
    }
   ],
   "source": [
    "### Get all IIDs\n",
    "iids = give_ho_iids_all()\n",
    "print(f\"Loaded {len(iids)} HO Individuals\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1941 / 4616 Individuals from Meta\n",
      "Saved to: ./Empirical/HO/CombinedROH/combined_roh05.csv\n",
      "CPU times: user 3min 52s, sys: 460 ms, total: 3min 52s\n",
      "Wall time: 4min 2s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df1 = pp_individual_roh(iids[:], meta_path=meta_path, base_folder=\"./Empirical/HO/\",\n",
    "                        save_path=\"./Empirical/HO/CombinedROH/combined_roh05.csv\", \n",
    "                        output=False, min_cm=[4,8,12,20], snp_cm=50, \n",
    "                        gap=0.5, min_len1=2, min_len2=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>iid</th>\n",
       "      <th>pop</th>\n",
       "      <th>max_roh</th>\n",
       "      <th>sum_roh&gt;4</th>\n",
       "      <th>n_roh&gt;4</th>\n",
       "      <th>sum_roh&gt;8</th>\n",
       "      <th>n_roh&gt;8</th>\n",
       "      <th>sum_roh&gt;12</th>\n",
       "      <th>n_roh&gt;12</th>\n",
       "      <th>sum_roh&gt;20</th>\n",
       "      <th>...</th>\n",
       "      <th>study</th>\n",
       "      <th>clst_alt</th>\n",
       "      <th>period_alt</th>\n",
       "      <th>include_alt</th>\n",
       "      <th>clst</th>\n",
       "      <th>mean_cov</th>\n",
       "      <th>med_cov</th>\n",
       "      <th>n_cov_snp_read</th>\n",
       "      <th>full_iid</th>\n",
       "      <th>n_cov_snp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Surui_0</td>\n",
       "      <td>Surui</td>\n",
       "      <td>45.051694</td>\n",
       "      <td>688.967521</td>\n",
       "      <td>49</td>\n",
       "      <td>581.753403</td>\n",
       "      <td>30</td>\n",
       "      <td>521.008477</td>\n",
       "      <td>24</td>\n",
       "      <td>283.860490</td>\n",
       "      <td>...</td>\n",
       "      <td>Lazaridis et al. 2014</td>\n",
       "      <td>Surui</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>Surui</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>HGDP00832</td>\n",
       "      <td>555005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Karitiana_11</td>\n",
       "      <td>Karitiana</td>\n",
       "      <td>71.722305</td>\n",
       "      <td>596.386334</td>\n",
       "      <td>37</td>\n",
       "      <td>494.799619</td>\n",
       "      <td>20</td>\n",
       "      <td>455.792115</td>\n",
       "      <td>16</td>\n",
       "      <td>392.679617</td>\n",
       "      <td>...</td>\n",
       "      <td>Lazaridis et al. 2014</td>\n",
       "      <td>Karitiana</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>Karitiana</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>HGDP01019</td>\n",
       "      <td>553410</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows Ã— 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            iid        pop    max_roh   sum_roh>4  n_roh>4   sum_roh>8  \\\n",
       "0       Surui_0      Surui  45.051694  688.967521       49  581.753403   \n",
       "1  Karitiana_11  Karitiana  71.722305  596.386334       37  494.799619   \n",
       "\n",
       "   n_roh>8  sum_roh>12  n_roh>12  sum_roh>20    ...                     study  \\\n",
       "0       30  521.008477        24  283.860490    ...     Lazaridis et al. 2014   \n",
       "1       20  455.792115        16  392.679617    ...     Lazaridis et al. 2014   \n",
       "\n",
       "    clst_alt  period_alt  include_alt       clst  mean_cov  med_cov  \\\n",
       "0      Surui         NaN            1      Surui       NaN      NaN   \n",
       "1  Karitiana         NaN            1  Karitiana       NaN      NaN   \n",
       "\n",
       "  n_cov_snp_read   full_iid n_cov_snp  \n",
       "0            NaN  HGDP00832    555005  \n",
       "1            NaN  HGDP01019    553410  \n",
       "\n",
       "[2 rows x 27 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Area 51\n",
    "Area to test code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set Output Log path: ./Empirical/1240k/HO/Italian_South_0/chr4/e01/hmm_run_log.txt\n"
     ]
    }
   ],
   "source": [
    "analyze_individual_gt(*prms[3])  # Single Test Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(prms[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mod_df[\"clst\"].value_counts()\n",
    "set(mod_df[\"clst\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Start</th>\n",
       "      <th>End</th>\n",
       "      <th>StartM</th>\n",
       "      <th>EndM</th>\n",
       "      <th>length</th>\n",
       "      <th>lengthM</th>\n",
       "      <th>iid</th>\n",
       "      <th>ch</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>22400</td>\n",
       "      <td>22557</td>\n",
       "      <td>1.370264</td>\n",
       "      <td>1.384025</td>\n",
       "      <td>157</td>\n",
       "      <td>0.013761</td>\n",
       "      <td>Sardinian_0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12447</td>\n",
       "      <td>13306</td>\n",
       "      <td>0.782127</td>\n",
       "      <td>0.817144</td>\n",
       "      <td>859</td>\n",
       "      <td>0.035017</td>\n",
       "      <td>Sardinian_0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>39936</td>\n",
       "      <td>40095</td>\n",
       "      <td>2.120861</td>\n",
       "      <td>2.133356</td>\n",
       "      <td>159</td>\n",
       "      <td>0.012495</td>\n",
       "      <td>Sardinian_0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Start    End    StartM      EndM length   lengthM          iid ch\n",
       "0  22400  22557  1.370264  1.384025    157  0.013761  Sardinian_0  1\n",
       "0  12447  13306  0.782127  0.817144    859  0.035017  Sardinian_0  2\n",
       "1  39936  40095  2.120861  2.133356    159  0.012495  Sardinian_0  2"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1941"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ho_df = meta_df[meta_df[\"study\"]==\"Lazaridis et al. 2014\"]\n",
    "len(ho_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Yoruba              70\n",
       "Turkish             56\n",
       "Spanish             53\n",
       "Druze               39\n",
       "Palestinian         38\n",
       "Han                 33\n",
       "Japanese            29\n",
       "Basque              29\n",
       "Sardinian           27\n",
       "French              25\n",
       "Ulchi               25\n",
       "BedouinA            25\n",
       "Burusho             23\n",
       "Chukchi             23\n",
       "Tubalar             22\n",
       "Russian             22\n",
       "Eskimo              22\n",
       "Brahui              21\n",
       "Mozabite            21\n",
       "Hungarian           20\n",
       "Biaka               20\n",
       "Balochi             20\n",
       "Yakut               20\n",
       "Makrani             20\n",
       "Greek               20\n",
       "Pathan              19\n",
       "BedouinB            19\n",
       "Yukagir             19\n",
       "Kalash              18\n",
       "Mayan               18\n",
       "                    ..\n",
       "Finnish              7\n",
       "BantuKenya           6\n",
       "Korean               6\n",
       "Itelmen              6\n",
       "Saharawi             6\n",
       "Iraqi_Jew            6\n",
       "Moroccan_Jew         6\n",
       "Mongola              6\n",
       "Yemen                6\n",
       "Gambian              6\n",
       "Albanian             6\n",
       "GujaratiD            5\n",
       "Cochin_Jew           5\n",
       "GujaratiA            5\n",
       "Hadza                5\n",
       "Ju_hoan_North        5\n",
       "GujaratiC            5\n",
       "GujaratiB            5\n",
       "Spanish_North        5\n",
       "Quechua              5\n",
       "Tlingit              4\n",
       "Scottish             4\n",
       "Kikuyu               4\n",
       "Piapoco              4\n",
       "Dolgan               3\n",
       "Australian           3\n",
       "Datog                3\n",
       "Canary_Islanders     2\n",
       "Italian_South        1\n",
       "Saami_WGA            1\n",
       "Name: clst, Length: 162, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ho_df[\"clst\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "143"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(ho_df[\"clst\"].value_counts()>5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
