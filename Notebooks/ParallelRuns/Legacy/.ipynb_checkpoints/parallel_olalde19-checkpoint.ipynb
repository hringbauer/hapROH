{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "midway2-0401.rcc.local\n",
      "Midway jnovmbre partition detected.\n",
      "/project2/jnovembre/hringbauer/HAPSBURG\n",
      "CPU Count: 28\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os as os\n",
    "import sys as sys\n",
    "import multiprocessing as mp\n",
    "import pandas as pd\n",
    "import socket\n",
    "\n",
    "### Pick the right path (whether on cluster or at home)\n",
    "socket_name = socket.gethostname()\n",
    "print(socket_name)\n",
    "if socket_name == \"VioletQueen\":\n",
    "    path = \"/home/harald/git/HAPSBURG/\"   # The Path on Harald's machine\n",
    "elif socket_name.startswith(\"midway2\"):\n",
    "    print(\"Midway jnovmbre partition detected.\")\n",
    "    path = \"/project2/jnovembre/hringbauer/HAPSBURG/\"  # The Path on Midway Cluster\n",
    "else: \n",
    "    raise RuntimeWarning(\"Not compatible machine. Check!!\")\n",
    "    \n",
    "os.chdir(path)  # Set the right Path (in line with Atom default)\n",
    "print(os.getcwd()) # Show the current working directory. Should be HAPSBURG/Notebooks/ParallelRuns\n",
    "print(f\"CPU Count: {mp.cpu_count()}\")\n",
    "\n",
    "### Assume that now we are in the root directory\n",
    "sys.path.append(\"./Python3/\")  \n",
    "sys.path.append(\"./PackagesSupport/parallel_runs/\")\n",
    "sys.path.append(\"./PackagesSupport/\")\n",
    "\n",
    "from hmm_inference import HMM_Analyze   # The HMM core object\n",
    "from helper_functions import prepare_path, multi_run, combine_individual_data\n",
    "from hapsburg_run import hapsb_chrom, hapsb_ind"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test single Individual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doing Individual I0843...\n",
      "Running 22 jobs in parallel.\n",
      "Set Output Log path: ./Empirical/Eigenstrat/Olalde19/I0843/chr1/hmm_run_log.txt\n",
      "Set Output Log path: ./Empirical/Eigenstrat/Olalde19/I0843/chr5/hmm_run_log.txt\n",
      "Set Output Log path: ./Empirical/Eigenstrat/Olalde19/I0843/chr3/hmm_run_log.txt\n",
      "Set Output Log path: ./Empirical/Eigenstrat/Olalde19/I0843/chr7/hmm_run_log.txt\n",
      "Combining Information for 22 Chromosomes...\n",
      "Run finished successfully!\n"
     ]
    }
   ],
   "source": [
    "hapsb_ind(iid=\"I0843\", chs=range(1,23), processes=4, \n",
    "          h5_path_targets = \"./Data/Olalde19/Olalde_et_al_genotypes\",\n",
    "          base_out_folder=\"./Empirical/Eigenstrat/Olalde19/\",\n",
    "          e_model=\"haploid\", p_model=\"Eigenstrat\", n_ref=2504,\n",
    "          destroy_phase=True, readcounts=False,\n",
    "          delete=False, logfile=True, combine=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run all individuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_path=\"./Data/Olalde19/meta_processed.csv\"\n",
    "df = pd.read_csv(meta_path)\n",
    "df = df[df[\"n_cov_snp\"]>400000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doing Individual I10899...\n",
      "Running 22 jobs in parallel.\n",
      "Set Output Log path: ./Empirical/Eigenstrat/Olalde19/I10899/chr1/hmm_run_log.txt\n",
      "Set Output Log path: ./Empirical/Eigenstrat/Olalde19/I10899/chr5/hmm_run_log.txt\n",
      "Set Output Log path: ./Empirical/Eigenstrat/Olalde19/I10899/chr2/hmm_run_log.txt\n",
      "Set Output Log path: ./Empirical/Eigenstrat/Olalde19/I10899/chr4/hmm_run_log.txt\n",
      "Set Output Log path: ./Empirical/Eigenstrat/Olalde19/I10899/chr3/hmm_run_log.txt\n",
      "Set Output Log path: ./Empirical/Eigenstrat/Olalde19/I10899/chr6/hmm_run_log.txt\n"
     ]
    }
   ],
   "source": [
    "for iid in df[\"iid\"]:\n",
    "    hapsb_ind(iid=iid, chs=range(1,23), processes=6, \n",
    "              h5_path_targets = \"./Data/Olalde19/Olalde_et_al_genotypes\",\n",
    "              base_out_folder=\"./Empirical/Eigenstrat/Olalde19/\",\n",
    "              e_model=\"haploid\", p_model=\"Eigenstrat\", n_ref=2504,\n",
    "              destroy_phase=True, readcounts=False,\n",
    "              delete=False, logfile=True, combine=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Post-Process all Individuals with >400k Coverage\n",
    "(run individuals via sbatch in Packages Support for Antonio Files\n",
    "\n",
    "MODIFY STILL RAW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(\"./PackagesSupport/\")\n",
    "from pp_individual_roh_csvs import create_combined_ROH_df, give_iid_paths, pp_individual_roh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_path=\"./Data/Antonio/meta_processed.csv\"\n",
    "df_anno = pd.read_csv(meta_path)\n",
    "df_ana = df_anno[df_anno[\"mean_cov\"]>0.5]\n",
    "print(f\"{len(df_ana)} Individuals with coverage > {0.5}\")\n",
    "iids = df_ana[\"iid\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df1 = pp_individual_roh(iids, meta_path=\"./Data/Antonio/meta_processed.csv\", base_folder=\"./Empirical/1240k/Antonio/\",\n",
    "                        save_path=\"./Empirical/1240k/Antonio/combined_roh05.csv\",\n",
    "                        output=False, min_cm=[4,8,12], snp_cm=50, gap=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Area 51"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Original Code to process Eigenstrat\n",
    "\n",
    "def analyze_chromosome_es(iid, ch=3, n_ref=503, save=True, save_fp=False, exclude_pops=[], \n",
    "                          base_out_folder=\"./Empirical/Eigenstrat/Reichall/test/\", prefix_out=\"\",\n",
    "                          roh_in=100, roh_out=100, roh_jump=300, e_rate=0.01, e_rate_ref=0.001, \n",
    "                          max_gap=0, logfile=True):\n",
    "    \"\"\"Run the analysis for one individual and chromosome on eigenstrat data\n",
    "    Wrapper for HMM Class. Takes 13 Parameters\"\"\"\n",
    "    \n",
    "    ### The folder on what to run the Data on (PERMANENTLY set here to fixed loaction)\n",
    "    ## What Eigenstrat File to run on:\n",
    "    es_target_path=\"./Data/ReichLabEigenstrat/Raw/v37.2.1240K\"\n",
    "    \n",
    "    ## Reference Files:\n",
    "    h5_path1000g = \"./Data/1000Genomes/HDF5/1240kHDF5/all1240/chr\" \n",
    "    meta_path_ref = \"./Data/1000Genomes/Individuals/meta_df_all.csv\"\n",
    "    \n",
    "    ### Create Folder if needed, and pipe output if wanted\n",
    "    path_out = prepare_path(base_out_folder, iid, ch, prefix_out, logfile=logfile)\n",
    "    \n",
    "    hmm = HMM_Analyze(cython=2, p_model=\"Eigenstrat\", e_model=\"haploid\", post_model=\"Standard\",\n",
    "                      manual_load=True, save=save, save_fp=save_fp)\n",
    "\n",
    "    ### Load and prepare the pre-processing Model\n",
    "    hmm.load_preprocessing_model()              # Load the preprocessing Model\n",
    "    hmm.p_obj.set_params(es_target_path=es_target_path, readcounts = False, destroy_phase=True,\n",
    "                base_out_folder=base_out_folder, prefix_out_data=prefix_out, excluded=exclude_pops)   \n",
    "    \n",
    "    ### Set to run with full 1000G reference. DELETE when run for with European Reference!!\n",
    "    hmm.p_obj.set_params(h5_path1000g = h5_path1000g, meta_path_ref = meta_path_ref)\n",
    "    \n",
    "    hmm.load_data(iid=iid, ch=ch, n_ref=n_ref)  # Load the actual Data\n",
    "    hmm.load_secondary_objects()\n",
    "\n",
    "    ### Set the Parameters\n",
    "    hmm.e_obj.set_params(e_rate = e_rate, e_rate_ref = e_rate_ref)\n",
    "    hmm.t_obj.set_params(roh_in=roh_in, roh_out=roh_out, roh_jump=roh_jump)\n",
    "    hmm.post_obj.set_params(max_gap=max_gap)\n",
    "    \n",
    "    #hmm.calc_viterbi_path(save=save)           # Calculate the Viterbi Path.\n",
    "    hmm.calc_posterior(save=save)              # Calculate the Posterior.\n",
    "    hmm.post_processing(save=save)             # Do the Post-Processing.\n",
    "                         \n",
    "#########################################################\n",
    "def analyze_individual_es(iid, chs=range(1,23), n_ref=2504, save=True, save_fp=False, \n",
    "                          exclude_pops=[], base_out_folder=\"./Empirical/Eigenstrat/Reichall/\", \n",
    "                          prefix_out=\"\", roh_in=100, roh_out=100, roh_jump=300, e_rate=0.01, \n",
    "                          e_rate_ref=0.01, max_gap=0, logfile=True, output=True, processes=5, delete=True):\n",
    "    \"\"\"Analyze a full single individual in a parallelized fasion. Run all Chromosome analyses in parallel\n",
    "    Wrapper for analyze_chromosome_gt.\n",
    "    logfile: Whether to use a logfile\n",
    "    output: Whether to print general Output\"\"\"\n",
    "                            \n",
    "    if output == True:\n",
    "        print(f\"Doing Individual {iid}...\")\n",
    "    \n",
    "    ### Prepare the Parameters for that Indivdiual\n",
    "    prms = [[iid, ch, n_ref, save, save_fp, exclude_pops, base_out_folder, prefix_out,\n",
    "         roh_in, roh_out, roh_jump, e_rate, e_rate_ref, max_gap, logfile] for ch in chs] \n",
    "                            \n",
    "    ### Run the analysis in parallel\n",
    "    #multi_run(analyze_chromosome_es, prms, processes = processes)\n",
    "                            \n",
    "    ### Merge results for that Individual\n",
    "    combine_individual_data(base_out_folder, iid=iid, delete=delete, chs=chs)                  \n",
    "    return #prms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
