{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook to call ROH in parallel within HO origin Individuals\n",
    "Notebooks that import the code for the calling ROHs on diploid genotype individuals, and then a function to parallelize it.\n",
    "\n",
    "Very similar to parallel_mosaic_callroh.ipynb\n",
    "\n",
    "\n",
    "@Author: Harald Ringbauer, September 2019\n",
    "All rights reserved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "midway2-0401.rcc.local\n",
      "Midway jnovmbre partition detected.\n",
      "/project2/jnovembre/hringbauer/HAPSBURG\n",
      "CPU Count: 28\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os as os\n",
    "import sys as sys\n",
    "import multiprocessing as mp\n",
    "import pandas as pd\n",
    "import socket\n",
    "\n",
    "### Pick the right path (whether on cluster or at home)\n",
    "socket_name = socket.gethostname()\n",
    "print(socket_name)\n",
    "if socket_name == \"VioletQueen\":\n",
    "    path = \"/home/harald/git/HAPSBURG/\"   # The Path on Harald's machine\n",
    "elif socket_name.startswith(\"midway2\"):\n",
    "    print(\"Midway jnovmbre partition detected.\")\n",
    "    path = \"/project2/jnovembre/hringbauer/HAPSBURG/\"  # The Path on Midway Cluster\n",
    "else: \n",
    "    raise RuntimeWarning(\"Not compatible machine. Check!!\")\n",
    "    \n",
    "os.chdir(path)  # Set the right Path (in line with Atom default)\n",
    "print(os.getcwd()) # Show the current working directory. Should be HAPSBURG/Notebooks/ParallelRuns\n",
    "print(f\"CPU Count: {mp.cpu_count()}\")\n",
    "\n",
    "\n",
    "sys.path.append(\"./package/\")  # Since now we are in the Root Directory\n",
    "from hapsburg.PackagesSupport.hapsburg_run import hapsb_chrom, hapsb_ind\n",
    "#from hapsburg.PackagesSupport.parallel_runs.helper_functions import prepare_path, multi_run, combine_individual_data\n",
    "from hapsburg.PackagesSupport.pp_individual_roh_csvs import create_combined_ROH_df, give_iid_paths, pp_individual_roh\n",
    "#from createMosaicsMulti import Mosaic_1000G_Multi  # Import the object that can create the Multiruns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyze HO Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze a single Individual\n",
    "For reanalysis with delete=True to plot that indivdual / further analysis of posterior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Haploid Mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "hapsb_ind(iid=\"Luo_4\", chs=range(2, 3), \n",
    "          path_targets= \"./Data/Marcus2019_1240k/sardinia_hapsburg.h5\", \n",
    "          h5_path1000g= './Data/1000Genomes/HDF5/1240kHDF5/all1240int8/chr', \n",
    "          meta_path_ref= './Data/1000Genomes/Individuals/meta_df_all.csv', \n",
    "          folder_out = './Empirical/HO/Afr_hap/', \n",
    "          prefix_out = '', e_model=\"haploid\", p_model=\"SardHDF5\", post_model='Standard', \n",
    "          processes=1, delete=False, output=True, save=True, save_fp=False, \n",
    "          n_ref=2504, exclude_pops=[], readcounts=False, \n",
    "          random_allele=True, roh_in=1, roh_out=20, roh_jump=300, e_rate=0.01, e_rate_ref=0.0, \n",
    "          cutoff_post=0.999, max_gap=0, roh_min_l=0.01, logfile=False, \n",
    "          combine=True, file_result='_roh_full.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Diploid Mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "hapsb_ind(iid=\"Luo_4\", chs=range(2, 3), \n",
    "          path_targets= \"./Data/Marcus2019_1240k/sardinia_hapsburg.h5\", \n",
    "          h5_path1000g='./Data/1000Genomes/HDF5/1240kHDF5/all1240int8/chr', \n",
    "          meta_path_ref='./Data/1000Genomes/Individuals/meta_df_all.csv', \n",
    "          folder_out='./Empirical/HO/Afr_dip/', \n",
    "          prefix_out='', e_model=\"diploid_gt\", p_model=\"SardHDF5\", post_model='Standard', \n",
    "          processes=1, delete=False, output=True, save=True, save_fp=False, n_ref=2504, exclude_pops=[], readcounts=False, \n",
    "          random_allele=False, roh_in=1, roh_out=20, roh_jump=300, e_rate=0.01, e_rate_ref=0.0, \n",
    "          cutoff_post=0.999, max_gap=0, roh_min_l=0.01, logfile=False, combine=False, file_result='_roh_full.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test individual with haploid mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "iid = dct_iid[\"Khomani_7\"]\n",
    "hapsb_ind(iid=iid, chs=range(8, 9), \n",
    "          path_targets= \"./Data/Marcus2019_1240k/mod_reich_sardinia_ancients_rev_mrg_dedup_3trm_anno.h5\", \n",
    "          h5_path1000g='./Data/1000Genomes/HDF5/1240kHDF5/all1240int8/chr', \n",
    "          meta_path_ref='./Data/1000Genomes/Individuals/meta_df_all.csv', \n",
    "          folder_out = './Empirical/HO/Afr_hap/', \n",
    "          prefix_out = '', e_model=\"haploid\", p_model=\"SardHDF5\", post_model='Standard', \n",
    "          processes=1, delete=False, output=True, save=True, save_fp=False, n_ref=2504, exclude_pops=[], readcounts=False, \n",
    "          random_allele=True, roh_in=1, roh_out=20, roh_jump=300, e_rate=0.01, e_rate_ref=0.0, \n",
    "          cutoff_post=0.999, max_gap=0, roh_min_l=0.01, logfile=False, combine=True, file_result='_roh_full.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Run in cluster_runs/HO_callROH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_iid_from_df(i, path_df=\"./Empirical/HO/combined_roh05.csv\", \n",
    "                    sep=\"\\t\", cm=12, output=False):\n",
    "    \"\"\"Return ith iid to run\"\"\"\n",
    "\n",
    "    if output:\n",
    "        print(f\"Indivdiuals with ROH>{cm}: {np.sum(idx)}/{len(idx)}\")\n",
    "    iids = df[\"iid\"][idx].values\n",
    "    return iids[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Post-Process long ROH Individuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./Empirical/HO/combined_roh05.csv\", sep=\"\\t\")\n",
    "idx = df[f\"sum_roh>{12}\"]>0\n",
    "iids = df[\"iid\"][idx].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "555940"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(df[\"n_cov_snp\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Do the long diploid callss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 599 / 4616 Individuals from Meta\n",
      "Saved to: ./Empirical/HO/combined_roh_dip05.tsv\n",
      "CPU times: user 1min 56s, sys: 139 ms, total: 1min 57s\n",
      "Wall time: 2min 3s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df1 = pp_individual_roh(iids[:], meta_path=\"./Data/Marcus2019_1240k/meta_rev_unique_ids.csv\", base_folder=\"./Empirical/HO/long_dip/\",\n",
    "                        save_path=\"./Empirical/HO/combined_roh_dip05.tsv\", \n",
    "                        output=False, min_cm=[4,8,12,20], snp_cm=50, \n",
    "                        gap=0.5, min_len1=2, min_len2=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do the long haploid calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 599 / 4616 Individuals from Meta\n",
      "Saved to: ./Empirical/HO/combined_roh_hap05.tsv\n",
      "CPU times: user 5min 12s, sys: 275 ms, total: 5min 12s\n",
      "Wall time: 5min 18s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df1 = pp_individual_roh(iids[:], meta_path=\"./Data/Marcus2019_1240k/meta_rev_unique_ids.csv\", base_folder=\"./Empirical/HO/long_hap/\",\n",
    "                        save_path=\"./Empirical/HO/combined_roh_hap05.tsv\", \n",
    "                        output=False, min_cm=[4,8,12,20], snp_cm=50, \n",
    "                        gap=0.5, min_len1=2, min_len2=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Postprocess all Individuals into one Dataframe\n",
    "1) Get all Individual IIDs\n",
    "2) Combine results into one Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Get all IIDs\n",
    "iids = give_ho_iids_all()\n",
    "print(f\"Loaded {len(iids)} HO Individuals\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1941 / 4616 Individuals from Meta\n",
      "Saved to: ./Empirical/HO/CombinedROH/combined_roh05.csv\n",
      "CPU times: user 3min 52s, sys: 460 ms, total: 3min 52s\n",
      "Wall time: 4min 2s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df1 = pp_individual_roh(iids[:], meta_path=meta_path, base_folder=\"./Empirical/HO/\",\n",
    "                        save_path=\"./Empirical/HO/CombinedROH/combined_roh05.csv\", \n",
    "                        output=False, min_cm=[4,8,12,20], snp_cm=50, \n",
    "                        gap=0.5, min_len1=2, min_len2=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Legacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_path(path_output, iid, ch, prefix_out, logfile=True, output=False):\n",
    "    \"\"\"Prepare the path and pipe printing for one Individual\n",
    "    logfile: Whether to pipe output to log-file\"\"\"   \n",
    "    #if not os.path.exists(path_output):\n",
    "    #        raise RuntimeError(f\"Path {path_output} not Found. Check!\")\n",
    "    path_log = os.path.join(path_output, str(iid), \"chr\"+str(ch), prefix_out, \"\")      \n",
    "    #path_log =  path_output + str(iid) + \"/chr\" + str(ch) + \"/\" + prefix_out\n",
    "    \n",
    "    if not os.path.exists(path_log):\n",
    "        if output==True:\n",
    "            print(f\"Creating {path_log}...\")\n",
    "        os.makedirs(path_log)\n",
    "    \n",
    "    if logfile == True:\n",
    "        path_log = path_log + \"hmm_run_log.txt\"\n",
    "        if output==True:\n",
    "            print(f\"Set Output Log path: {path_log}\")\n",
    "        sys.stdout = open(path_log, 'w') \n",
    "    \n",
    "def analyze_chromosome_gt(iid, ch=3, n_ref=503, save=True, save_fp=False, exclude_pops=[\"TSI\", ], \n",
    "                          base_out_folder=\"./Empirical/HO/\", prefix_out=\"gt/\",\n",
    "                          roh_in=100, roh_out=100, roh_jump=385, e_rate=0.01, e_rate_ref=0.001, \n",
    "                          max_gap=0, logfile=True):\n",
    "    \"\"\"Run the analysis for one individual and chromosome on readcount data\n",
    "    Wrapper for HMM Class. Takes 13 Parameters\"\"\"\n",
    "    \n",
    "    ### The folder on what to run the Data on (Permanently set here to fixed loaction)\n",
    "    h5_path_targets = \"./Data/Marcus2019_1240k/mod_reich_sardinia_ancients_rev_mrg_dedup_3trm_anno.h5\"\n",
    "    meta_path_targets = \"./Data/Marcus2019_1240k/meta_rev_unique_ids.csv\"  ### Path with the unique IDs per Modern Group\n",
    "    \n",
    "    ### Create Folder if needed, and pipe output if wanted\n",
    "    prepare_path(base_out_folder, iid, ch, prefix_out, logfile=logfile)\n",
    "    \n",
    "    hmm = HMM_Analyze(cython=2, p_model=\"SardHDF5\", e_model=\"diploid_gt\", post_model=\"Standard\",\n",
    "                      manual_load=True, save=save, save_fp=save_fp)\n",
    "\n",
    "    # Load and prepare the pre-processing Model\n",
    "    hmm.load_preprocessing_model()              # Load the preprocessing Model\n",
    "    hmm.p_obj.set_params(readcounts = False, destroy_phase=False,\n",
    "                prefix_out_data=prefix_out, excluded=exclude_pops, base_out_folder=base_out_folder,\n",
    "                h5_path_targets = h5_path_targets, meta_path_targets=meta_path_targets)    \n",
    "    \n",
    "    ### DELETE when run for with European Reference!!\n",
    "    hmm.p_obj.set_params(h5_path1000g = \"./Data/1000Genomes/HDF5/1240kHDF5/all1240/chr\", \n",
    "                         meta_path_ref = \"./Data/1000Genomes/Individuals/meta_df_all.csv\")\n",
    "    \n",
    "    hmm.load_data(iid=iid, ch=ch, n_ref=n_ref)  # Load the actual Data\n",
    "    hmm.load_secondary_objects()\n",
    "\n",
    "    ### Set the Parameters\n",
    "    hmm.e_obj.set_params(e_rate = e_rate, e_rate_ref = e_rate_ref)\n",
    "    hmm.t_obj.set_params(roh_in=roh_in, roh_out=roh_out, roh_jump=roh_jump)\n",
    "    hmm.post_obj.set_params(max_gap=max_gap)\n",
    "    \n",
    "    #hmm.calc_viterbi_path(save=save)           # Calculate the Viterbi Path.\n",
    "    hmm.calc_posterior(save=save)              # Calculate the Posterior.\n",
    "    hmm.post_processing(save=save)             # Do the Post-Processing.\n",
    "\n",
    "    \n",
    "#########################################################\n",
    "def combine_individual_data(base_path, iid, delete=False, chs=range(1,23), prefix_out=\"\"):\n",
    "    \"\"\"Function to merge data from one Individual Analysis (all Chromosome)\n",
    "    chs: Which Chromosomes to combine\"\n",
    "    delete: Whether to delete individual folder and contents after combining.\"\"\"\n",
    "    \n",
    "    full_df_vec =[]  # The full dataframe of inferred ROH blocks\n",
    "    \n",
    "    ### Walk through Chromosomes and combine the Dataframes\n",
    "    for ch in chs:\n",
    "        path_roh = os.path.join(base_path, str(iid), \"chr\"+str(ch), prefix_out, \"roh.csv\") \n",
    "        df_temp = pd.read_csv(path_roh, sep=\",\")\n",
    "        full_df_vec.append(df_temp)\n",
    "        \n",
    "    full_df = pd.concat(full_df_vec)\n",
    "        \n",
    "    ### Save to Path:\n",
    "    path_save = os.path.join(base_path, str(iid) + \"_roh_full.csv\")\n",
    "    full_df.to_csv(path_save, index=False)\n",
    "    \n",
    "    ### Delete files in folder if need\n",
    "    if delete == True:\n",
    "        for ch in chs:\n",
    "            path_folder = os.path.join(base_path, str(iid), \"chr\"+str(ch), prefix_out, \"\") \n",
    "            \n",
    "            for root, _, files in os.walk(path_folder):\n",
    "                for file in files:\n",
    "                    os.remove(os.path.join(root, file))\n",
    "            os.rmdir(path_folder) # Remove the Chromosome Folders\n",
    "        os.rmdir(os.path.join(base_path, str(iid), \"\"))  # Remove the Individual Folder\n",
    "    \n",
    "    return full_df\n",
    "                             \n",
    "#########################################################\n",
    "def analyze_individual_ho(iid, chs=range(1,23), n_ref=2504, save=True, save_fp=False, exclude_pops=[], \n",
    "                          base_out_folder=\"./Empirical/HO/\", prefix_out=\"\",\n",
    "                          roh_in=100, roh_out=100, roh_jump=300, e_rate=0.001, \n",
    "                          e_rate_ref=0.001, max_gap=0, logfile=True, output=True, processes=5, delete=True):\n",
    "    \"\"\"Analyze a full single individual in a parallelized fasion. Run all Chromosome analyses in parallel\n",
    "    Wrapper for analyze_chromosome_gt.\n",
    "    logfile: Whether to use a logfile\n",
    "    output: Whether to print general Output\"\"\"\n",
    "                            \n",
    "    if output == True:\n",
    "        print(f\"Doing Individual {iid}...\")\n",
    "    \n",
    "    ### Prepare the Parameters for that Indivdiual\n",
    "    prms = [[iid, ch, n_ref, save, save_fp, exclude_pops, base_out_folder, prefix_out,\n",
    "         roh_in, roh_out, roh_jump, e_rate, e_rate_ref, max_gap, logfile] for ch in chs] \n",
    "                            \n",
    "    ### Run the analysis in parallel\n",
    "    multi_run(analyze_chromosome_gt, prms, processes = processes)\n",
    "                            \n",
    "    ### Merge results for that Individual\n",
    "    combine_individual_data(base_out_folder, iid=iid, delete=delete, chs=chs)\n",
    "                            \n",
    "    return\n",
    "        \n",
    "#########################################################\n",
    "#########################################################\n",
    "    \n",
    "def multi_run(fun, prms, processes = 4):\n",
    "    \"\"\"Implementation of running in Parallel.\n",
    "    fun: Function\n",
    "    prms: The Parameter Files\n",
    "    processes: How many Processes to use\"\"\"\n",
    "    print(f\"Running {len(prms)} jobs in parallel.\")\n",
    "    \n",
    "    with mp.Pool(processes = processes) as pool:\n",
    "            results = pool.starmap(fun, prms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Write the Command for iid\n",
    "def give_iids_populations_ho(pop):\n",
    "    \"\"\"Return all IIDs of Population pop in meta_df (in Lazaridis HO paper)\"\"\"\n",
    "    ho_df = meta_df[meta_df[\"study\"]==\"Lazaridis et al. 2014\"]\n",
    "    iids = ho_df[\"iid\"][meta_df[\"clst\"]==pop]\n",
    "    assert(len(iids)>0)\n",
    "    return iids.values\n",
    "\n",
    "def give_ho_iids_all():\n",
    "    \"\"\"Return individual IIDs of all HO samples\"\"\"\n",
    "    ho_df = meta_df[meta_df[\"study\"]==\"Lazaridis et al. 2014\"]\n",
    "    iids = ho_df[\"iid\"].values  # Extract Individuals\n",
    "    return iids\n",
    "\n",
    "def run_ho_pops(pops, chs=range(1,23), delete=True, processes=5, base_out_folder=\"./Empirical/HO/\"):\n",
    "    \"\"\"Run HAPSBURG on all Individuals of HO pops\"\"\"\n",
    "    for pop in pops:\n",
    "        iids = give_iids_populations_ho(pop)\n",
    "        for iid in iids:\n",
    "            analyze_individual_ho(iid=iid, chs=chs, processes=processes, delete=delete)\n",
    "                   \n",
    "def run_ho_inds(ind_range=[], chs=range(1,23), delete=True, processes=5, base_out_folder=\"./Empirical/HO/\"):\n",
    "    \"\"\"Run batches of HO Individuals, 1 Individual a time (parallelized)\"\"\"\n",
    "    iids = give_ho_iids_all()\n",
    "    iids = iids[ind_range]\n",
    "    for iid in iids:\n",
    "        analyze_individual_ho(iid=iid, chs=chs, processes=processes, delete=delete, base_out_folder=base_out_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run a whole HO Population or Range of HO individuals\n",
    "TODO: Update to newer function for the trun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_ho_pops(pops=[\"Yi\",], chs=range(1,2), delete=False, processes=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create HO Analysis Data. Run in batches of ind_range (to not submit everything at once)\n",
    "This is the cell that does the final data analysis\n",
    "\n",
    "Analysis counter: Done until 130 (Python Indexing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "run_ho_inds(ind_range=range(30,130), chs=range(1,23), delete=True, processes=6, base_out_folder=\"./Empirical/HO/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Area 51\n",
    "Area to test code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set Output Log path: ./Empirical/1240k/HO/Italian_South_0/chr4/e01/hmm_run_log.txt\n"
     ]
    }
   ],
   "source": [
    "analyze_individual_gt(*prms[3])  # Single Test Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(prms[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mod_df[\"clst\"].value_counts()\n",
    "set(mod_df[\"clst\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Start</th>\n",
       "      <th>End</th>\n",
       "      <th>StartM</th>\n",
       "      <th>EndM</th>\n",
       "      <th>length</th>\n",
       "      <th>lengthM</th>\n",
       "      <th>iid</th>\n",
       "      <th>ch</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>22400</td>\n",
       "      <td>22557</td>\n",
       "      <td>1.370264</td>\n",
       "      <td>1.384025</td>\n",
       "      <td>157</td>\n",
       "      <td>0.013761</td>\n",
       "      <td>Sardinian_0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12447</td>\n",
       "      <td>13306</td>\n",
       "      <td>0.782127</td>\n",
       "      <td>0.817144</td>\n",
       "      <td>859</td>\n",
       "      <td>0.035017</td>\n",
       "      <td>Sardinian_0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>39936</td>\n",
       "      <td>40095</td>\n",
       "      <td>2.120861</td>\n",
       "      <td>2.133356</td>\n",
       "      <td>159</td>\n",
       "      <td>0.012495</td>\n",
       "      <td>Sardinian_0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Start    End    StartM      EndM length   lengthM          iid ch\n",
       "0  22400  22557  1.370264  1.384025    157  0.013761  Sardinian_0  1\n",
       "0  12447  13306  0.782127  0.817144    859  0.035017  Sardinian_0  2\n",
       "1  39936  40095  2.120861  2.133356    159  0.012495  Sardinian_0  2"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1941"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ho_df = meta_df[meta_df[\"study\"]==\"Lazaridis et al. 2014\"]\n",
    "len(ho_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
