{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook to call ROH in parallel within HO origin Individuals\n",
    "Notebooks that import the code for the calling ROHs on diploid genotype individuals, and then a function to parallelize it.\n",
    "\n",
    "Very similar to parallel_mosaic_callroh.ipynb\n",
    "\n",
    "\n",
    "@Author: Harald Ringbauer, September 2019\n",
    "All rights reserved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "midway2-0401.rcc.local\n",
      "Midway jnovmbre partition detected.\n",
      "/project/jnovembre/hringbauer/HAPSBURG\n",
      "CPU Count: 28\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os as os\n",
    "import sys as sys\n",
    "import multiprocessing as mp\n",
    "import pandas as pd\n",
    "import socket\n",
    "\n",
    "### Pick the right path (whether on cluster or at home)\n",
    "socket_name = socket.gethostname()\n",
    "print(socket_name)\n",
    "if socket_name == \"VioletQueen\":\n",
    "    path = \"/home/harald/git/HAPSBURG/\"   # The Path on Harald's machine\n",
    "elif socket_name.startswith(\"midway2\"):\n",
    "    print(\"Midway jnovmbre partition detected.\")\n",
    "    path = \"/project/jnovembre/hringbauer/HAPSBURG/\"  # The Path on Midway Cluster\n",
    "else: \n",
    "    raise RuntimeWarning(\"Not compatible machine. Check!!\")\n",
    "    \n",
    "os.chdir(path)  # Set the right Path (in line with Atom default)\n",
    "\n",
    "sys.path.append(\"./Python3/\")  # Since now we are in the Root Directory\n",
    "from hmm_inference import HMM_Analyze   # Do not move. Should be after sys.path..\n",
    "#sys.path.append(\"./Python3/create1000G_Mosaic/\")  # Since now we are in the Root Directory\n",
    "#from createMosaicsMulti import Mosaic_1000G_Multi  # Import the object that can create the Multiruns\n",
    "\n",
    "print(os.getcwd()) # Show the current working directory. Should be HAPSBURG/Notebooks/ParallelRuns\n",
    "print(f\"CPU Count: {mp.cpu_count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_path(path_output, iid, ch, prefix_out, logfile=True, output=False):\n",
    "    \"\"\"Prepare the path and pipe printing for one Individual\n",
    "    logfile: Whether to pipe output to log-file\"\"\"   \n",
    "    #if not os.path.exists(path_output):\n",
    "    #        raise RuntimeError(f\"Path {path_output} not Found. Check!\")\n",
    "    path_log = os.path.join(path_output, str(iid), \"chr\"+str(ch), prefix_out, \"\")      \n",
    "    #path_log =  path_output + str(iid) + \"/chr\" + str(ch) + \"/\" + prefix_out\n",
    "    \n",
    "    if not os.path.exists(path_log):\n",
    "        if output==True:\n",
    "            print(f\"Creating {path_log}...\")\n",
    "        os.makedirs(path_log)\n",
    "    \n",
    "    if logfile == True:\n",
    "        path_log = path_log + \"hmm_run_log.txt\"\n",
    "        if output==True:\n",
    "            print(f\"Set Output Log path: {path_log}\")\n",
    "        sys.stdout = open(path_log, 'w') \n",
    "    \n",
    "def analyze_chromosome_gt(iid, ch=3, n_ref=503, save=True, save_fp=False, exclude_pops=[\"TSI\", ], \n",
    "                          base_out_folder=\"./Empirical/HO/\", prefix_out=\"gt/\",\n",
    "                          roh_in=100, roh_out=100, roh_jump=385, e_rate=0.01, e_rate_ref=0.001, \n",
    "                          max_gap=0, logfile=True):\n",
    "    \"\"\"Run the analysis for one individual and chromosome on readcount data\n",
    "    Wrapper for HMM Class. Takes 13 Parameters\"\"\"\n",
    "    \n",
    "    ### The folder on what to run the Data on (Permanently set here to fixed loaction)\n",
    "    h5_path_targets = \"./Data/Marcus2019_1240k/mod_reich_sardinia_ancients_rev_mrg_dedup_3trm_anno.h5\"\n",
    "    meta_path_targets = \"./Data/Marcus2019_1240k/meta_rev_unique_ids.csv\"  ### Path with the unique IDs per Modern Group\n",
    "    \n",
    "    ### Create Folder if needed, and pipe output if wanted\n",
    "    prepare_path(base_out_folder, iid, ch, prefix_out, logfile=logfile)\n",
    "    \n",
    "    hmm = HMM_Analyze(cython=2, p_model=\"SardHDF5\", e_model=\"diploid_gt\", post_model=\"Standard\",\n",
    "                      manual_load=True, save=save, save_fp=save_fp)\n",
    "\n",
    "    # Load and prepare the pre-processing Model\n",
    "    hmm.load_preprocessing_model()              # Load the preprocessing Model\n",
    "    hmm.p_obj.set_params(readcounts = False, destroy_phase=False,\n",
    "                prefix_out_data=prefix_out, excluded=exclude_pops, base_out_folder=base_out_folder,\n",
    "                h5_path_targets = h5_path_targets, meta_path_targets=meta_path_targets)    \n",
    "    \n",
    "    ### DELETE when run for with European Reference!!\n",
    "    hmm.p_obj.set_params(h5_path1000g = \"./Data/1000Genomes/HDF5/1240kHDF5/all1240/chr\", \n",
    "                         meta_path_ref = \"./Data/1000Genomes/Individuals/meta_df_all.csv\")\n",
    "    \n",
    "    hmm.load_data(iid=iid, ch=ch, n_ref=n_ref)  # Load the actual Data\n",
    "    hmm.load_secondary_objects()\n",
    "\n",
    "    ### Set the Parameters\n",
    "    hmm.e_obj.set_params(e_rate = e_rate, e_rate_ref = e_rate_ref)\n",
    "    hmm.t_obj.set_params(roh_in=roh_in, roh_out=roh_out, roh_jump=roh_jump)\n",
    "    hmm.post_obj.set_params(max_gap=max_gap)\n",
    "    \n",
    "    #hmm.calc_viterbi_path(save=save)           # Calculate the Viterbi Path.\n",
    "    hmm.calc_posterior(save=save)              # Calculate the Posterior.\n",
    "    hmm.post_processing(save=save)             # Do the Post-Processing.\n",
    "\n",
    "    \n",
    "#########################################################\n",
    "def combine_individual_data(base_path, iid, delete=False, chs=range(1,23), prefix_out=\"\"):\n",
    "    \"\"\"Function to merge data from one Individual Analysis (all Chromosome)\n",
    "    chs: Which Chromosomes to combine\"\n",
    "    delete: Whether to delete individual folder and contents after combining.\"\"\"\n",
    "    \n",
    "    full_df_vec =[]  # The full dataframe of inferred ROH blocks\n",
    "    \n",
    "    ### Walk through Chromosomes and combine the Dataframes\n",
    "    for ch in chs:\n",
    "        path_roh = os.path.join(base_path, str(iid), \"chr\"+str(ch), prefix_out, \"roh.csv\") \n",
    "        df_temp = pd.read_csv(path_roh, sep=\",\")\n",
    "        full_df_vec.append(df_temp)\n",
    "        \n",
    "    full_df = pd.concat(full_df_vec)\n",
    "        \n",
    "    ### Save to Path:\n",
    "    path_save = os.path.join(base_path, str(iid) + \"_roh_full.csv\")\n",
    "    full_df.to_csv(path_save, index=False)\n",
    "    \n",
    "    ### Delete files in folder if need\n",
    "    if delete == True:\n",
    "        for ch in chs:\n",
    "            path_folder = os.path.join(base_path, str(iid), \"chr\"+str(ch), prefix_out, \"\") \n",
    "            \n",
    "            for root, _, files in os.walk(path_folder):\n",
    "                for file in files:\n",
    "                    os.remove(os.path.join(root, file))\n",
    "            os.rmdir(path_folder) # Remove the Folder\n",
    "    \n",
    "    return full_df\n",
    "                            \n",
    "    \n",
    "#########################################################\n",
    "def analyze_individual_ho(iid, chs=range(1,23), n_ref=2504, save=True, save_fp=False, exclude_pops=[], \n",
    "                          base_out_folder=\"./Empirical/HO/\", prefix_out=\"\",\n",
    "                          roh_in=100, roh_out=100, roh_jump=300, e_rate=0.001, \n",
    "                          e_rate_ref=0.001, max_gap=0, logfile=True, output=True, processes=5, delete=True):\n",
    "    \"\"\"Analyze a full single individual in a parallelized fasion. Run all Chromosome analyses in parallel\n",
    "    Wrapper for analyze_chromosome_gt.\n",
    "    logfile: Whether to use a logfile\n",
    "    output: Whether to print general Output\"\"\"\n",
    "                            \n",
    "    if output == True:\n",
    "        print(f\"Doing Individual {iid}...\")\n",
    "    \n",
    "    ### Prepare the Parameters for that Indivdiual\n",
    "    prms = [[iid, ch, n_ref, save, save_fp, exclude_pops, base_out_folder, prefix_out,\n",
    "         roh_in, roh_out, roh_jump, e_rate, e_rate_ref, max_gap, logfile] for ch in chs] \n",
    "                            \n",
    "    ### Run the analysis in parallel\n",
    "    multi_run(analyze_chromosome_gt, prms, processes = 5)\n",
    "                            \n",
    "    ### Merge results for that Individual\n",
    "    combine_individual_data(base_out_folder, iid=iid, delete=delete, chs=chs)\n",
    "                            \n",
    "    return\n",
    "        \n",
    "#########################################################\n",
    "#########################################################\n",
    "    \n",
    "def multi_run(fun, prms, processes = 4):\n",
    "    \"\"\"Implementation of running in Parallel.\n",
    "    fun: Function\n",
    "    prms: The Parameter Files\n",
    "    processes: How many Processes to use\"\"\"\n",
    "    print(f\"Running {len(prms)} jobs in parallel.\")\n",
    "    \n",
    "    with mp.Pool(processes = processes) as pool:\n",
    "            results = pool.starmap(fun, prms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyze HO Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze a single Individual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "analyze_individual_ho(iid=\"Sardinian_0\", chs=range(1,23), processes=5, delete=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze Single Individual without deleting Posterior\n",
    "(For checking in more detail and plotting in ./notebooks/posterior_plot_individual.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doing Individual Sardinian_0...\n",
      "Running 1 jobs in parallel.\n",
      "CPU times: user 91.1 ms, sys: 91.8 ms, total: 183 ms\n",
      "Wall time: 2min 23s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "analyze_individual_ho(iid=\"Sardinian_0\", chs=range(3,4), processes=5, delete=False, logfile=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run a whole Population (or set of individuals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Write the Command for iid\n",
    "def give_iids_populations_ho(pop):\n",
    "    \"\"\"Return all IIDs of Population pop in meta_df (in Lazaridis HO paper)\"\"\"\n",
    "    ho_df = meta_df[meta_df[\"study\"]==\"Lazaridis et al. 2014\"]\n",
    "    iids = ho_df[\"iid\"][meta_df[\"clst\"]==pop]\n",
    "    assert(len(iids)>0)\n",
    "    return iids.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_ho_pops(pops, chs=range(1,23), delete=True, processes=5):\n",
    "    \"\"\"Run HAPSBURG on all Individuals of HO pops\"\"\"\n",
    "    \n",
    "    for pop in pops:\n",
    "        iids = give_iids_populations_ho(pop)\n",
    "        for iid in iids:\n",
    "            analyze_individual_ho(iid=iid, chs=chs, processes=processes, delete=delete)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doing Individual Yi_0...\n",
      "Running 1 jobs in parallel.\n"
     ]
    }
   ],
   "source": [
    "run_ho_pops(pops=[\"Yi\",], chs=range(1,2), delete=False, processes=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Area 51\n",
    "Area to test code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set Output Log path: ./Empirical/1240k/HO/Italian_South_0/chr4/e01/hmm_run_log.txt\n"
     ]
    }
   ],
   "source": [
    "analyze_individual_gt(*prms[3])  # Single Test Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(prms[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_path = \"./Data/Marcus2019_1240k/meta_rev_unique_ids.csv\"\n",
    "meta_df = pd.read_csv(meta_path)\n",
    "mod_df = meta_df[1098:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'?',\n",
       " 'AA',\n",
       " 'Abkhasian',\n",
       " 'Adygei',\n",
       " 'Albanian',\n",
       " 'Aleut',\n",
       " 'Algerian',\n",
       " 'Altaian',\n",
       " 'Ami',\n",
       " 'Armenian',\n",
       " 'Ashkenazi_Jew',\n",
       " 'Atayal',\n",
       " 'Australian',\n",
       " 'Balkar',\n",
       " 'Balochi',\n",
       " 'BantuKenya',\n",
       " 'BantuSA',\n",
       " 'Basque',\n",
       " 'BedouinA',\n",
       " 'BedouinB',\n",
       " 'Belarusian',\n",
       " 'Bengali',\n",
       " 'Bergamo',\n",
       " 'Biaka',\n",
       " 'Bolivian',\n",
       " 'Bougainville',\n",
       " 'Brahui',\n",
       " 'Bulgarian',\n",
       " 'Burusho',\n",
       " 'Cag',\n",
       " 'Cam',\n",
       " 'Cambodian',\n",
       " 'Canary_Islanders',\n",
       " 'Car',\n",
       " 'Chechen',\n",
       " 'Chukchi',\n",
       " 'Chuvash',\n",
       " 'Cochin_Jew',\n",
       " 'Croatian',\n",
       " 'Cypriot',\n",
       " 'Czech',\n",
       " 'Dai',\n",
       " 'Datog',\n",
       " 'Daur',\n",
       " 'Dolgan',\n",
       " 'Druze',\n",
       " 'Egyptian',\n",
       " 'English',\n",
       " 'Esan',\n",
       " 'Eskimo',\n",
       " 'Estonian',\n",
       " 'Ethiopian_Jew',\n",
       " 'Even',\n",
       " 'Finnish',\n",
       " 'French',\n",
       " 'French_South',\n",
       " 'Gambian',\n",
       " 'Georgian',\n",
       " 'Georgian_Jew',\n",
       " 'Greek',\n",
       " 'GujaratiA',\n",
       " 'GujaratiB',\n",
       " 'GujaratiC',\n",
       " 'GujaratiD',\n",
       " 'Hadza',\n",
       " 'Han',\n",
       " 'Han_NChina',\n",
       " 'Hazara',\n",
       " 'Hezhen',\n",
       " 'Hungarian',\n",
       " 'Icelandic',\n",
       " 'Iranian',\n",
       " 'Iranian_Jew',\n",
       " 'Iraqi_Jew',\n",
       " 'Italian_South',\n",
       " 'Itelmen',\n",
       " 'Japanese',\n",
       " 'Jordanian',\n",
       " 'Ju_hoan_North',\n",
       " 'Kalash',\n",
       " 'Kalmyk',\n",
       " 'Karitiana',\n",
       " 'Khomani',\n",
       " 'Kikuyu',\n",
       " 'Kinh',\n",
       " 'Korean',\n",
       " 'Koryak',\n",
       " 'Kumyk',\n",
       " 'Kusunda',\n",
       " 'Kyrgyz',\n",
       " 'Lahu',\n",
       " 'Lebanese',\n",
       " 'Lezgin',\n",
       " 'Libyan_Jew',\n",
       " 'Lithuanian',\n",
       " 'Luhya',\n",
       " 'Luo',\n",
       " 'Makrani',\n",
       " 'Maltese',\n",
       " 'Mandenka',\n",
       " 'Mansi',\n",
       " 'Masai',\n",
       " 'Mayan',\n",
       " 'Mbuti',\n",
       " 'Mende',\n",
       " 'Miao',\n",
       " 'Mixe',\n",
       " 'Mixtec',\n",
       " 'Mongola',\n",
       " 'Mordovian',\n",
       " 'Moroccan_Jew',\n",
       " 'Mozabite',\n",
       " 'Naxi',\n",
       " 'Nganasan',\n",
       " 'Nogai',\n",
       " 'North_Ossetian',\n",
       " 'Norwegian',\n",
       " 'Nuo',\n",
       " 'Ogl',\n",
       " 'Olb',\n",
       " 'Orcadian',\n",
       " 'Ori',\n",
       " 'Oroqen',\n",
       " 'Palestinian',\n",
       " 'Papuan',\n",
       " 'Pathan',\n",
       " 'Piapoco',\n",
       " 'Pima',\n",
       " 'Punjabi',\n",
       " 'Quechua',\n",
       " 'Russian',\n",
       " 'Saami_WGA',\n",
       " 'Saharawi',\n",
       " 'Sardinian',\n",
       " 'Sas',\n",
       " 'Saudi',\n",
       " 'Scottish',\n",
       " 'Selkup',\n",
       " 'She',\n",
       " 'Sicilian',\n",
       " 'Sindhi',\n",
       " 'Somali',\n",
       " 'Spanish',\n",
       " 'Spanish_North',\n",
       " 'Surui',\n",
       " 'Syrian',\n",
       " 'Tajik_Pomiri',\n",
       " 'Thai',\n",
       " 'Tlingit',\n",
       " 'Tu',\n",
       " 'Tubalar',\n",
       " 'Tujia',\n",
       " 'Tunisian',\n",
       " 'Tunisian_Jew',\n",
       " 'Turkish',\n",
       " 'Turkish_Jew',\n",
       " 'Turkmen',\n",
       " 'Tuscan',\n",
       " 'Tuvinian',\n",
       " 'Ukrainian',\n",
       " 'Ulchi',\n",
       " 'Uygur',\n",
       " 'Uzbek',\n",
       " 'Xibo',\n",
       " 'Yakut',\n",
       " 'Yemen',\n",
       " 'Yemenite_Jew',\n",
       " 'Yi',\n",
       " 'Yoruba',\n",
       " 'Yukagir',\n",
       " 'Zapotec'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#mod_df[\"clst\"].value_counts()\n",
    "set(mod_df[\"clst\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>iid</th>\n",
       "      <th>label</th>\n",
       "      <th>lat</th>\n",
       "      <th>lon</th>\n",
       "      <th>x_contam</th>\n",
       "      <th>mt_contam</th>\n",
       "      <th>age</th>\n",
       "      <th>study</th>\n",
       "      <th>clst_alt</th>\n",
       "      <th>period_alt</th>\n",
       "      <th>include_alt</th>\n",
       "      <th>clst</th>\n",
       "      <th>mean_cov</th>\n",
       "      <th>med_cov</th>\n",
       "      <th>n_cov_snp_read</th>\n",
       "      <th>full_iid</th>\n",
       "      <th>n_cov_snp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1840</th>\n",
       "      <td>Yi_0</td>\n",
       "      <td>Yi</td>\n",
       "      <td>28.0</td>\n",
       "      <td>103.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Lazaridis et al. 2014</td>\n",
       "      <td>Yi</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>Yi</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>HGDP01179</td>\n",
       "      <td>552493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1841</th>\n",
       "      <td>Yi_1</td>\n",
       "      <td>Yi</td>\n",
       "      <td>28.0</td>\n",
       "      <td>103.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Lazaridis et al. 2014</td>\n",
       "      <td>Yi</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>Yi</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>HGDP01180</td>\n",
       "      <td>555469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1842</th>\n",
       "      <td>Yi_2</td>\n",
       "      <td>Yi</td>\n",
       "      <td>28.0</td>\n",
       "      <td>103.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Lazaridis et al. 2014</td>\n",
       "      <td>Yi</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>Yi</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>HGDP01181</td>\n",
       "      <td>555431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1843</th>\n",
       "      <td>Yi_3</td>\n",
       "      <td>Yi</td>\n",
       "      <td>28.0</td>\n",
       "      <td>103.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Lazaridis et al. 2014</td>\n",
       "      <td>Yi</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>Yi</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>HGDP01182</td>\n",
       "      <td>554322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1844</th>\n",
       "      <td>Yi_4</td>\n",
       "      <td>Yi</td>\n",
       "      <td>28.0</td>\n",
       "      <td>103.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Lazaridis et al. 2014</td>\n",
       "      <td>Yi</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>Yi</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>HGDP01183</td>\n",
       "      <td>555310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1845</th>\n",
       "      <td>Yi_5</td>\n",
       "      <td>Yi</td>\n",
       "      <td>28.0</td>\n",
       "      <td>103.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Lazaridis et al. 2014</td>\n",
       "      <td>Yi</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>Yi</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>HGDP01184</td>\n",
       "      <td>555543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1846</th>\n",
       "      <td>Yi_6</td>\n",
       "      <td>Yi</td>\n",
       "      <td>28.0</td>\n",
       "      <td>103.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Lazaridis et al. 2014</td>\n",
       "      <td>Yi</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>Yi</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>HGDP01185</td>\n",
       "      <td>555487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1847</th>\n",
       "      <td>Yi_7</td>\n",
       "      <td>Yi</td>\n",
       "      <td>28.0</td>\n",
       "      <td>103.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Lazaridis et al. 2014</td>\n",
       "      <td>Yi</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>Yi</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>HGDP01186</td>\n",
       "      <td>555184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1848</th>\n",
       "      <td>Yi_8</td>\n",
       "      <td>Yi</td>\n",
       "      <td>28.0</td>\n",
       "      <td>103.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Lazaridis et al. 2014</td>\n",
       "      <td>Yi</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>Yi</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>HGDP01187</td>\n",
       "      <td>554827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1849</th>\n",
       "      <td>Yi_9</td>\n",
       "      <td>Yi</td>\n",
       "      <td>28.0</td>\n",
       "      <td>103.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Lazaridis et al. 2014</td>\n",
       "      <td>Yi</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>Yi</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>HGDP01188</td>\n",
       "      <td>553935</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       iid label   lat    lon  x_contam  mt_contam  age  \\\n",
       "1840  Yi_0    Yi  28.0  103.0       NaN        NaN  0.0   \n",
       "1841  Yi_1    Yi  28.0  103.0       NaN        NaN  0.0   \n",
       "1842  Yi_2    Yi  28.0  103.0       NaN        NaN  0.0   \n",
       "1843  Yi_3    Yi  28.0  103.0       NaN        NaN  0.0   \n",
       "1844  Yi_4    Yi  28.0  103.0       NaN        NaN  0.0   \n",
       "1845  Yi_5    Yi  28.0  103.0       NaN        NaN  0.0   \n",
       "1846  Yi_6    Yi  28.0  103.0       NaN        NaN  0.0   \n",
       "1847  Yi_7    Yi  28.0  103.0       NaN        NaN  0.0   \n",
       "1848  Yi_8    Yi  28.0  103.0       NaN        NaN  0.0   \n",
       "1849  Yi_9    Yi  28.0  103.0       NaN        NaN  0.0   \n",
       "\n",
       "                      study clst_alt period_alt  include_alt clst  mean_cov  \\\n",
       "1840  Lazaridis et al. 2014       Yi        NaN            1   Yi       NaN   \n",
       "1841  Lazaridis et al. 2014       Yi        NaN            1   Yi       NaN   \n",
       "1842  Lazaridis et al. 2014       Yi        NaN            1   Yi       NaN   \n",
       "1843  Lazaridis et al. 2014       Yi        NaN            1   Yi       NaN   \n",
       "1844  Lazaridis et al. 2014       Yi        NaN            1   Yi       NaN   \n",
       "1845  Lazaridis et al. 2014       Yi        NaN            1   Yi       NaN   \n",
       "1846  Lazaridis et al. 2014       Yi        NaN            1   Yi       NaN   \n",
       "1847  Lazaridis et al. 2014       Yi        NaN            1   Yi       NaN   \n",
       "1848  Lazaridis et al. 2014       Yi        NaN            1   Yi       NaN   \n",
       "1849  Lazaridis et al. 2014       Yi        NaN            1   Yi       NaN   \n",
       "\n",
       "      med_cov  n_cov_snp_read   full_iid  n_cov_snp  \n",
       "1840      NaN             NaN  HGDP01179     552493  \n",
       "1841      NaN             NaN  HGDP01180     555469  \n",
       "1842      NaN             NaN  HGDP01181     555431  \n",
       "1843      NaN             NaN  HGDP01182     554322  \n",
       "1844      NaN             NaN  HGDP01183     555310  \n",
       "1845      NaN             NaN  HGDP01184     555543  \n",
       "1846      NaN             NaN  HGDP01185     555487  \n",
       "1847      NaN             NaN  HGDP01186     555184  \n",
       "1848      NaN             NaN  HGDP01187     554827  \n",
       "1849      NaN             NaN  HGDP01188     553935  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mod_df[mod_df[\"clst\"]==\"Yi\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Start</th>\n",
       "      <th>End</th>\n",
       "      <th>StartM</th>\n",
       "      <th>EndM</th>\n",
       "      <th>length</th>\n",
       "      <th>lengthM</th>\n",
       "      <th>iid</th>\n",
       "      <th>ch</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>22400</td>\n",
       "      <td>22557</td>\n",
       "      <td>1.370264</td>\n",
       "      <td>1.384025</td>\n",
       "      <td>157</td>\n",
       "      <td>0.013761</td>\n",
       "      <td>Sardinian_0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12447</td>\n",
       "      <td>13306</td>\n",
       "      <td>0.782127</td>\n",
       "      <td>0.817144</td>\n",
       "      <td>859</td>\n",
       "      <td>0.035017</td>\n",
       "      <td>Sardinian_0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>39936</td>\n",
       "      <td>40095</td>\n",
       "      <td>2.120861</td>\n",
       "      <td>2.133356</td>\n",
       "      <td>159</td>\n",
       "      <td>0.012495</td>\n",
       "      <td>Sardinian_0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Start    End    StartM      EndM length   lengthM          iid ch\n",
       "0  22400  22557  1.370264  1.384025    157  0.013761  Sardinian_0  1\n",
       "0  12447  13306  0.782127  0.817144    859  0.035017  Sardinian_0  2\n",
       "1  39936  40095  2.120861  2.133356    159  0.012495  Sardinian_0  2"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1941"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ho_df = meta_df[meta_df[\"study\"]==\"Lazaridis et al. 2014\"]\n",
    "len(ho_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Yoruba              70\n",
       "Turkish             56\n",
       "Spanish             53\n",
       "Druze               39\n",
       "Palestinian         38\n",
       "Han                 33\n",
       "Japanese            29\n",
       "Basque              29\n",
       "Sardinian           27\n",
       "BedouinA            25\n",
       "Ulchi               25\n",
       "French              25\n",
       "Burusho             23\n",
       "Chukchi             23\n",
       "Eskimo              22\n",
       "Russian             22\n",
       "Tubalar             22\n",
       "Mozabite            21\n",
       "Brahui              21\n",
       "Balochi             20\n",
       "Yakut               20\n",
       "Biaka               20\n",
       "Makrani             20\n",
       "Hungarian           20\n",
       "Greek               20\n",
       "Yukagir             19\n",
       "BedouinB            19\n",
       "Pathan              19\n",
       "Kalash              18\n",
       "Sindhi              18\n",
       "                    ..\n",
       "Finnish              7\n",
       "Korean               6\n",
       "Itelmen              6\n",
       "Iraqi_Jew            6\n",
       "Albanian             6\n",
       "Yemen                6\n",
       "Mongola              6\n",
       "BantuKenya           6\n",
       "Saharawi             6\n",
       "Gambian              6\n",
       "Moroccan_Jew         6\n",
       "Quechua              5\n",
       "Ju_hoan_North        5\n",
       "GujaratiB            5\n",
       "GujaratiD            5\n",
       "Spanish_North        5\n",
       "Cochin_Jew           5\n",
       "GujaratiA            5\n",
       "GujaratiC            5\n",
       "Hadza                5\n",
       "Tlingit              4\n",
       "Scottish             4\n",
       "Piapoco              4\n",
       "Kikuyu               4\n",
       "Dolgan               3\n",
       "Australian           3\n",
       "Datog                3\n",
       "Canary_Islanders     2\n",
       "Italian_South        1\n",
       "Saami_WGA            1\n",
       "Name: clst, Length: 162, dtype: int64"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ho_df[\"clst\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "162"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ho_df[\"clst\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
