{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook to call ROH in parallel within HO origin Individuals\n",
    "Notebooks that import the code for the calling ROHs on diploid genotype individuals, and then a function to parallelize it.\n",
    "\n",
    "Very similar to parallel_mosaic_callroh.ipynb\n",
    "\n",
    "\n",
    "@Author: Harald Ringbauer, September 2019\n",
    "All rights reserved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "midway2-0402.rcc.local\n",
      "Midway jnovmbre partition detected.\n",
      "/project2/jnovembre/hringbauer/HAPSBURG\n",
      "CPU Count: 28\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os as os\n",
    "import sys as sys\n",
    "import multiprocessing as mp\n",
    "import pandas as pd\n",
    "import socket\n",
    "\n",
    "### Pick the right path (whether on cluster or at home)\n",
    "socket_name = socket.gethostname()\n",
    "print(socket_name)\n",
    "if socket_name == \"VioletQueen\":\n",
    "    path = \"/home/harald/git/HAPSBURG/\"   # The Path on Harald's machine\n",
    "elif socket_name.startswith(\"midway2\"):\n",
    "    print(\"Midway jnovmbre partition detected.\")\n",
    "    path = \"/project2/jnovembre/hringbauer/HAPSBURG/\"  # The Path on Midway Cluster\n",
    "else: \n",
    "    raise RuntimeWarning(\"Not compatible machine. Check!!\")\n",
    "    \n",
    "os.chdir(path)  # Set the right Path (in line with Atom default)\n",
    "\n",
    "sys.path.append(\"./hapsburg/\")  # Since now we are in the Root Directory\n",
    "from hmm_inference import HMM_Analyze   # Do not move. Should be after sys.path..\n",
    "from PackagesSupport.pp_individual_roh_csvs import pp_individual_roh\n",
    "#from createMosaicsMulti import Mosaic_1000G_Multi  # Import the object that can create the Multiruns\n",
    "\n",
    "print(os.getcwd()) # Show the current working directory. Should be HAPSBURG/Notebooks/ParallelRuns\n",
    "print(f\"CPU Count: {mp.cpu_count()}\")\n",
    "\n",
    "### Load the Meta File\n",
    "meta_path = \"./Data/Marcus2019_1240k/meta_rev_unique_ids.csv\"\n",
    "meta_df = pd.read_csv(meta_path)\n",
    "mod_df = meta_df[1098:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_path(path_output, iid, ch, prefix_out, logfile=True, output=False):\n",
    "    \"\"\"Prepare the path and pipe printing for one Individual\n",
    "    logfile: Whether to pipe output to log-file\"\"\"   \n",
    "    #if not os.path.exists(path_output):\n",
    "    #        raise RuntimeError(f\"Path {path_output} not Found. Check!\")\n",
    "    path_log = os.path.join(path_output, str(iid), \"chr\"+str(ch), prefix_out, \"\")      \n",
    "    #path_log =  path_output + str(iid) + \"/chr\" + str(ch) + \"/\" + prefix_out\n",
    "    \n",
    "    if not os.path.exists(path_log):\n",
    "        if output==True:\n",
    "            print(f\"Creating {path_log}...\")\n",
    "        os.makedirs(path_log)\n",
    "    \n",
    "    if logfile == True:\n",
    "        path_log = path_log + \"hmm_run_log.txt\"\n",
    "        if output==True:\n",
    "            print(f\"Set Output Log path: {path_log}\")\n",
    "        sys.stdout = open(path_log, 'w') \n",
    "    \n",
    "def analyze_chromosome_gt(iid, ch=3, n_ref=503, save=True, save_fp=False, exclude_pops=[\"TSI\", ], \n",
    "                          base_out_folder=\"./Empirical/HO/\", prefix_out=\"gt/\",\n",
    "                          roh_in=100, roh_out=100, roh_jump=385, e_rate=0.01, e_rate_ref=0.001, \n",
    "                          max_gap=0, logfile=True):\n",
    "    \"\"\"Run the analysis for one individual and chromosome on readcount data\n",
    "    Wrapper for HMM Class. Takes 13 Parameters\"\"\"\n",
    "    \n",
    "    ### The folder on what to run the Data on (Permanently set here to fixed loaction)\n",
    "    h5_path_targets = \"./Data/Marcus2019_1240k/mod_reich_sardinia_ancients_rev_mrg_dedup_3trm_anno.h5\"\n",
    "    meta_path_targets = \"./Data/Marcus2019_1240k/meta_rev_unique_ids.csv\"  ### Path with the unique IDs per Modern Group\n",
    "    \n",
    "    ### Create Folder if needed, and pipe output if wanted\n",
    "    prepare_path(base_out_folder, iid, ch, prefix_out, logfile=logfile)\n",
    "    \n",
    "    hmm = HMM_Analyze(cython=2, p_model=\"SardHDF5\", e_model=\"diploid_gt\", post_model=\"Standard\",\n",
    "                      manual_load=True, save=save, save_fp=save_fp)\n",
    "\n",
    "    # Load and prepare the pre-processing Model\n",
    "    hmm.load_preprocessing_model()              # Load the preprocessing Model\n",
    "    hmm.p_obj.set_params(readcounts = False, destroy_phase=False,\n",
    "                prefix_out_data=prefix_out, excluded=exclude_pops, base_out_folder=base_out_folder,\n",
    "                h5_path_targets = h5_path_targets, meta_path_targets=meta_path_targets)    \n",
    "    \n",
    "    ### DELETE when run for with European Reference!!\n",
    "    hmm.p_obj.set_params(h5_path1000g = \"./Data/1000Genomes/HDF5/1240kHDF5/all1240/chr\", \n",
    "                         meta_path_ref = \"./Data/1000Genomes/Individuals/meta_df_all.csv\")\n",
    "    \n",
    "    hmm.load_data(iid=iid, ch=ch, n_ref=n_ref)  # Load the actual Data\n",
    "    hmm.load_secondary_objects()\n",
    "\n",
    "    ### Set the Parameters\n",
    "    hmm.e_obj.set_params(e_rate = e_rate, e_rate_ref = e_rate_ref)\n",
    "    hmm.t_obj.set_params(roh_in=roh_in, roh_out=roh_out, roh_jump=roh_jump)\n",
    "    hmm.post_obj.set_params(max_gap=max_gap)\n",
    "    \n",
    "    #hmm.calc_viterbi_path(save=save)           # Calculate the Viterbi Path.\n",
    "    hmm.calc_posterior(save=save)              # Calculate the Posterior.\n",
    "    hmm.post_processing(save=save)             # Do the Post-Processing.\n",
    "\n",
    "    \n",
    "#########################################################\n",
    "def combine_individual_data(base_path, iid, delete=False, chs=range(1,23), prefix_out=\"\"):\n",
    "    \"\"\"Function to merge data from one Individual Analysis (all Chromosome)\n",
    "    chs: Which Chromosomes to combine\"\n",
    "    delete: Whether to delete individual folder and contents after combining.\"\"\"\n",
    "    \n",
    "    full_df_vec =[]  # The full dataframe of inferred ROH blocks\n",
    "    \n",
    "    ### Walk through Chromosomes and combine the Dataframes\n",
    "    for ch in chs:\n",
    "        path_roh = os.path.join(base_path, str(iid), \"chr\"+str(ch), prefix_out, \"roh.csv\") \n",
    "        df_temp = pd.read_csv(path_roh, sep=\",\")\n",
    "        full_df_vec.append(df_temp)\n",
    "        \n",
    "    full_df = pd.concat(full_df_vec)\n",
    "        \n",
    "    ### Save to Path:\n",
    "    path_save = os.path.join(base_path, str(iid) + \"_roh_full.csv\")\n",
    "    full_df.to_csv(path_save, index=False)\n",
    "    \n",
    "    ### Delete files in folder if need\n",
    "    if delete == True:\n",
    "        for ch in chs:\n",
    "            path_folder = os.path.join(base_path, str(iid), \"chr\"+str(ch), prefix_out, \"\") \n",
    "            \n",
    "            for root, _, files in os.walk(path_folder):\n",
    "                for file in files:\n",
    "                    os.remove(os.path.join(root, file))\n",
    "            os.rmdir(path_folder) # Remove the Chromosome Folders\n",
    "        os.rmdir(os.path.join(base_path, str(iid), \"\"))  # Remove the Individual Folder\n",
    "    \n",
    "    return full_df\n",
    "                             \n",
    "#########################################################\n",
    "def analyze_individual_ho(iid, chs=range(1,23), n_ref=2504, save=True, save_fp=False, exclude_pops=[], \n",
    "                          base_out_folder=\"./Empirical/HO/\", prefix_out=\"\",\n",
    "                          roh_in=100, roh_out=100, roh_jump=300, e_rate=0.001, \n",
    "                          e_rate_ref=0.001, max_gap=0, logfile=True, output=True, processes=5, delete=True):\n",
    "    \"\"\"Analyze a full single individual in a parallelized fasion. Run all Chromosome analyses in parallel\n",
    "    Wrapper for analyze_chromosome_gt.\n",
    "    logfile: Whether to use a logfile\n",
    "    output: Whether to print general Output\"\"\"\n",
    "                            \n",
    "    if output == True:\n",
    "        print(f\"Doing Individual {iid}...\")\n",
    "    \n",
    "    ### Prepare the Parameters for that Indivdiual\n",
    "    prms = [[iid, ch, n_ref, save, save_fp, exclude_pops, base_out_folder, prefix_out,\n",
    "         roh_in, roh_out, roh_jump, e_rate, e_rate_ref, max_gap, logfile] for ch in chs] \n",
    "                            \n",
    "    ### Run the analysis in parallel\n",
    "    multi_run(analyze_chromosome_gt, prms, processes = processes)\n",
    "                            \n",
    "    ### Merge results for that Individual\n",
    "    combine_individual_data(base_out_folder, iid=iid, delete=delete, chs=chs)\n",
    "                            \n",
    "    return\n",
    "        \n",
    "#########################################################\n",
    "#########################################################\n",
    "    \n",
    "def multi_run(fun, prms, processes = 4):\n",
    "    \"\"\"Implementation of running in Parallel.\n",
    "    fun: Function\n",
    "    prms: The Parameter Files\n",
    "    processes: How many Processes to use\"\"\"\n",
    "    print(f\"Running {len(prms)} jobs in parallel.\")\n",
    "    \n",
    "    with mp.Pool(processes = processes) as pool:\n",
    "            results = pool.starmap(fun, prms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyze HO Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze a single Individual\n",
    "For reanalysis with delete=True to plot that indivdual / further analysis of posterior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doing Individual Croatian_5...\n",
      "Running 22 jobs in parallel.\n",
      "CPU times: user 409 ms, sys: 116 ms, total: 525 ms\n",
      "Wall time: 5min 46s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "analyze_individual_ho(iid=\"Croatian_5\", chs=range(1,23), processes=6, delete=False, logfile=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run a whole HO Population or Range of HO individuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Write the Command for iid\n",
    "def give_iids_populations_ho(pop):\n",
    "    \"\"\"Return all IIDs of Population pop in meta_df (in Lazaridis HO paper)\"\"\"\n",
    "    ho_df = meta_df[meta_df[\"study\"]==\"Lazaridis et al. 2014\"]\n",
    "    iids = ho_df[\"iid\"][meta_df[\"clst\"]==pop]\n",
    "    assert(len(iids)>0)\n",
    "    return iids.values\n",
    "\n",
    "def give_ho_iids_all():\n",
    "    \"\"\"Return individual IIDs of all HO samples\"\"\"\n",
    "    ho_df = meta_df[meta_df[\"study\"]==\"Lazaridis et al. 2014\"]\n",
    "    iids = ho_df[\"iid\"].values  # Extract Individuals\n",
    "    return iids\n",
    "\n",
    "def run_ho_pops(pops, chs=range(1,23), delete=True, processes=5, base_out_folder=\"./Empirical/HO/\"):\n",
    "    \"\"\"Run HAPSBURG on all Individuals of HO pops\"\"\"\n",
    "    for pop in pops:\n",
    "        iids = give_iids_populations_ho(pop)\n",
    "        for iid in iids:\n",
    "            analyze_individual_ho(iid=iid, chs=chs, processes=processes, delete=delete)\n",
    "                   \n",
    "def run_ho_inds(ind_range=[], chs=range(1,23), delete=True, processes=5, base_out_folder=\"./Empirical/HO/\"):\n",
    "    \"\"\"Run batches of HO Individuals, 1 Individual a time (parallelized)\"\"\"\n",
    "    iids = give_ho_iids_all()\n",
    "    iids = iids[ind_range]\n",
    "    for iid in iids:\n",
    "        analyze_individual_ho(iid=iid, chs=chs, processes=processes, delete=delete, base_out_folder=base_out_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doing Individual Yi_0...\n",
      "Running 1 jobs in parallel.\n"
     ]
    }
   ],
   "source": [
    "run_ho_pops(pops=[\"Yi\",], chs=range(1,2), delete=False, processes=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create HO Analysis Data. Run in batches of ind_range (to not submit everything at once)\n",
    "This is the cell that does the final data analysis\n",
    "\n",
    "Analysis counter: Done until 130 (Python Indexing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doing Individual Chukchi_8...\n",
      "Running 22 jobs in parallel.\n",
      "Doing Individual Chukchi_9...\n",
      "Running 22 jobs in parallel.\n",
      "Doing Individual Chukchi_10...\n",
      "Running 22 jobs in parallel.\n",
      "Doing Individual Eskimo_3...\n",
      "Running 22 jobs in parallel.\n",
      "Doing Individual Ulchi_0...\n",
      "Running 22 jobs in parallel.\n",
      "Doing Individual Ulchi_1...\n",
      "Running 22 jobs in parallel.\n",
      "Doing Individual Ulchi_2...\n",
      "Running 22 jobs in parallel.\n",
      "Doing Individual Tubalar_0...\n",
      "Running 22 jobs in parallel.\n",
      "Doing Individual Tubalar_1...\n",
      "Running 22 jobs in parallel.\n",
      "Doing Individual Yukagir_10...\n",
      "Running 22 jobs in parallel.\n",
      "Doing Individual Even_0...\n",
      "Running 22 jobs in parallel.\n",
      "Doing Individual Eskimo_4...\n",
      "Running 22 jobs in parallel.\n",
      "Doing Individual Koryak_0...\n",
      "Running 22 jobs in parallel.\n",
      "Doing Individual Ulchi_3...\n",
      "Running 22 jobs in parallel.\n",
      "Doing Individual Ulchi_4...\n",
      "Running 22 jobs in parallel.\n",
      "Doing Individual Tubalar_2...\n",
      "Running 22 jobs in parallel.\n",
      "Doing Individual Tubalar_3...\n",
      "Running 22 jobs in parallel.\n",
      "Doing Individual Tubalar_4...\n",
      "Running 22 jobs in parallel.\n",
      "Doing Individual Even_1...\n",
      "Running 22 jobs in parallel.\n",
      "Doing Individual Even_2...\n",
      "Running 22 jobs in parallel.\n",
      "Doing Individual Eskimo_5...\n",
      "Running 22 jobs in parallel.\n",
      "Doing Individual Koryak_1...\n",
      "Running 22 jobs in parallel.\n",
      "Doing Individual Itelmen_0...\n",
      "Running 22 jobs in parallel.\n",
      "Doing Individual Ulchi_5...\n",
      "Running 22 jobs in parallel.\n",
      "Doing Individual Tubalar_5...\n",
      "Running 22 jobs in parallel.\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "run_ho_inds(ind_range=range(30,130), chs=range(1,23), delete=True, processes=6, base_out_folder=\"./Empirical/HO/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Postprocess all Individuals into one Dataframe\n",
    "1) Get all Individual IIDs\n",
    "2) Combine results into one Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Get all IIDs\n",
    "iids = give_ho_iids_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1941 / 4616 Individuals from Meta\n",
      "Saved to: ./Empirical/HO/CombinedROH/combined_roh05.csv\n",
      "CPU times: user 3min 46s, sys: 484 ms, total: 3min 46s\n",
      "Wall time: 4min 1s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df1 = pp_individual_roh(iids[:], meta_path=meta_path, base_folder=\"./Empirical/HO/\",\n",
    "                        save_path=\"./Empirical/HO/CombinedROH/combined_roh05.csv\", \n",
    "                        output=False, min_cm=[4,8,12,20], snp_cm=50, \n",
    "                        gap=0.5, min_len1=2, min_len2=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>iid</th>\n",
       "      <th>pop</th>\n",
       "      <th>max_roh</th>\n",
       "      <th>sum_roh&gt;4</th>\n",
       "      <th>n_roh&gt;4</th>\n",
       "      <th>sum_roh&gt;8</th>\n",
       "      <th>n_roh&gt;8</th>\n",
       "      <th>sum_roh&gt;12</th>\n",
       "      <th>n_roh&gt;12</th>\n",
       "      <th>sum_roh&gt;20</th>\n",
       "      <th>...</th>\n",
       "      <th>study</th>\n",
       "      <th>clst_alt</th>\n",
       "      <th>period_alt</th>\n",
       "      <th>include_alt</th>\n",
       "      <th>clst</th>\n",
       "      <th>mean_cov</th>\n",
       "      <th>med_cov</th>\n",
       "      <th>n_cov_snp_read</th>\n",
       "      <th>full_iid</th>\n",
       "      <th>n_cov_snp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Khomani_7</td>\n",
       "      <td>Khomani</td>\n",
       "      <td>32.090604</td>\n",
       "      <td>197.364506</td>\n",
       "      <td>12</td>\n",
       "      <td>177.142608</td>\n",
       "      <td>9</td>\n",
       "      <td>136.365103</td>\n",
       "      <td>5</td>\n",
       "      <td>136.365103</td>\n",
       "      <td>...</td>\n",
       "      <td>Lazaridis et al. 2014</td>\n",
       "      <td>Khomani</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>Khomani</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>SA083</td>\n",
       "      <td>555325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Yukagir_0</td>\n",
       "      <td>Yukagir</td>\n",
       "      <td>41.506994</td>\n",
       "      <td>167.379575</td>\n",
       "      <td>12</td>\n",
       "      <td>146.839982</td>\n",
       "      <td>8</td>\n",
       "      <td>128.102284</td>\n",
       "      <td>6</td>\n",
       "      <td>64.607191</td>\n",
       "      <td>...</td>\n",
       "      <td>Lazaridis et al. 2014</td>\n",
       "      <td>Yukagir</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>Yukagir</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Yuk_009</td>\n",
       "      <td>554701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Yukagir_1</td>\n",
       "      <td>Yukagir</td>\n",
       "      <td>17.948800</td>\n",
       "      <td>35.638695</td>\n",
       "      <td>4</td>\n",
       "      <td>17.948800</td>\n",
       "      <td>1</td>\n",
       "      <td>17.948800</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>Lazaridis et al. 2014</td>\n",
       "      <td>Yukagir</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>Yukagir</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Yuk_025</td>\n",
       "      <td>555245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Khomani_4</td>\n",
       "      <td>Khomani</td>\n",
       "      <td>5.583096</td>\n",
       "      <td>5.583096</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>Lazaridis et al. 2014</td>\n",
       "      <td>Khomani</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>Khomani</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>SA073</td>\n",
       "      <td>555490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Khomani_3</td>\n",
       "      <td>Khomani</td>\n",
       "      <td>4.016301</td>\n",
       "      <td>4.016301</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>Lazaridis et al. 2014</td>\n",
       "      <td>Khomani</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>Khomani</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>SA064</td>\n",
       "      <td>555470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Khomani_0</td>\n",
       "      <td>Khomani</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>Lazaridis et al. 2014</td>\n",
       "      <td>Khomani</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>Khomani</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>SA1004</td>\n",
       "      <td>555253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Khomani_1</td>\n",
       "      <td>Khomani</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>Lazaridis et al. 2014</td>\n",
       "      <td>Khomani</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>Khomani</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>SA063</td>\n",
       "      <td>555054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Khomani_2</td>\n",
       "      <td>Khomani</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>Lazaridis et al. 2014</td>\n",
       "      <td>Khomani</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>Khomani</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>SA010</td>\n",
       "      <td>555387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Khomani_5</td>\n",
       "      <td>Khomani</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>Lazaridis et al. 2014</td>\n",
       "      <td>Khomani</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>Khomani</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>SA1025</td>\n",
       "      <td>554876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Khomani_6</td>\n",
       "      <td>Khomani</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>Lazaridis et al. 2014</td>\n",
       "      <td>Khomani</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>Khomani</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>SA078</td>\n",
       "      <td>555066</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         iid      pop    max_roh   sum_roh>4  n_roh>4   sum_roh>8  n_roh>8  \\\n",
       "0  Khomani_7  Khomani  32.090604  197.364506       12  177.142608        9   \n",
       "1  Yukagir_0  Yukagir  41.506994  167.379575       12  146.839982        8   \n",
       "2  Yukagir_1  Yukagir  17.948800   35.638695        4   17.948800        1   \n",
       "3  Khomani_4  Khomani   5.583096    5.583096        1    0.000000        0   \n",
       "4  Khomani_3  Khomani   4.016301    4.016301        1    0.000000        0   \n",
       "5  Khomani_0  Khomani   0.000000    0.000000        0    0.000000        0   \n",
       "6  Khomani_1  Khomani   0.000000    0.000000        0    0.000000        0   \n",
       "7  Khomani_2  Khomani   0.000000    0.000000        0    0.000000        0   \n",
       "8  Khomani_5  Khomani   0.000000    0.000000        0    0.000000        0   \n",
       "9  Khomani_6  Khomani   0.000000    0.000000        0    0.000000        0   \n",
       "\n",
       "   sum_roh>12  n_roh>12  sum_roh>20    ...                     study clst_alt  \\\n",
       "0  136.365103         5  136.365103    ...     Lazaridis et al. 2014  Khomani   \n",
       "1  128.102284         6   64.607191    ...     Lazaridis et al. 2014  Yukagir   \n",
       "2   17.948800         1    0.000000    ...     Lazaridis et al. 2014  Yukagir   \n",
       "3    0.000000         0    0.000000    ...     Lazaridis et al. 2014  Khomani   \n",
       "4    0.000000         0    0.000000    ...     Lazaridis et al. 2014  Khomani   \n",
       "5    0.000000         0    0.000000    ...     Lazaridis et al. 2014  Khomani   \n",
       "6    0.000000         0    0.000000    ...     Lazaridis et al. 2014  Khomani   \n",
       "7    0.000000         0    0.000000    ...     Lazaridis et al. 2014  Khomani   \n",
       "8    0.000000         0    0.000000    ...     Lazaridis et al. 2014  Khomani   \n",
       "9    0.000000         0    0.000000    ...     Lazaridis et al. 2014  Khomani   \n",
       "\n",
       "   period_alt  include_alt     clst  mean_cov  med_cov n_cov_snp_read  \\\n",
       "0         NaN            1  Khomani       NaN      NaN            NaN   \n",
       "1         NaN            1  Yukagir       NaN      NaN            NaN   \n",
       "2         NaN            1  Yukagir       NaN      NaN            NaN   \n",
       "3         NaN            1  Khomani       NaN      NaN            NaN   \n",
       "4         NaN            1  Khomani       NaN      NaN            NaN   \n",
       "5         NaN            1  Khomani       NaN      NaN            NaN   \n",
       "6         NaN            1  Khomani       NaN      NaN            NaN   \n",
       "7         NaN            1  Khomani       NaN      NaN            NaN   \n",
       "8         NaN            1  Khomani       NaN      NaN            NaN   \n",
       "9         NaN            1  Khomani       NaN      NaN            NaN   \n",
       "\n",
       "  full_iid n_cov_snp  \n",
       "0    SA083    555325  \n",
       "1  Yuk_009    554701  \n",
       "2  Yuk_025    555245  \n",
       "3    SA073    555490  \n",
       "4    SA064    555470  \n",
       "5   SA1004    555253  \n",
       "6    SA063    555054  \n",
       "7    SA010    555387  \n",
       "8   SA1025    554876  \n",
       "9    SA078    555066  \n",
       "\n",
       "[10 rows x 27 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Area 51\n",
    "Area to test code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set Output Log path: ./Empirical/1240k/HO/Italian_South_0/chr4/e01/hmm_run_log.txt\n"
     ]
    }
   ],
   "source": [
    "analyze_individual_gt(*prms[3])  # Single Test Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(prms[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mod_df[\"clst\"].value_counts()\n",
    "set(mod_df[\"clst\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Start</th>\n",
       "      <th>End</th>\n",
       "      <th>StartM</th>\n",
       "      <th>EndM</th>\n",
       "      <th>length</th>\n",
       "      <th>lengthM</th>\n",
       "      <th>iid</th>\n",
       "      <th>ch</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>22400</td>\n",
       "      <td>22557</td>\n",
       "      <td>1.370264</td>\n",
       "      <td>1.384025</td>\n",
       "      <td>157</td>\n",
       "      <td>0.013761</td>\n",
       "      <td>Sardinian_0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12447</td>\n",
       "      <td>13306</td>\n",
       "      <td>0.782127</td>\n",
       "      <td>0.817144</td>\n",
       "      <td>859</td>\n",
       "      <td>0.035017</td>\n",
       "      <td>Sardinian_0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>39936</td>\n",
       "      <td>40095</td>\n",
       "      <td>2.120861</td>\n",
       "      <td>2.133356</td>\n",
       "      <td>159</td>\n",
       "      <td>0.012495</td>\n",
       "      <td>Sardinian_0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Start    End    StartM      EndM length   lengthM          iid ch\n",
       "0  22400  22557  1.370264  1.384025    157  0.013761  Sardinian_0  1\n",
       "0  12447  13306  0.782127  0.817144    859  0.035017  Sardinian_0  2\n",
       "1  39936  40095  2.120861  2.133356    159  0.012495  Sardinian_0  2"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1941"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ho_df = meta_df[meta_df[\"study\"]==\"Lazaridis et al. 2014\"]\n",
    "len(ho_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Yoruba              70\n",
       "Turkish             56\n",
       "Spanish             53\n",
       "Druze               39\n",
       "Palestinian         38\n",
       "Han                 33\n",
       "Japanese            29\n",
       "Basque              29\n",
       "Sardinian           27\n",
       "French              25\n",
       "Ulchi               25\n",
       "BedouinA            25\n",
       "Burusho             23\n",
       "Chukchi             23\n",
       "Tubalar             22\n",
       "Russian             22\n",
       "Eskimo              22\n",
       "Brahui              21\n",
       "Mozabite            21\n",
       "Hungarian           20\n",
       "Biaka               20\n",
       "Balochi             20\n",
       "Yakut               20\n",
       "Makrani             20\n",
       "Greek               20\n",
       "Pathan              19\n",
       "BedouinB            19\n",
       "Yukagir             19\n",
       "Kalash              18\n",
       "Mayan               18\n",
       "                    ..\n",
       "Finnish              7\n",
       "BantuKenya           6\n",
       "Korean               6\n",
       "Itelmen              6\n",
       "Saharawi             6\n",
       "Iraqi_Jew            6\n",
       "Moroccan_Jew         6\n",
       "Mongola              6\n",
       "Yemen                6\n",
       "Gambian              6\n",
       "Albanian             6\n",
       "GujaratiD            5\n",
       "Cochin_Jew           5\n",
       "GujaratiA            5\n",
       "Hadza                5\n",
       "Ju_hoan_North        5\n",
       "GujaratiC            5\n",
       "GujaratiB            5\n",
       "Spanish_North        5\n",
       "Quechua              5\n",
       "Tlingit              4\n",
       "Scottish             4\n",
       "Kikuyu               4\n",
       "Piapoco              4\n",
       "Dolgan               3\n",
       "Australian           3\n",
       "Datog                3\n",
       "Canary_Islanders     2\n",
       "Italian_South        1\n",
       "Saami_WGA            1\n",
       "Name: clst, Length: 162, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ho_df[\"clst\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "143"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(ho_df[\"clst\"].value_counts()>5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
