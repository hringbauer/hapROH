{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook to call ROH in parallel within Ancient Individuals\n",
    "Notebooks that import the code for the calling ROHs on Ancients, and then functions for various cases to parallelize it.\n",
    "\n",
    "Highly similar to parallel_mosaic_callroh.ipynb\n",
    "\n",
    "@Author: Harald Ringbauer, June 2019\n",
    "All rights reserved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "midway2-0401.rcc.local\n",
      "Midway jnovmbre partition detected.\n",
      "/project2/jnovembre/hringbauer/HAPSBURG\n",
      "CPU Count: 28\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os as os\n",
    "import sys as sys\n",
    "import multiprocessing as mp\n",
    "import pandas as pd\n",
    "import socket\n",
    "\n",
    "### Pick the right path (whether on cluster or at home)\n",
    "socket_name = socket.gethostname()\n",
    "print(socket_name)\n",
    "if socket_name == \"VioletQueen\":\n",
    "    path = \"/home/harald/git/HAPSBURG/\"   # The Path on Harald's machine\n",
    "elif socket_name.startswith(\"midway2\"):\n",
    "    print(\"Midway jnovmbre partition detected.\")\n",
    "    path = \"/project2/jnovembre/hringbauer/HAPSBURG/\"  # The Path on Midway Cluster\n",
    "else: \n",
    "    raise RuntimeWarning(\"Not compatible machine. Check!!\")\n",
    "    \n",
    "os.chdir(path)  # Set the right Path (in line with Atom default)\n",
    "\n",
    "sys.path.append(\"./Python3/\")  # Since now we are in the Root Directory\n",
    "from hmm_inference import HMM_Analyze   # Do not move. Should be after sys.path..\n",
    "#sys.path.append(\"./Python3/create1000G_Mosaic/\")  # Since now we are in the Root Directory\n",
    "#from createMosaicsMulti import Mosaic_1000G_Multi  # Import the object that can create the Multiruns\n",
    "\n",
    "print(os.getcwd()) # Show the current working directory. Should be HAPSBURG/Notebooks/ParallelRuns\n",
    "print(f\"CPU Count: {mp.cpu_count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_path(path_output, iid, ch, prefix_out, logfile=True):\n",
    "    \"\"\"Prepare the path and pipe printing for one Individual\n",
    "    logfile: Whether to pipe output to log-file\"\"\"   \n",
    "    #if not os.path.exists(path_output):\n",
    "    #        raise RuntimeError(f\"Path {path_output} not Found. Check!\")\n",
    "            \n",
    "    path_log =  path_output + str(iid) + \"/chr\" + str(ch) + \"/\" + prefix_out\n",
    "    \n",
    "    if not os.path.exists(path_log):\n",
    "            os.makedirs(path_log)\n",
    "    \n",
    "    if logfile == True:\n",
    "        path_log = path_log + \"hmm_run_log.txt\"\n",
    "        print(f\"Set Output Log path: {path_log}\")\n",
    "        sys.stdout = open(path_log, 'w') \n",
    "    \n",
    "def analyze_individual(iid, ch=3, n_ref=503, save=True, save_fp=False,\n",
    "                       path_output=\"./Empirical/1240k/\",\n",
    "                       exclude_pops=[\"TSI\", ], prefix_out=\"\", \n",
    "                       roh_in =1, roh_out=10, roh_jump=100, e_rate=0.001):\n",
    "    \"\"\"Run the analysis for one individual and chromosome.\n",
    "    Wrapper for HMM Class\"\"\"\n",
    "    \n",
    "    ### Create Folder if needed, and pipe output if wanted\n",
    "    prepare_path(path_output, iid, ch, prefix_out, logfile=True)\n",
    "    \n",
    "    ### Do the full HMM Analysis\n",
    "    hmm = HMM_Analyze(cython=2, p_model=\"SardHDF5\",\n",
    "                      manual_load=True, save=save, save_fp=save_fp)\n",
    "\n",
    "    ### Load and prepare the pre-processing Model\n",
    "    hmm.load_preprocessing_model()              # Load the preprocessing Model\n",
    "    hmm.p_obj.set_params(destroy_phase=True, prefix_out_data=prefix_out,\n",
    "                        excluded=eclude_pops)\n",
    "    hmm.load_data(iid=iid, ch=ch, n_ref=n_ref)  # Load the actual Data\n",
    "    \n",
    "    ### Emission and Transition Model\n",
    "    hmm.load_secondary_objects()\n",
    "\n",
    "    #hmm.set_diploid_observations()            # To diploidize Individuals\n",
    "    hmm.t_obj.set_params(roh_in=roh_in, roh_out=roh_out, roh_jump=roh_jump)  # Set Jump Parameters\n",
    "    hmm.e_obj.set_params(e_rate=e_rate)        # Set error rates\n",
    "    \n",
    "    hmm.calc_viterbi_path(save=save)           # Calculate the Viterbi Path.\n",
    "    hmm.calc_posterior(save=save)              # Calculate the Posterior.\n",
    "    hmm.post_processing(save=save)             # Do the Post-Processing.\n",
    "    \n",
    "    print(f\"Analysis of {iid} and Chr. {ch} successfully concluded!\")\n",
    "    \n",
    "\n",
    "#########################################################\n",
    "#########################################################\n",
    "### Do the Read Count Analysis Function\n",
    "\n",
    "def analyze_individual_rc(iid, ch=3, n_ref=503, save=True, save_fp=False,\n",
    "                          exclude_pops=[\"TSI\", ], base_out_folder = \"./Empirical/1240k/\", prefix_out=\"rc/\",\n",
    "                          roh_in=1, roh_out=10, roh_jump=100, e_rate=0.01, e_rate_ref=0.001, logfile=True):\n",
    "    \"\"\"Run the analysis for one individual and chromosome on readcount data\n",
    "    Wrapper for HMM Class. Takes 13 Parameters\"\"\"\n",
    "    \n",
    "    ### The folder on what to run the Data on (Permanently set here to fixed loaction)\n",
    "    h5_path_targets = \"./Data/Marcus2019_1240k/mod_reich_sardinia_ancients_rev_mrg_dedup_3trm_anno.h5\"\n",
    "    meta_path_targets = \"./Data/Marcus2019_1240k/meta_rev_final.csv\"\n",
    "    \n",
    "    ### Create Folder if needed, and pipe output if wanted\n",
    "    prepare_path(base_out_folder, iid, ch, prefix_out, logfile=logfile)\n",
    "    \n",
    "    hmm = HMM_Analyze(cython=2, p_model=\"SardHDF5\", e_model=\"readcount\", post_model=\"Standard\",\n",
    "                      manual_load=True, save=save, save_fp=save_fp)\n",
    "\n",
    "    # Load and prepare the pre-processing Model\n",
    "    hmm.load_preprocessing_model()              # Load the preprocessing Model\n",
    "    hmm.p_obj.set_params(readcounts=True, destroy_phase=False, base_out_folder=base_out_folder,\n",
    "                prefix_out_data=prefix_out, excluded=exclude_pops,\n",
    "                h5_path_targets = h5_path_targets, meta_path_targets=meta_path_targets)    \n",
    "    hmm.load_data(iid=iid, ch=ch, n_ref=n_ref)  # Load the actual Data\n",
    "    \n",
    "    hmm.load_secondary_objects()\n",
    "\n",
    "    ### Set the Parameters\n",
    "    hmm.e_obj.set_params(e_rate = e_rate, e_rate_ref = e_rate_ref)\n",
    "    hmm.t_obj.set_params(roh_in=roh_in, roh_out=roh_out, roh_jump=roh_jump)\n",
    "    \n",
    "    hmm.calc_viterbi_path(save=save)           # Calculate the Viterbi Path.\n",
    "    hmm.calc_posterior(save=save)              # Calculate the Posterior.\n",
    "    hmm.post_processing(save=save)             # Do the Post-Processing.\n",
    "    \n",
    "def analyze_individual_gt(iid, ch=3, n_ref=503, save=True, save_fp=False, exclude_pops=[\"TSI\", ], \n",
    "                          base_out_folder=\"./Empirical/1240k/\", prefix_out=\"gt/\",\n",
    "                          roh_in=100, roh_out=100, roh_jump=385, e_rate=0.01, e_rate_ref=0.001, logfile=True):\n",
    "    \"\"\"Run the analysis for one individual and chromosome on readcount data\n",
    "    Wrapper for HMM Class. Takes 13 Parameters\"\"\"\n",
    "    \n",
    "    ### The folder on what to run the Data on (Permanently set here to fixed loaction)\n",
    "    h5_path_targets = \"./Data/Marcus2019_1240k/mod_reich_sardinia_ancients_rev_mrg_dedup_3trm_anno.h5\"\n",
    "    meta_path_targets = \"./Data/Marcus2019_1240k/meta_rev_unique_ids.csv\"  ### Path with the unique IDs per Modern Group\n",
    "    \n",
    "    ### Create Folder if needed, and pipe output if wanted\n",
    "    prepare_path(base_out_folder, iid, ch, prefix_out, logfile=logfile)\n",
    "    \n",
    "    hmm = HMM_Analyze(cython=2, p_model=\"SardHDF5\", e_model=\"diploid_gt\", post_model=\"Standard\",\n",
    "                      manual_load=True, save=save, save_fp=save_fp)\n",
    "\n",
    "    # Load and prepare the pre-processing Model\n",
    "    hmm.load_preprocessing_model()              # Load the preprocessing Model\n",
    "    hmm.p_obj.set_params(readcounts = False, destroy_phase=False,\n",
    "                prefix_out_data=prefix_out, excluded=exclude_pops, base_out_folder=base_out_folder,\n",
    "                h5_path_targets = h5_path_targets, meta_path_targets=meta_path_targets)    \n",
    "    hmm.load_data(iid=iid, ch=ch, n_ref=n_ref)  # Load the actual Data\n",
    "    \n",
    "    hmm.load_secondary_objects()\n",
    "\n",
    "    ### Set the Parameters\n",
    "    hmm.e_obj.set_params(e_rate = e_rate, e_rate_ref = e_rate_ref)\n",
    "    hmm.t_obj.set_params(roh_in=roh_in, roh_out=roh_out, roh_jump=roh_jump)\n",
    "    \n",
    "    hmm.calc_viterbi_path(save=save)           # Calculate the Viterbi Path.\n",
    "    hmm.calc_posterior(save=save)              # Calculate the Posterior.\n",
    "    hmm.post_processing(save=save)             # Do the Post-Processing.\n",
    "    \n",
    "#########################################################\n",
    "#########################################################\n",
    "    \n",
    "def multi_run(fun, prms, processes = 4):\n",
    "    \"\"\"Implementation of running in Parallel.\n",
    "    fun: Function\n",
    "    prms: The Parameter Files\n",
    "    processes: How many Processes to use\"\"\"\n",
    "    print(f\"Running {len(prms)} jobs in parallel.\")\n",
    "    \n",
    "    with mp.Pool(processes = processes) as pool:\n",
    "            results = pool.starmap(fun, prms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyze Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Ancient Readcount Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def give_param_list_rc_individual(iid, ch, n_ref=503, save=True, save_fp=False, base_out_folder = \"./Empirical/1240k/\", prefix_out= \"e01/\",\n",
    "                                  exclude_pops = [], roh_in = 100, roh_out= 100, roh_jump=385, e_rate = 0.01, e_rate_ref = 0.001, logfile=True):\n",
    "    \"\"\"Return List of Parameters for individual iid at Chromosome 1-23, which will be input for Starmap\"\"\"\n",
    "\n",
    "    prms = [iid, ch, n_ref, save, save_fp, exclude_pops, base_out_folder, prefix_out, roh_in, roh_out, roh_jump, e_rate, e_rate_ref, logfile]        \n",
    "    assert(len(prms) == 14)  # The RC function takes 13 Parameters as input\n",
    "    return prms\n",
    "\n",
    "def prepare_high_coverage_sardinian_prms_rc(cutoff_cov = 0.5):\n",
    "    \"\"\"Return List of High Coverage Ancient Sardinian Parameters for\n",
    "    RC Analysis. \n",
    "    cutoff_cov: Which minimum Coverage to Load\"\"\"\n",
    "    meta_path = \"./Data/Marcus2019_1240k/meta_rev_final.csv\"\n",
    "    anc_sardind= 85\n",
    "    anc_ind =  1087\n",
    "    base_out_folder=\"./Empirical/1240k/\"\n",
    "    \n",
    "    meta_df = pd.read_csv(meta_path)\n",
    "    anc_sard_df = meta_df[anc_sardind:anc_ind]\n",
    "\n",
    "    high_cov_df = anc_sard_df[(anc_sard_df[\"mean_cov\"] > cutoff_cov) & (anc_sard_df[\"include_alt\"] > 0)]\n",
    "    print(f\"Loaded {len(high_cov_df)} High Coverage Ancients\")\n",
    "\n",
    "    iids = high_cov_df[\"iid\"].values  \n",
    "    chs = range(1, 23)   # All human autosomes\n",
    "\n",
    "    prms = [give_param_list_rc_individual(iid=iid, ch=c, base_out_folder=base_out_folder) for iid in iids for c in chs]\n",
    "    return prms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 517 High Coverage Ancients\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "11374"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#prms = [give_param_list_rc_individual(iid = \"SEC002\", ch=3)]\n",
    "prms = prepare_high_coverage_sardinian_prms_rc(cutoff_cov = 0.5)\n",
    "len(prms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running 11374 jobs in parallel.\n",
      "Set Output Log path: ./Empirical/1240k/AllAnc/I2105/chr1/e01/hmm_run_log.txt\n",
      "Set Output Log path: ./Empirical/1240k/AllAnc/I4873/chr22/e01/hmm_run_log.txt\n",
      "Set Output Log path: ./Empirical/1240k/AllAnc/I4435/chr20/e01/hmm_run_log.txt\n",
      "Set Output Log path: ./Empirical/1240k/AllAnc/I4878/chr17/e01/hmm_run_log.txt\n",
      "Set Output Log path: ./Empirical/1240k/AllAnc/I6561/chr21/e01/hmm_run_log.txt\n",
      "Set Output Log path: ./Empirical/1240k/AllAnc/I4880/chr19/e01/hmm_run_log.txt\n",
      "Set Output Log path: ./Empirical/1240k/AllAnc/I5235/chr16/e01/hmm_run_log.txt\n",
      "Set Output Log path: ./Empirical/1240k/AllAnc/I5079/chr18/e01/hmm_run_log.txt\n",
      "Set Output Log path: ./Empirical/1240k/AllAnc/I5436/chr15/e01/hmm_run_log.txt\n",
      "Set Output Log path: ./Empirical/1240k/AllAnc/I5233/chr14/e01/hmm_run_log.txt\n"
     ]
    }
   ],
   "source": [
    "multi_run(analyze_individual_rc, prms, processes = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Hello? Blizzard?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze Modern diploid Genotypes, for comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_mod_sardinian_prms(pop_list = [], max_n = 20, mod_ind =  1098,\n",
    "                              chs = range(1, 23), meta_path = \"./Data/Marcus2019_1240k/meta_rev_unique_ids.csv\", \n",
    "                              base_out_folder = \"./Empirical/1240k/\", prefix_out = \"e01/\",\n",
    "                              e_rate_ref = 0, e_rate = 0.01, logfile=True):\n",
    "    \"\"\"Return List of Parameters for Modern \n",
    "    GT Analysis. \n",
    "    mod_ind: Where Modern Indivduals Start\n",
    "    pop_list: List of Populations which to produce Individuals for\n",
    "    chs:  Which chromosomes to use. Default: All human autosomes\n",
    "    meta_path: Where to find the modern Individuals\"\"\"\n",
    "    meta_df = pd.read_csv(meta_path)\n",
    "    mod_df = meta_df[mod_ind:] # Cut out only the modern Data\n",
    "    \n",
    "    iids = [] # Will be the IID List\n",
    "    for p in pop_list:\n",
    "        new_iids = mod_df.loc[mod_df[\"clst\"]==p, \"iid\"].values\n",
    "        iids += list(new_iids)[:max_n]  # Load max_n individuals\n",
    "    print(f\"Loaded {len(iids)} Individuals\")\n",
    "  \n",
    "    prms = [give_param_list_rc_individual(iid=iid, ch=c, e_rate_ref=e_rate_ref, e_rate=e_rate, \n",
    "                                          base_out_folder = base_out_folder, prefix_out = prefix_out, logfile=logfile) for iid in iids for c in chs]\n",
    "    return prms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3542"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#pop_list = [\"Basque\", 'Spanish', 'French', 'Croatian', \"Cag\", \"Ogl\", \"Olb\"]\n",
    "#pop_list = [\"Car\", \"Cam\", \"Ori\", \"Sas\", \"Nuo\", \"Czech\", \"Bergamo\"]\n",
    "pop_list = ['Albanian',  'Ashkenazi_Jew', 'Belarusian', 'Bulgarian', 'Estonian', 'Finnish', 'French_South', 'Hungarian', 'Lithuanian', 'Maltese', 'Russian', 'Scottish', 'Spanish_North', 'Turkish', 'Tuscan', 'Ukrainian']\n",
    "#pop_list = [\"Italian_South\", \"Sicilian\", \"Icelandic\", \"Norwegian\", \"English\", \"Greek\"]\n",
    "\n",
    "prms = prepare_mod_sardinian_prms(pop_list, max_n = 20, base_out_folder=\"./Empirical/1240k/HO/\", logfile=True)\n",
    "len(prms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_run(analyze_individual_gt, prms, processes = 8)\n",
    "print(\"Finished run!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Finished\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Area 51"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1430"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(prms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set Output Log path: ./Empirical/1240k/HO/Italian_South_0/chr4/e01/hmm_run_log.txt\n"
     ]
    }
   ],
   "source": [
    "analyze_individual_gt(*prms[3])  # Single Test Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(prms[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_path = \"./Data/Marcus2019_1240k/meta_rev_unique_ids.csv\"\n",
    "meta_df = pd.read_csv(meta_path)\n",
    "mod_df = meta_df[1098:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'?',\n",
       " 'AA',\n",
       " 'Abkhasian',\n",
       " 'Adygei',\n",
       " 'Albanian',\n",
       " 'Aleut',\n",
       " 'Algerian',\n",
       " 'Altaian',\n",
       " 'Ami',\n",
       " 'Armenian',\n",
       " 'Ashkenazi_Jew',\n",
       " 'Atayal',\n",
       " 'Australian',\n",
       " 'Balkar',\n",
       " 'Balochi',\n",
       " 'BantuKenya',\n",
       " 'BantuSA',\n",
       " 'Basque',\n",
       " 'BedouinA',\n",
       " 'BedouinB',\n",
       " 'Belarusian',\n",
       " 'Bengali',\n",
       " 'Bergamo',\n",
       " 'Biaka',\n",
       " 'Bolivian',\n",
       " 'Bougainville',\n",
       " 'Brahui',\n",
       " 'Bulgarian',\n",
       " 'Burusho',\n",
       " 'Cag',\n",
       " 'Cam',\n",
       " 'Cambodian',\n",
       " 'Canary_Islanders',\n",
       " 'Car',\n",
       " 'Chechen',\n",
       " 'Chukchi',\n",
       " 'Chuvash',\n",
       " 'Cochin_Jew',\n",
       " 'Croatian',\n",
       " 'Cypriot',\n",
       " 'Czech',\n",
       " 'Dai',\n",
       " 'Datog',\n",
       " 'Daur',\n",
       " 'Dolgan',\n",
       " 'Druze',\n",
       " 'Egyptian',\n",
       " 'English',\n",
       " 'Esan',\n",
       " 'Eskimo',\n",
       " 'Estonian',\n",
       " 'Ethiopian_Jew',\n",
       " 'Even',\n",
       " 'Finnish',\n",
       " 'French',\n",
       " 'French_South',\n",
       " 'Gambian',\n",
       " 'Georgian',\n",
       " 'Georgian_Jew',\n",
       " 'Greek',\n",
       " 'GujaratiA',\n",
       " 'GujaratiB',\n",
       " 'GujaratiC',\n",
       " 'GujaratiD',\n",
       " 'Hadza',\n",
       " 'Han',\n",
       " 'Han_NChina',\n",
       " 'Hazara',\n",
       " 'Hezhen',\n",
       " 'Hungarian',\n",
       " 'Icelandic',\n",
       " 'Iranian',\n",
       " 'Iranian_Jew',\n",
       " 'Iraqi_Jew',\n",
       " 'Italian_South',\n",
       " 'Itelmen',\n",
       " 'Japanese',\n",
       " 'Jordanian',\n",
       " 'Ju_hoan_North',\n",
       " 'Kalash',\n",
       " 'Kalmyk',\n",
       " 'Karitiana',\n",
       " 'Khomani',\n",
       " 'Kikuyu',\n",
       " 'Kinh',\n",
       " 'Korean',\n",
       " 'Koryak',\n",
       " 'Kumyk',\n",
       " 'Kusunda',\n",
       " 'Kyrgyz',\n",
       " 'Lahu',\n",
       " 'Lebanese',\n",
       " 'Lezgin',\n",
       " 'Libyan_Jew',\n",
       " 'Lithuanian',\n",
       " 'Luhya',\n",
       " 'Luo',\n",
       " 'Makrani',\n",
       " 'Maltese',\n",
       " 'Mandenka',\n",
       " 'Mansi',\n",
       " 'Masai',\n",
       " 'Mayan',\n",
       " 'Mbuti',\n",
       " 'Mende',\n",
       " 'Miao',\n",
       " 'Mixe',\n",
       " 'Mixtec',\n",
       " 'Mongola',\n",
       " 'Mordovian',\n",
       " 'Moroccan_Jew',\n",
       " 'Mozabite',\n",
       " 'Naxi',\n",
       " 'Nganasan',\n",
       " 'Nogai',\n",
       " 'North_Ossetian',\n",
       " 'Norwegian',\n",
       " 'Nuo',\n",
       " 'Ogl',\n",
       " 'Olb',\n",
       " 'Orcadian',\n",
       " 'Ori',\n",
       " 'Oroqen',\n",
       " 'Palestinian',\n",
       " 'Papuan',\n",
       " 'Pathan',\n",
       " 'Piapoco',\n",
       " 'Pima',\n",
       " 'Punjabi',\n",
       " 'Quechua',\n",
       " 'Russian',\n",
       " 'Saami_WGA',\n",
       " 'Saharawi',\n",
       " 'Sardinian',\n",
       " 'Sas',\n",
       " 'Saudi',\n",
       " 'Scottish',\n",
       " 'Selkup',\n",
       " 'She',\n",
       " 'Sicilian',\n",
       " 'Sindhi',\n",
       " 'Somali',\n",
       " 'Spanish',\n",
       " 'Spanish_North',\n",
       " 'Surui',\n",
       " 'Syrian',\n",
       " 'Tajik_Pomiri',\n",
       " 'Thai',\n",
       " 'Tlingit',\n",
       " 'Tu',\n",
       " 'Tubalar',\n",
       " 'Tujia',\n",
       " 'Tunisian',\n",
       " 'Tunisian_Jew',\n",
       " 'Turkish',\n",
       " 'Turkish_Jew',\n",
       " 'Turkmen',\n",
       " 'Tuscan',\n",
       " 'Tuvinian',\n",
       " 'Ukrainian',\n",
       " 'Ulchi',\n",
       " 'Uygur',\n",
       " 'Uzbek',\n",
       " 'Xibo',\n",
       " 'Yakut',\n",
       " 'Yemen',\n",
       " 'Yemenite_Jew',\n",
       " 'Yi',\n",
       " 'Yoruba',\n",
       " 'Yukagir',\n",
       " 'Zapotec'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#mod_df[\"clst\"].value_counts()\n",
    "set(mod_df[\"clst\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10120"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pops)*20*23"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
