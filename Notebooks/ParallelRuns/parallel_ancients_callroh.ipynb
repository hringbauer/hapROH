{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook to call ROH in parallel within Ancient Individuals\n",
    "Notebooks that import the code for the calling ROHs on Ancients, and then functions for various cases to parallelize it.\n",
    "\n",
    "Highly similar to parallel_mosaic_callroh.ipynb\n",
    "\n",
    "@Author: Harald Ringbauer, June 2019\n",
    "All rights reserved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Midway jnovmbre partition detected.\n",
      "/project/jnovembre/hringbauer/HAPSBURG\n",
      "CPU Count: 28\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os as os\n",
    "import sys as sys\n",
    "import multiprocessing as mp\n",
    "import pandas as pd\n",
    "import socket\n",
    "\n",
    "### Pick the right path (whether on cluster or at home)\n",
    "if socket.gethostname() == \"VioletQueen\":\n",
    "    path = \"/home/harald/git/HAPSBURG/\"   # The Path on Harald's machine\n",
    "elif socket.gethostname() == \"midway2-0401.rcc.local\" or socket.gethostname() == 'midway2-0402.rcc.local':\n",
    "    print(\"Midway jnovmbre partition detected.\")\n",
    "    path = \"/project/jnovembre/hringbauer/HAPSBURG/\"  # The Path on Midway Cluster\n",
    "else: \n",
    "    raise RuntimeWarning(\"Not compatible machine. Check!!\")\n",
    "    \n",
    "os.chdir(path)  # Set the right Path (in line with Atom default)\n",
    "\n",
    "sys.path.append(\"./Python3/\")  # Since now we are in the Root Directory\n",
    "from hmm_inference import HMM_Analyze   # Do not move. Should be after sys.path..\n",
    "#sys.path.append(\"./Python3/create1000G_Mosaic/\")  # Since now we are in the Root Directory\n",
    "#from createMosaicsMulti import Mosaic_1000G_Multi  # Import the object that can create the Multiruns\n",
    "\n",
    "print(os.getcwd()) # Show the current working directory. Should be HAPSBURG/Notebooks/ParallelRuns\n",
    "print(f\"CPU Count: {mp.cpu_count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_path(path_output, iid, ch, prefix_out, logfile=True):\n",
    "    \"\"\"Prepare the path and pipe printing for one Individual\n",
    "    logfile: Whether to pipe output to log-file\"\"\"   \n",
    "    #if not os.path.exists(path_output):\n",
    "    #        raise RuntimeError(f\"Path {path_output} not Found. Check!\")\n",
    "            \n",
    "    path_log =  path_output + str(iid) + \"/chr\" + str(ch) + \"/\" + prefix_out\n",
    "    \n",
    "    if not os.path.exists(path_log):\n",
    "            os.makedirs(path_log)\n",
    "    \n",
    "    if logfile == True:\n",
    "        path_log = path_log + \"hmm_run_log.txt\"\n",
    "        print(f\"Set Output Log path: {path_log}\")\n",
    "        sys.stdout = open(path_log, 'w') \n",
    "    \n",
    "def analyze_individual(iid, ch=3, n_ref=503, save=True, save_fp=False,\n",
    "                       path_output=\"./Empirical/1240k/\",\n",
    "                       exclude_pops=[\"TSI\", ], prefix_out=\"\", \n",
    "                       roh_in =1, roh_out=10, roh_jump=100, e_rate=0.001):\n",
    "    \"\"\"Run the analysis for one individual and chromosome.\n",
    "    Wrapper for HMM Class\"\"\"\n",
    "    \n",
    "    ### Create Folder if needed, and pipe output if wanted\n",
    "    prepare_path(path_output, iid, ch, prefix_out, logfile=True)\n",
    "    \n",
    "    ### Do the full HMM Analysis\n",
    "    hmm = HMM_Analyze(cython=2, p_model=\"SardHDF5\",\n",
    "                      manual_load=True, save=save, save_fp=save_fp)\n",
    "\n",
    "    ### Load and prepare the pre-processing Model\n",
    "    hmm.load_preprocessing_model()              # Load the preprocessing Model\n",
    "    hmm.p_obj.set_params(destroy_phase=True, prefix_out_data=prefix_out,\n",
    "                        excluded=eclude_pops)\n",
    "    hmm.load_data(iid=iid, ch=ch, n_ref=n_ref)  # Load the actual Data\n",
    "    \n",
    "    ### Emission and Transition Model\n",
    "    hmm.load_emission_model()\n",
    "    hmm.load_transition_model()\n",
    "\n",
    "    #hmm.set_diploid_observations()            # To diploidize Individuals\n",
    "    hmm.t_obj.set_params(roh_in=roh_in, roh_out=roh_out, roh_jump=roh_jump)  # Set Jump Parameters\n",
    "    hmm.e_obj.set_params(e_rate=e_rate)        # Set error rates\n",
    "    \n",
    "    hmm.calc_viterbi_path(save=save)           # Calculate the Viterbi Path.\n",
    "    hmm.calc_posterior(save=save)              # Calculate the Posterior.\n",
    "    hmm.post_processing(save=save)             # Do the Post-Processing.\n",
    "    \n",
    "    print(f\"Analysis of {iid} and Chr. {ch} successfully concluded!\")\n",
    "    \n",
    "\n",
    "#########################################################\n",
    "#########################################################\n",
    "### Do the Read Count Analysis Function\n",
    "\n",
    "def analyze_individual_rc(iid, ch=3, n_ref=503, save=True, save_fp=False,\n",
    "                          path_output=\"./Empirical/1240k/\",\n",
    "                          exclude_pops=[\"TSI\", ], prefix_out=\"rc/\",\n",
    "                          roh_in=1, roh_out=10, roh_jump=100, e_rate=0.01, e_rate_ref=0.001):\n",
    "    \"\"\"Run the analysis for one individual and chromosome on readcount data\n",
    "    Wrapper for HMM Class. Takes 13 Parameters\"\"\"\n",
    "    \n",
    "    ### The folder on what to run the Data on (Permanently set here to fixed loaction)\n",
    "    h5_path_targets = \"./Data/Marcus2019_1240k/mod_reich_sardinia_ancients_rev_mrg_dedup_3trm_anno.h5\"\n",
    "    meta_path_targets = \"./Data/Marcus2019_1240k/meta_rev_final.csv\"\n",
    "    \n",
    "    ### Create Folder if needed, and pipe output if wanted\n",
    "    prepare_path(path_output, iid, ch, prefix_out, logfile=True)\n",
    "    \n",
    "    hmm = HMM_Analyze(cython=2, p_model=\"SardHDF5\", e_model=\"readcount\",\n",
    "                      manual_load=True, save=save, save_fp=save_fp)\n",
    "\n",
    "    # Load and prepare the pre-processing Model\n",
    "    hmm.load_preprocessing_model()              # Load the preprocessing Model\n",
    "    hmm.p_obj.set_params(readcounts = True, destroy_phase=False,\n",
    "                prefix_out_data=prefix_out, excluded=exclude_pops,\n",
    "                h5_path_targets = h5_path_targets, meta_path_targets=meta_path_targets)    \n",
    "    hmm.load_data(iid=iid, ch=ch, n_ref=n_ref)  # Load the actual Data\n",
    "    \n",
    "    hmm.load_emission_model()\n",
    "    hmm.load_transition_model()\n",
    "\n",
    "    ### Set the Parameters\n",
    "    hmm.e_obj.set_params(e_rate = e_rate, e_rate_ref = e_rate_ref)\n",
    "    hmm.t_obj.set_params(roh_in=roh_in, roh_out=roh_out, roh_jump=roh_jump)\n",
    "    \n",
    "    hmm.calc_viterbi_path(save=save)           # Calculate the Viterbi Path.\n",
    "    hmm.calc_posterior(save=save)              # Calculate the Posterior.\n",
    "    hmm.post_processing(save=save)             # Do the Post-Processing.\n",
    "    \n",
    "def analyze_individual_gt(iid, ch=3, n_ref=503, save=True, save_fp=False,\n",
    "                          path_output=\"./Empirical/1240k/\",\n",
    "                          exclude_pops=[\"TSI\", ], prefix_out=\"gt/\",\n",
    "                          roh_in=100, roh_out=100, roh_jump=385, e_rate=0.01, e_rate_ref=0.001):\n",
    "    \"\"\"Run the analysis for one individual and chromosome on readcount data\n",
    "    Wrapper for HMM Class. Takes 13 Parameters\"\"\"\n",
    "    \n",
    "    ### The folder on what to run the Data on (Permanently set here to fixed loaction)\n",
    "    h5_path_targets = \"./Data/Marcus2019_1240k/mod_reich_sardinia_ancients_rev_mrg_dedup_3trm_anno.h5\"\n",
    "    meta_path_targets = \"./Data/Marcus2019_1240k/meta_rev_unique_ids.csv\"  ### Path with the unique IDs per Modern Group\n",
    "    \n",
    "    ### Create Folder if needed, and pipe output if wanted\n",
    "    prepare_path(path_output, iid, ch, prefix_out, logfile=True)\n",
    "    \n",
    "    hmm = HMM_Analyze(cython=2, p_model=\"SardHDF5\", e_model=\"diploid_gt\",\n",
    "                      manual_load=True, save=save, save_fp=save_fp)\n",
    "\n",
    "    # Load and prepare the pre-processing Model\n",
    "    hmm.load_preprocessing_model()              # Load the preprocessing Model\n",
    "    hmm.p_obj.set_params(readcounts = False, destroy_phase=False,\n",
    "                prefix_out_data=prefix_out, excluded=exclude_pops,\n",
    "                h5_path_targets = h5_path_targets, meta_path_targets=meta_path_targets)    \n",
    "    hmm.load_data(iid=iid, ch=ch, n_ref=n_ref)  # Load the actual Data\n",
    "    \n",
    "    hmm.load_emission_model()\n",
    "    hmm.load_transition_model()\n",
    "\n",
    "    ### Set the Parameters\n",
    "    hmm.e_obj.set_params(e_rate = e_rate, e_rate_ref = e_rate_ref)\n",
    "    hmm.t_obj.set_params(roh_in=roh_in, roh_out=roh_out, roh_jump=roh_jump)\n",
    "    \n",
    "    hmm.calc_viterbi_path(save=save)           # Calculate the Viterbi Path.\n",
    "    hmm.calc_posterior(save=save)              # Calculate the Posterior.\n",
    "    hmm.post_processing(save=save)             # Do the Post-Processing.\n",
    "    \n",
    "#########################################################\n",
    "#########################################################\n",
    "    \n",
    "def multi_run(fun, prms, processes = 4):\n",
    "    \"\"\"Implementation of running in Parallel.\n",
    "    fun: Function\n",
    "    prms: The Parameter Files\n",
    "    processes: How many Processes to use\"\"\"\n",
    "    print(f\"Running {len(prms)} jobs in parallel.\")\n",
    "    \n",
    "    with mp.Pool(processes = processes) as pool:\n",
    "        results = pool.starmap(fun, prms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyze Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Ancient Readcount Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def give_param_list_rc_individual(iid, ch, n_ref=503, save=True, save_fp=False, prefix_out= \"e01/\",\n",
    "                      path_output=\"./Empirical/1240k/\", exclude_pops = [],\n",
    "                      roh_in = 100, roh_out= 100, roh_jump=385, e_rate = 0.01, e_rate_ref = 0.001):\n",
    "    \"\"\"Return List of Parameters for individual iid at Chromosome 1-23, which will be input for Starmap\"\"\"\n",
    "\n",
    "    prms = [iid, ch, n_ref, save, save_fp, path_output, exclude_pops, prefix_out, roh_in, roh_out, roh_jump, e_rate, e_rate_ref]        \n",
    "    assert(len(prms) == 13)  # The RC function takes 13 Parameters as input\n",
    "    return prms\n",
    "\n",
    "def prepare_high_coverage_sardinian_prms_rc(cutoff_cov = 0.5):\n",
    "    \"\"\"Return List of High Coverage Ancient Sardinian Parameters for\n",
    "    RC Analysis. \n",
    "    cutoff_cov: Which minimum Coverage to Load\"\"\"\n",
    "    meta_path = \"./Data/Marcus2019_1240k/meta_rev_final.csv\"\n",
    "    anc_sardind= 85\n",
    "    anc_ind =  1087\n",
    "    path_output=\"./Empirical/1240k/AllAnc/\"\n",
    "    \n",
    "    \n",
    "    meta_df = pd.read_csv(meta_path)\n",
    "    anc_sard_df = meta_df[anc_sardind:anc_ind]\n",
    "\n",
    "    high_cov_df = anc_sard_df[(anc_sard_df[\"mean_cov\"] > cutoff_cov) & (anc_sard_df[\"include_alt\"] > 0)]\n",
    "    print(f\"Loaded {len(high_cov_df)} High Coverage Ancients\")\n",
    "\n",
    "    iids = high_cov_df[\"iid\"].values  \n",
    "    chs = range(1, 23)   # All human autosomes\n",
    "\n",
    "    prms = [give_param_list_rc_individual(iid=iid, ch=c, path_output=path_output) for iid in iids for c in chs]\n",
    "    return prms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 517 High Coverage Ancients\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "11374"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#prms = [give_param_list_rc_individual(iid = \"SEC002\", ch=3)]\n",
    "prms = prepare_high_coverage_sardinian_prms_rc(cutoff_cov = 0.5)\n",
    "len(prms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running 11374 jobs in parallel.\n",
      "Set Output Log path: ./Empirical/1240k/AllAnc/I2105/chr1/e01/hmm_run_log.txt\n",
      "Set Output Log path: ./Empirical/1240k/AllAnc/I4873/chr22/e01/hmm_run_log.txt\n",
      "Set Output Log path: ./Empirical/1240k/AllAnc/I4435/chr20/e01/hmm_run_log.txt\n",
      "Set Output Log path: ./Empirical/1240k/AllAnc/I4878/chr17/e01/hmm_run_log.txt\n",
      "Set Output Log path: ./Empirical/1240k/AllAnc/I6561/chr21/e01/hmm_run_log.txt\n",
      "Set Output Log path: ./Empirical/1240k/AllAnc/I4880/chr19/e01/hmm_run_log.txt\n",
      "Set Output Log path: ./Empirical/1240k/AllAnc/I5235/chr16/e01/hmm_run_log.txt\n",
      "Set Output Log path: ./Empirical/1240k/AllAnc/I5079/chr18/e01/hmm_run_log.txt\n",
      "Set Output Log path: ./Empirical/1240k/AllAnc/I5436/chr15/e01/hmm_run_log.txt\n",
      "Set Output Log path: ./Empirical/1240k/AllAnc/I5233/chr14/e01/hmm_run_log.txt\n"
     ]
    }
   ],
   "source": [
    "multi_run(analyze_individual_rc, prms, processes = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Hello? Blizzard?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze Modern diploid Genotypes, for comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_mod_sardinian_prms(pop_list = [], max_n = 20, path_output=\"./Empirical/1240k/AllAnc/\"):\n",
    "    \"\"\"Return List of Parameters for Modern \n",
    "    GT Analysis. \n",
    "    pop_list: List of Populations which to produce Individuals for\"\"\"\n",
    "    chs = range(1, 23)   # All human autosomes\n",
    "    mod_ind =  1098      # ID where Modern Individuals start\n",
    "    meta_path = \"./Data/Marcus2019_1240k/meta_rev_unique_ids.csv\"\n",
    "    meta_df = pd.read_csv(meta_path)\n",
    "    mod_df = meta_df[mod_ind:]\n",
    "    \n",
    "    iids = [] # Will be the IID List\n",
    "    for p in pop_list:\n",
    "        new_iids = mod_df.loc[mod_df[\"clst\"]==p, \"iid\"].values\n",
    "        iids += list(new_iids)[:max_n]  # Load max_n individuals\n",
    "    print(f\"Loaded {len(iids)} Individuals\")\n",
    "  \n",
    "    prms = [give_param_list_rc_individual(iid=iid, ch=c, path_output=path_output, e_rate_ref=0, e_rate=0.01, prefix_out= \"e01/\") for iid in iids for c in chs]\n",
    "    return prms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 122 Individuals\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2684"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#pop_list = [\"Basque\", 'Spanish', 'French', 'Croatian', \"Cag\", \"Ogl\", \"Olb\"]\n",
    "pop_list = [\"Car\", \"Cam\", \"Ori\", \"Sas\", \"Nuo\", \"Czech\", \"Bergamo\"]\n",
    "prms = prepare_mod_sardinian_prms(pop_list, max_n = 20)\n",
    "len(prms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running 2684 jobs in parallel.\n",
      "Set Output Log path: ./Empirical/1240k/AllAnc/Car_0/chr1/e01/hmm_run_log.txt\n",
      "Set Output Log path: ./Empirical/1240k/AllAnc/Car_3/chr19/e01/hmm_run_log.txt\n",
      "Set Output Log path: ./Empirical/1240k/AllAnc/Car_11/chr11/e01/hmm_run_log.txt\n",
      "Set Output Log path: ./Empirical/1240k/AllAnc/Car_15/chr7/e01/hmm_run_log.txt\n",
      "Set Output Log path: ./Empirical/1240k/AllAnc/Car_7/chr15/e01/hmm_run_log.txt\n",
      "Set Output Log path: ./Empirical/1240k/AllAnc/Cam_6/chr17/e01/hmm_run_log.txt\n",
      "Set Output Log path: ./Empirical/1240k/AllAnc/Car_19/chr3/e01/hmm_run_log.txt\n",
      "Set Output Log path: ./Empirical/1240k/AllAnc/Cam_2/chr21/e01/hmm_run_log.txt\n"
     ]
    }
   ],
   "source": [
    "multi_run(analyze_individual_gt, prms, processes = 8)\n",
    "print(\"Finished run (within Sard)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Area 51"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2816"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(prms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Linear-State Speed-Up\n",
      "Loaded Pre Processing Model: SardHDF5\n",
      "Loading Individual: Basque_0\n",
      "\n",
      "Loaded 1145647 variants\n",
      "Loaded 4616 individuals\n",
      "HDF5 loaded from ./Data/Marcus2019_1240k/mod_reich_sardinia_ancients_rev_mrg_dedup_3trm_anno.h5\n",
      "\n",
      "Loaded 89147 variants\n",
      "Loaded 503 individuals\n",
      "HDF5 loaded from ./Data/1000Genomes/HDF5/1240kHDF5/Eur1240chr1.hdf5\n",
      "\n",
      "Intersection on Positions: 89143\n",
      "Nr of Matching Refs: 89143 / 89143\n",
      "Full Intersection Ref/Alt Identical: 89079 / 89143\n",
      "503 / 503 Individuals included in Reference\n",
      "Extraction of 1006 Haplotypes Complete!\n",
      "Markers called 47602 / 89079\n",
      "Successfully saved to: ./Empirical/1240k/Basque_0/chr1/e01/\n",
      "Successfully loaded Data from: ./Empirical/1240k/Basque_0/chr1/e01/\n",
      "Loaded Emission Model: diploid_gt\n",
      "Loaded Transition Model: model\n",
      "Minimum Genetic Map: 0.0201\n",
      "Maximum Genetic Map: 2.8623\n",
      "Gaps bigger than 0.1 cM: 424\n",
      "Maximum Gap: 3.9814 cM\n",
      "Reference Number: 1006\n",
      "Loaded Transition and Emission Matrix:\n",
      "(3, 3)\n",
      "(1007, 47602, 3)\n",
      "Loaded Observations:\n",
      "(2, 47602)\n",
      "Log likelihood Path: -23033.975\n",
      "Saved to: ./Empirical/1240k/Basque_0/chr1/e01/viterbi_path.csv\n",
      "Finished Calculation Viterbi Path: [0 0 0 ... 0 0 0]\n",
      "Minimum Genetic Map: 0.0201\n",
      "Maximum Genetic Map: 2.8623\n",
      "Gaps bigger than 0.1 cM: 424\n",
      "Maximum Gap: 3.9814 cM\n",
      "Loaded Transition and Emission Matrix:\n",
      "(3, 3)\n",
      "(1007, 47602, 3)\n",
      "Loaded Observations:\n",
      "(2, 47602)\n",
      "Reference Number: 1006\n",
      "Total Log likelihood: -21241.207\n",
      "Finished Calculation State Posteriors\n",
      "Saved Zero State Posterior to ./Empirical/1240k/Basque_0/chr1/e01/.\n",
      "Successfully loaded for PP. from ./Empirical/1240k/Basque_0/chr1/e01/\n",
      "Fraction Markers in ROH: 0.2994\n",
      "Merged n=0 gaps < 0.01 M\n",
      "Called n=3 ROH Blocks > 1.0 cM\n",
      "Longest Block: 2.339\n",
      "Successfully saved to ./Empirical/1240k/Basque_0/chr1/e01/roh.csv\n"
     ]
    }
   ],
   "source": [
    "analyze_individual_gt(*prms[0])  # Single Test Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_path = \"./Data/Marcus2019_1240k/meta_rev_unique_ids.csv\"\n",
    "meta_df = pd.read_csv(meta_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
