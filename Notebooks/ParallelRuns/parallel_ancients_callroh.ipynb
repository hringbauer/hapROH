{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook to call ROH in parallel within Ancient Individuals\n",
    "Notebooks that import the code for the calling ROHs on Ancients, and then functions for various cases to parallelize it.\n",
    "\n",
    "Highly similar to parallel_mosaic_callroh.ipynb\n",
    "\n",
    "@Author: Harald Ringbauer, June 2019\n",
    "All rights reserved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "midway2-0401.rcc.local\n",
      "Midway jnovmbre partition detected.\n",
      "/project2/jnovembre/hringbauer/HAPSBURG\n",
      "CPU Count: 28\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os as os\n",
    "import sys as sys\n",
    "import multiprocessing as mp\n",
    "import pandas as pd\n",
    "import socket\n",
    "\n",
    "### Pick the right path (whether on cluster or at home)\n",
    "socket_name = socket.gethostname()\n",
    "print(socket_name)\n",
    "if socket_name == \"VioletQueen\":\n",
    "    path = \"/home/harald/git/HAPSBURG/\"   # The Path on Harald's machine\n",
    "elif socket_name.startswith(\"midway2\"):\n",
    "    print(\"Midway jnovmbre partition detected.\")\n",
    "    path = \"/project2/jnovembre/hringbauer/HAPSBURG/\"  # The Path on Midway Cluster\n",
    "else: \n",
    "    raise RuntimeWarning(\"Not compatible machine. Check!!\")\n",
    "    \n",
    "os.chdir(path)  # Set the right Path (in line with Atom default)\n",
    "\n",
    "sys.path.append(\"./Python3/\")  # Since now we are in the Root Directory\n",
    "from hmm_inference import HMM_Analyze   # Do not move. Should be after sys.path..\n",
    "\n",
    "sys.path.append(\"./PackagesSupport/parallel_runs/\")\n",
    "sys.path.append(\"./PackagesSupport/\")\n",
    "from helper_functions import combine_individual_data\n",
    "from pp_individual_roh_csvs import create_combined_ROH_df, give_iid_paths, pp_individual_roh\n",
    "\n",
    "print(os.getcwd()) # Show the current working directory. Should be HAPSBURG/Notebooks/ParallelRuns\n",
    "print(f\"CPU Count: {mp.cpu_count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_path(path_output, iid, ch, prefix_out, logfile=True):\n",
    "    \"\"\"Prepare the path and pipe printing for one Individual\n",
    "    logfile: Whether to pipe output to log-file\"\"\"   \n",
    "    #if not os.path.exists(path_output):\n",
    "    #        raise RuntimeError(f\"Path {path_output} not Found. Check!\")\n",
    "            \n",
    "    path_log =  path_output + str(iid) + \"/chr\" + str(ch) + \"/\" + prefix_out\n",
    "    \n",
    "    if not os.path.exists(path_log):\n",
    "            os.makedirs(path_log)\n",
    "    \n",
    "    if logfile == True:\n",
    "        path_log = path_log + \"hmm_run_log.txt\"\n",
    "        print(f\"Set Output Log path: {path_log}\")\n",
    "        sys.stdout = open(path_log, 'w') \n",
    "    \n",
    "def analyze_individual(iid, ch=3, n_ref=503, save=True, save_fp=False,\n",
    "                       path_output=\"./Empirical/1240k/\",\n",
    "                       exclude_pops=[\"TSI\", ], prefix_out=\"\", \n",
    "                       roh_in =1, roh_out=10, roh_jump=100, e_rate=0.001):\n",
    "    \"\"\"Run the analysis for one individual and chromosome.\n",
    "    Wrapper for HMM Class\"\"\"\n",
    "    \n",
    "    ### Create Folder if needed, and pipe output if wanted\n",
    "    prepare_path(path_output, iid, ch, prefix_out, logfile=True)\n",
    "    \n",
    "    ### Do the full HMM Analysis\n",
    "    hmm = HMM_Analyze(cython=2, p_model=\"SardHDF5\",\n",
    "                      manual_load=True, save=save, save_fp=save_fp)\n",
    "\n",
    "    ### Load and prepare the pre-processing Model\n",
    "    hmm.load_preprocessing_model()              # Load the preprocessing Model\n",
    "    hmm.p_obj.set_params(destroy_phase=True, prefix_out_data=prefix_out,\n",
    "                        excluded=eclude_pops)\n",
    "    hmm.load_data(iid=iid, ch=ch, n_ref=n_ref)  # Load the actual Data\n",
    "    \n",
    "    ### Emission and Transition Model\n",
    "    hmm.load_secondary_objects()\n",
    "\n",
    "    #hmm.set_diploid_observations()            # To diploidize Individuals\n",
    "    hmm.t_obj.set_params(roh_in=roh_in, roh_out=roh_out, roh_jump=roh_jump)  # Set Jump Parameters\n",
    "    hmm.e_obj.set_params(e_rate=e_rate)        # Set error rates\n",
    "    \n",
    "    hmm.calc_viterbi_path(save=save)           # Calculate the Viterbi Path.\n",
    "    hmm.calc_posterior(save=save)              # Calculate the Posterior.\n",
    "    hmm.post_processing(save=save)             # Do the Post-Processing.\n",
    "    \n",
    "    print(f\"Analysis of {iid} and Chr. {ch} successfully concluded!\")\n",
    "    \n",
    "\n",
    "#########################################################\n",
    "#########################################################\n",
    "### Do the Read Count Analysis Function\n",
    "\n",
    "def analyze_individual_rc(iid, ch=3, n_ref=503, save=True, save_fp=False,\n",
    "                          exclude_pops=[\"TSI\", ], base_out_folder = \"./Empirical/1240k/\", prefix_out=\"rc/\",\n",
    "                          roh_in=1, roh_out=10, roh_jump=100, e_rate=0.01, e_rate_ref=0.001, logfile=True):\n",
    "    \"\"\"Run the analysis for one individual and chromosome on readcount data\n",
    "    Wrapper for HMM Class. Takes 13 Parameters\"\"\"\n",
    "    \n",
    "    ### The folder on what to run the Data on (Permanently set here to fixed loaction)\n",
    "    h5_path_targets = \"./Data/Marcus2019_1240k/mod_reich_sardinia_ancients_rev_mrg_dedup_3trm_anno.h5\"\n",
    "    meta_path_targets = \"./Data/Marcus2019_1240k/meta_rev_final.csv\"\n",
    "    \n",
    "    ### Create Folder if needed, and pipe output if wanted\n",
    "    prepare_path(base_out_folder, iid, ch, prefix_out, logfile=logfile)\n",
    "    \n",
    "    hmm = HMM_Analyze(cython=2, p_model=\"SardHDF5\", e_model=\"readcount\", post_model=\"Standard\",\n",
    "                      manual_load=True, save=save, save_fp=save_fp)\n",
    "\n",
    "    # Load and prepare the pre-processing Model\n",
    "    hmm.load_preprocessing_model()              # Load the preprocessing Model\n",
    "    hmm.p_obj.set_params(readcounts=True, destroy_phase=False, base_out_folder=base_out_folder,\n",
    "                prefix_out_data=prefix_out, excluded=exclude_pops,\n",
    "                h5_path_targets = h5_path_targets, meta_path_targets=meta_path_targets)    \n",
    "    hmm.load_data(iid=iid, ch=ch, n_ref=n_ref)  # Load the actual Data\n",
    "    \n",
    "    hmm.load_secondary_objects()\n",
    "\n",
    "    ### Set the Parameters\n",
    "    hmm.e_obj.set_params(e_rate = e_rate, e_rate_ref = e_rate_ref)\n",
    "    hmm.t_obj.set_params(roh_in=roh_in, roh_out=roh_out, roh_jump=roh_jump)\n",
    "    \n",
    "    hmm.calc_viterbi_path(save=save)           # Calculate the Viterbi Path.\n",
    "    hmm.calc_posterior(save=save)              # Calculate the Posterior.\n",
    "    hmm.post_processing(save=save)             # Do the Post-Processing.\n",
    "    \n",
    "def analyze_individual_gt(iid, ch=3, n_ref=503, save=True, save_fp=False, exclude_pops=[\"TSI\", ], \n",
    "                          base_out_folder=\"./Empirical/1240k/\", prefix_out=\"gt/\",\n",
    "                          roh_in=100, roh_out=100, roh_jump=385, e_rate=0.01, e_rate_ref=0.001, logfile=True):\n",
    "    \"\"\"Run the analysis for one individual and chromosome on readcount data\n",
    "    Wrapper for HMM Class. Takes 13 Parameters\"\"\"\n",
    "    \n",
    "    ### The folder on what to run the Data on (Permanently set here to fixed loaction)\n",
    "    h5_path_targets = \"./Data/Marcus2019_1240k/mod_reich_sardinia_ancients_rev_mrg_dedup_3trm_anno.h5\"\n",
    "    meta_path_targets = \"./Data/Marcus2019_1240k/meta_rev_unique_ids.csv\"  ### Path with the unique IDs per Modern Group\n",
    "    \n",
    "    ### Create Folder if needed, and pipe output if wanted\n",
    "    prepare_path(base_out_folder, iid, ch, prefix_out, logfile=logfile)\n",
    "    \n",
    "    hmm = HMM_Analyze(cython=2, p_model=\"SardHDF5\", e_model=\"diploid_gt\", post_model=\"Standard\",\n",
    "                      manual_load=True, save=save, save_fp=save_fp)\n",
    "\n",
    "    # Load and prepare the pre-processing Model\n",
    "    hmm.load_preprocessing_model()              # Load the preprocessing Model\n",
    "    hmm.p_obj.set_params(readcounts = False, destroy_phase=False,\n",
    "                prefix_out_data=prefix_out, excluded=exclude_pops, base_out_folder=base_out_folder,\n",
    "                h5_path_targets = h5_path_targets, meta_path_targets=meta_path_targets)    \n",
    "    hmm.load_data(iid=iid, ch=ch, n_ref=n_ref)  # Load the actual Data\n",
    "    \n",
    "    hmm.load_secondary_objects()\n",
    "\n",
    "    ### Set the Parameters\n",
    "    hmm.e_obj.set_params(e_rate = e_rate, e_rate_ref = e_rate_ref)\n",
    "    hmm.t_obj.set_params(roh_in=roh_in, roh_out=roh_out, roh_jump=roh_jump)\n",
    "    \n",
    "    hmm.calc_viterbi_path(save=save)           # Calculate the Viterbi Path.\n",
    "    hmm.calc_posterior(save=save)              # Calculate the Posterior.\n",
    "    hmm.post_processing(save=save)             # Do the Post-Processing.\n",
    "    \n",
    "#########################################################\n",
    "#########################################################\n",
    "    \n",
    "def multi_run(fun, prms, processes = 4):\n",
    "    \"\"\"Implementation of running in Parallel.\n",
    "    fun: Function\n",
    "    prms: The Parameter Files\n",
    "    processes: How many Processes to use\"\"\"\n",
    "    print(f\"Running {len(prms)} jobs in parallel.\")\n",
    "    \n",
    "    with mp.Pool(processes = processes) as pool:\n",
    "            results = pool.starmap(fun, prms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyze Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Ancient Readcount Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def give_param_list_rc_individual(iid, ch, n_ref=503, save=True, save_fp=False, base_out_folder = \"./Empirical/1240k/\", prefix_out= \"e01/\",\n",
    "                                  exclude_pops = [], roh_in = 100, roh_out= 100, roh_jump=385, e_rate = 0.01, e_rate_ref = 0.001, logfile=True):\n",
    "    \"\"\"Return List of Parameters for individual iid at Chromosome 1-23, which will be input for Starmap\"\"\"\n",
    "\n",
    "    prms = [iid, ch, n_ref, save, save_fp, exclude_pops, base_out_folder, prefix_out, roh_in, roh_out, roh_jump, e_rate, e_rate_ref, logfile]        \n",
    "    assert(len(prms) == 14)  # The RC function takes 13 Parameters as input\n",
    "    return prms\n",
    "\n",
    "def prepare_high_coverage_sardinian_prms_rc(cutoff_cov = 0.5):\n",
    "    \"\"\"Return List of High Coverage Ancient Sardinian Parameters for\n",
    "    RC Analysis. \n",
    "    cutoff_cov: Which minimum Coverage to Load\"\"\"\n",
    "    meta_path = \"./Data/Marcus2019_1240k/meta_rev_final.csv\"\n",
    "    anc_sardind= 85\n",
    "    anc_ind =  1087\n",
    "    base_out_folder=\"./Empirical/1240k/\"\n",
    "    \n",
    "    meta_df = pd.read_csv(meta_path)\n",
    "    anc_sard_df = meta_df[anc_sardind:anc_ind]\n",
    "\n",
    "    high_cov_df = anc_sard_df[(anc_sard_df[\"mean_cov\"] > cutoff_cov) & (anc_sard_df[\"include_alt\"] > 0)]\n",
    "    print(f\"Loaded {len(high_cov_df)} High Coverage Ancients\")\n",
    "\n",
    "    iids = high_cov_df[\"iid\"].values  \n",
    "    chs = range(1, 23)   # All human autosomes\n",
    "\n",
    "    prms = [give_param_list_rc_individual(iid=iid, ch=c, base_out_folder=base_out_folder) for iid in iids for c in chs]\n",
    "    return prms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 517 High Coverage Ancients\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "11374"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#prms = [give_param_list_rc_individual(iid = \"SEC002\", ch=3)]\n",
    "prms = prepare_high_coverage_sardinian_prms_rc(cutoff_cov = 0.5)\n",
    "len(prms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running 11374 jobs in parallel.\n",
      "Set Output Log path: ./Empirical/1240k/AllAnc/I2105/chr1/e01/hmm_run_log.txt\n",
      "Set Output Log path: ./Empirical/1240k/AllAnc/I4873/chr22/e01/hmm_run_log.txt\n",
      "Set Output Log path: ./Empirical/1240k/AllAnc/I4435/chr20/e01/hmm_run_log.txt\n",
      "Set Output Log path: ./Empirical/1240k/AllAnc/I4878/chr17/e01/hmm_run_log.txt\n",
      "Set Output Log path: ./Empirical/1240k/AllAnc/I6561/chr21/e01/hmm_run_log.txt\n",
      "Set Output Log path: ./Empirical/1240k/AllAnc/I4880/chr19/e01/hmm_run_log.txt\n",
      "Set Output Log path: ./Empirical/1240k/AllAnc/I5235/chr16/e01/hmm_run_log.txt\n",
      "Set Output Log path: ./Empirical/1240k/AllAnc/I5079/chr18/e01/hmm_run_log.txt\n",
      "Set Output Log path: ./Empirical/1240k/AllAnc/I5436/chr15/e01/hmm_run_log.txt\n",
      "Set Output Log path: ./Empirical/1240k/AllAnc/I5233/chr14/e01/hmm_run_log.txt\n"
     ]
    }
   ],
   "source": [
    "multi_run(analyze_individual_rc, prms, processes = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Hello? Blizzard?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze Modern diploid Genotypes, for comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_mod_sardinian_prms(pop_list = [], max_n = 20, mod_ind =  1098,\n",
    "                               chs = range(1, 23), meta_path = \"./Data/Marcus2019_1240k/meta_rev_unique_ids.csv\", \n",
    "                               base_out_folder = \"./Empirical/1240k/\", prefix_out = \"e01/\",\n",
    "                               e_rate_ref = 0, e_rate = 0.01, logfile=True):\n",
    "    \"\"\"Return List of Parameters for Modern \n",
    "    GT Analysis. \n",
    "    mod_ind: Where Modern Indivduals Start\n",
    "    pop_list: List of Populations which to produce Individuals for\n",
    "    chs:  Which chromosomes to use. Default: All human autosomes\n",
    "    meta_path: Where to find the modern Individuals\"\"\"\n",
    "    meta_df = pd.read_csv(meta_path)\n",
    "    mod_df = meta_df[mod_ind:] # Cut out only the modern Data\n",
    "    \n",
    "    iids = [] # Will be the IID List\n",
    "    for p in pop_list:\n",
    "        new_iids = mod_df.loc[mod_df[\"clst\"]==p, \"iid\"].values\n",
    "        iids += list(new_iids)[:max_n]  # Load max_n individuals\n",
    "    print(f\"Loaded {len(iids)} Individuals\")\n",
    "  \n",
    "    prms = [give_param_list_rc_individual(iid=iid, ch=c, e_rate_ref=e_rate_ref, e_rate=e_rate, \n",
    "                                          base_out_folder = base_out_folder, prefix_out = prefix_out, logfile=logfile) for iid in iids for c in chs]\n",
    "    return prms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3542"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#pop_list = [\"Basque\", 'Spanish', 'French', 'Croatian', \"Cag\", \"Ogl\", \"Olb\"]\n",
    "#pop_list = [\"Car\", \"Cam\", \"Ori\", \"Sas\", \"Nuo\", \"Czech\", \"Bergamo\"]\n",
    "pop_list = ['Albanian',  'Ashkenazi_Jew', 'Belarusian', 'Bulgarian', 'Estonian', 'Finnish', 'French_South', 'Hungarian', 'Lithuanian', 'Maltese', 'Russian', 'Scottish', 'Spanish_North', 'Turkish', 'Tuscan', 'Ukrainian']\n",
    "#pop_list = [\"Italian_South\", \"Sicilian\", \"Icelandic\", \"Norwegian\", \"English\", \"Greek\"]\n",
    "\n",
    "prms = prepare_mod_sardinian_prms(pop_list, max_n = 20, base_out_folder=\"./Empirical/1240k/HO/\", logfile=True)\n",
    "len(prms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_run(analyze_individual_gt, prms, processes = 8)\n",
    "print(\"Finished run!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Postprocess the Marcus Ancients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "High Coverage Samples: 571\n"
     ]
    }
   ],
   "source": [
    "### Which IIDs to postprocess\n",
    "meta_path=\"./Data/Marcus2019_1240k/meta_rev_unique_ids.csv\"\n",
    "df_anno = pd.read_csv(meta_path)\n",
    "df_ana = df_anno[df_anno[\"mean_cov\"]>0.5]\n",
    "print(f\"High Coverage Samples: {len(df_ana)}\")\n",
    "df_ana = df_ana[:]  # how many individuals to extract\n",
    "iids = df_ana[\"iid\"].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine Chromosomes ROH into Individual ROH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Individual I1917 has faulty data!\n",
      "Individual I5232 has faulty data!\n",
      "Individual ILK001 has faulty data!\n",
      "Individual ANI163 has faulty data!\n",
      "Individual I5241 has faulty data!\n",
      "Individual I2433 has faulty data!\n",
      "Individual I4882 has faulty data!\n",
      "Individual ILK002 has faulty data!\n",
      "Individual I2441 has faulty data!\n",
      "Individual I5407 has faulty data!\n",
      "Individual I3499 has faulty data!\n",
      "Individual ILK003 has faulty data!\n",
      "Individual I1955 has faulty data!\n",
      "Individual I0854 has faulty data!\n"
     ]
    }
   ],
   "source": [
    "for iid in iids[30:]:\n",
    "    try:\n",
    "        combine_individual_data(base_path=\"./Empirical/1240k/MarcusAncs/\", iid=iid, delete=False, chs=range(1,23), prefix_out=\"e01/\")\n",
    "    except:\n",
    "        print(f\"Individual {iid} has faulty data!\")\n",
    "print(\"Finished the run!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create one overall ROH .csv from individual ROH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 571 / 4616 Individuals from Meta\n",
      "Warning, could not find 14 Paths:\n",
      "['./Empirical/1240k/MarcusAncs/I1917_roh_full.csv', './Empirical/1240k/MarcusAncs/I5232_roh_full.csv', './Empirical/1240k/MarcusAncs/ILK001_roh_full.csv', './Empirical/1240k/MarcusAncs/ANI163_roh_full.csv', './Empirical/1240k/MarcusAncs/I5241_roh_full.csv', './Empirical/1240k/MarcusAncs/I2433_roh_full.csv', './Empirical/1240k/MarcusAncs/I4882_roh_full.csv', './Empirical/1240k/MarcusAncs/ILK002_roh_full.csv', './Empirical/1240k/MarcusAncs/I2441_roh_full.csv', './Empirical/1240k/MarcusAncs/I5407_roh_full.csv', './Empirical/1240k/MarcusAncs/I3499_roh_full.csv', './Empirical/1240k/MarcusAncs/ILK003_roh_full.csv', './Empirical/1240k/MarcusAncs/I1955_roh_full.csv', './Empirical/1240k/MarcusAncs/I0854_roh_full.csv']\n",
      "Saved to: ./Empirical/1240k/MarcusAncs/combined_roh05.csv\n"
     ]
    }
   ],
   "source": [
    "df1 = pp_individual_roh(iids, meta_path=\"./Data/Marcus2019_1240k/meta_rev_final.csv\", base_folder=\"./Empirical/1240k/MarcusAncs/\",\n",
    "                        save_path=\"./Empirical/1240k/MarcusAncs/combined_roh05.csv\", output=False, min_cm=[4,8,12], snp_cm=50, gap=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Area 51"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set Output Log path: ./Empirical/1240k/HO/Italian_South_0/chr4/e01/hmm_run_log.txt\n"
     ]
    }
   ],
   "source": [
    "analyze_individual_gt(*prms[3])  # Single Test Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_path = \"./Data/Marcus2019_1240k/meta_rev_unique_ids.csv\"\n",
    "meta_df = pd.read_csv(meta_path)\n",
    "mod_df = meta_df[1098:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_t = pd.read_csv(\"./Empirical/1240k/MarcusAncs/combined_roh05.csv\", sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
