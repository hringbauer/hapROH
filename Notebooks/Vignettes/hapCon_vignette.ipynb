{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estimating Contamination Rate with hapCon for Male aDNA Samples\n",
    "\n",
    "Starting from version 0.4a1, hapROH package now has an extension called hapCon to estimate contamination for male aDNA samples.\n",
    "\n",
    "This small notebook walks you through how to use hapCon to run estimate contamination in your male aDNA sample. We will use one 1240k sample SUA001, from Sardinia, and a WGS sample DA43, from Mongolia, XiongNu to illustrate hapCon's usage on two different reference panels. In this tutorial, both samples have been downsampled to 0.1x to keep runtime minimal.\n",
    "\n",
    "You can download the two BAM files at https://www.dropbox.com/sh/tgvwq75mvixeyic/AAAGURMdDGWLIxGzwAgtGm1Sa?dl=0\n",
    "\n",
    "You can download the reference panel, the bed file used by samtools to get readcounts, and the metadata for reference panel at https://www.dropbox.com/s/1vv8mz9athedpiq/data.zip?dl=0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#hapCon with 1240k Reference Panel\n",
    "\n",
    "You can run hapCon either from samtools's pileup file or directly from BAM file. Running hapCon with BAM file is slower. We will first see how to run hapCon from samtools's pileup file.\n",
    "\n",
    "To generate the pileup file for SUA001, we need a bed file to specify regions of interest, which is in the dropbox link provided above. Our bed file assumes that the contig name in your BAM file doesn't have chr or Chr prefix. If that is the case for your BAM file, please reset the header of your BAM file by \"samtools reheader -c 'perl -pe \"s/^(@SQ.*)(\\tSN:)chr/\\$1\\$2/\"' in.bam > out.bam\". If you are unsure about the contig name of your BAM file, you can check it by \"samtools view -H in.bam\".\n",
    "\n",
    "We have assumed that you have put the BAM file at ./Data, and please change the path to bed file for 1240k panel according to your setup. After that, we can run the following to generate the pileup file,\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[mpileup] 1 samples in 1 input files\n"
     ]
    }
   ],
   "source": [
    "path2bam=\"./Data/SUA001.bam\"\n",
    "path2bed1240k=\"/mnt/archgen/users/yilei/Data/1000G/1000g1240khdf5/all1240/1240kChrX.bed\"\n",
    "!samtools index $path2bam\n",
    "!samtools mpileup --positions $path2bed1240k -r X -q 30 -Q 30 -o ./Data/SUA001.mpileup $path2bam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the pileup file in hand, we can now ran hapCon to estimate contamination rate. Below is an example run with default setting. \n",
    "\n",
    "Please change the path to reference panel and meta data according to your setup. \n",
    "\n",
    "The function hapCON_chrom_BFGS should run for about 1 minute and a half. It produces two output files, which by default reside in the same directory as the input mpileup file. The first output file is a hdf5 file, which is used as an intermediary data file for our method, and can be removed by setting cleanup=True in the function. The second file is the contamination estimate, which is named as $iid.hapCon.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"/mnt/archgen/users/yilei/tools/hapROH/package\") # TODO DELETE THIS LATER\n",
    "from hapsburg.PackagesSupport.hapsburg_run import hapCon_chrom_BFGS\n",
    "path2ref1240k=\"/mnt/archgen/users/yilei/Data/1000G/1000g1240khdf5/all1240/chrX.hdf5\"\n",
    "path2meta=\"/mnt/archgen/users/yilei/Data/1000G/1000g1240khdf5/all1240/meta_df_all.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exclude 1033 sites outside the specified region\n",
      "exclude 0 non-SNP sites\n",
      "number of major reads at flanking sites: 10636\n",
      "number of minor reads at flanking sites: 16\n",
      "number of major reads at focal sites: 1291\n",
      "number of minor reads at focal sites: 27\n",
      "err rate at flanking sites: 0.001502\n",
      "err rate at focal sites: 0.020486\n",
      "saving sample as SUA001 in /mnt/archgen/users/yilei/tools/hapROH/Notebooks/Vignettes/Data/SUA001.hdf5\n",
      "estimated genotyping error by flanking sites: 0.001502\n",
      "number of sites covered by at least one read: 3999, fraction covered: 0.085\n",
      "hdf5 file saved to /mnt/archgen/users/yilei/tools/hapROH/Notebooks/Vignettes/Data/SUA001.hdf5\n",
      "finished reading mpileup file, takes 2.081.\n",
      "number of sites covered by at least one read: 3999\n",
      "hdf5 file saved to /mnt/archgen/users/yilei/tools/hapROH/Notebooks/Vignettes/Data/SUA001.hdf5\n",
      "estimated contamination rate: 0.102113(0.076802 - 0.127424)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.10211284424631027, 0.07680173621212716, 0.12742395228049339, 3999)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hapCon_chrom_BFGS(iid=\"SUA001\", mpileup=\"./Data/SUA001.mpileup\",\n",
    "    h5_path1000g = path2ref1240k, meta_path_ref = path2meta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you have a finished a hapCon run on SUA001! The estimated contamination should be about 10%. This is a highly contaminated sample! Now let's try to run hapCon directly from a BAM file. Running hapCon come BAM file is a bit slower. The following code should take about 1min."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exclude 1033 sites outside the specified region\n",
      "exclude 0 non-SNP sites\n",
      "total number of mapped reads: 14755\n",
      "number of major reads at flanking sites: 10521\n",
      "number of minor reads at flanking sites: 15\n",
      "number of major reads at focal sites: 1291\n",
      "number of minor reads at focal sites: 27\n",
      "err rate at flanking sites: 0.001424\n",
      "err rate at focal sites: 0.020486\n",
      "saving sample as SUA001 in /mnt/archgen/users/yilei/tools/hapROH/Notebooks/Vignettes/Data/SUA001.hdf5\n",
      "estimated genotyping error by flanking sites: 0.001424\n",
      "number of sites covered by at least one read: 3999, fraction covered: 0.085\n",
      "hdf5 file saved to /mnt/archgen/users/yilei/tools/hapROH/Notebooks/Vignettes/Data/SUA001.hdf5\n",
      "finished reading bam file, takes 65.195.\n",
      "number of sites covered by at least one read: 3999\n",
      "hdf5 file saved to /mnt/archgen/users/yilei/tools/hapROH/Notebooks/Vignettes/Data/SUA001.hdf5\n",
      "estimated contamination rate: 0.102190(0.076878 - 0.127501)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.10218983741613123, 0.07687833720392445, 0.127501337628338, 3999)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hapCon_chrom_BFGS(iid=\"SUA001\", bam=\"./Data/SUA001.bam\",\n",
    "    h5_path1000g = path2ref1240k, meta_path_ref = path2meta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# hapCon with 1000G Reference Panel\n",
    "With WGS data, we recommend using hapCon with the 1000G reference panel instead. This reference panel contains all biallelic sites with MAF greater than 5% in the 1000Genome dataset, therefore it is much more powerful than the 1240k reference panel. We will use DA43, a Mongolia XiongNu WGS sample. Let's first generate a pileup file for it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have assumed that you have put the BAM file of DA43 at ./Data, and please change the path to bed file for 1000G panel according to your setup. After that, we can run the following to generate the pileup file,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[mpileup] 1 samples in 1 input files\n"
     ]
    }
   ],
   "source": [
    "path2bam=\"./Data/DA43.bam\"\n",
    "path2bed1kg=\"/mnt/archgen/users/yilei/Data/1000G/1000g1240khdf5/all1240/maf5FilterChrX.bed\"\n",
    "!samtools index $path2bam\n",
    "!samtools mpileup --positions $path2bed1kg -r X -q 30 -Q 30 -o ./Data/DA43.mpileup $path2bam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the pileup file, we can run hapCon on DA43 similar as we did to SUA001. Please change the path to the 1000G referene panel according to your setup. Running hapCon with 1000G panel is slower than that with 1240k panel, as it contains 4 times more sites. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exclude 1033 sites outside the specified region\n",
      "exclude 0 non-SNP sites\n",
      "number of major reads at flanking sites: 18220\n",
      "number of minor reads at flanking sites: 184\n",
      "number of major reads at focal sites: 390\n",
      "number of minor reads at focal sites: 13\n",
      "err rate at flanking sites: 0.009998\n",
      "err rate at focal sites: 0.032258\n",
      "saving sample as DA43 in /mnt/archgen/users/yilei/tools/hapROH/Notebooks/Vignettes/Data/DA43.hdf5\n",
      "estimated genotyping error by flanking sites: 0.009998\n",
      "number of sites covered by at least one read: 3595, fraction covered: 0.077\n",
      "hdf5 file saved to /mnt/archgen/users/yilei/tools/hapROH/Notebooks/Vignettes/Data/DA43.hdf5\n",
      "finished reading mpileup file, takes 7.541.\n",
      "number of sites covered by at least one read: 3595\n",
      "hdf5 file saved to /mnt/archgen/users/yilei/tools/hapROH/Notebooks/Vignettes/Data/DA43.hdf5\n",
      "estimated contamination rate: 0.022810(0.008837 - 0.036783)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.02280989758577893, 0.00883673729864813, 0.03678305787290973, 3595)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path2ref1kg=\"/mnt/archgen/users/yilei/Data/1000G/1000g1240khdf5/all1240/maf5_filter_chrX.hdf5\"\n",
    "hapCon_chrom_BFGS(iid=\"DA43\", mpileup=\"./Data/DA43.mpileup\",\n",
    "    h5_path1000g = path2ref1240k, meta_path_ref = path2meta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The estimated contamination should be between 2% to 3%. Now you have finished your first trial with 1000G reference panel!\n",
    "\n",
    "Alternatively, we can also run directly from the BAM file. This should take about 1min."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hapCon_chrom_BFGS(iid=\"DA43\", bam=\"./Data/DA43.bam\",\n",
    "    h5_path1000g = path2ref1240k, meta_path_ref = path2meta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using hapCon's command line tool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you prefer to run hapCon from the command line, you can use our wrapper, which provides the same functionality as the hapCon_chrom_BFGS function, where all the parameters can be set by the command line argument. To see the full list of customizable parameters, use -h. Below we show the most basic use of this command line wrapper.\n",
    "\n",
    "To run hapCon on a single sample from the command line, use hapCONX.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exclude 1033 sites outside the specified region\n",
      "exclude 0 non-SNP sites\n",
      "number of major reads at flanking sites: 10636\n",
      "number of minor reads at flanking sites: 16\n",
      "number of major reads at focal sites: 1291\n",
      "number of minor reads at focal sites: 27\n",
      "err rate at flanking sites: 0.001502\n",
      "err rate at focal sites: 0.020486\n",
      "saving sample as SUA001 in /mnt/archgen/users/yilei/tools/hapROH/Notebooks/Vignettes/Data/SUA001.hdf5\n",
      "estimated genotyping error by flanking sites: 0.001502\n",
      "number of sites covered by at least one read: 3999, fraction covered: 0.085\n",
      "hdf5 file saved to /mnt/archgen/users/yilei/tools/hapROH/Notebooks/Vignettes/Data/SUA001.hdf5\n",
      "finished reading mpileup file, takes 1.859.\n",
      "number of sites covered by at least one read: 3999\n",
      "hdf5 file saved to /mnt/archgen/users/yilei/tools/hapROH/Notebooks/Vignettes/Data/SUA001.hdf5\n",
      "estimated contamination rate: 0.102113(0.076802 - 0.127424)\n",
      "exclude 1033 sites outside the specified region\n",
      "exclude 0 non-SNP sites\n",
      "total number of mapped reads: 14755\n",
      "number of major reads at flanking sites: 10521\n",
      "number of minor reads at flanking sites: 15\n",
      "number of major reads at focal sites: 1291\n",
      "number of minor reads at focal sites: 27\n",
      "err rate at flanking sites: 0.001424\n",
      "err rate at focal sites: 0.020486\n",
      "saving sample as SUA001 in /mnt/archgen/users/yilei/tools/hapROH/Notebooks/Vignettes/Data/SUA001.hdf5\n",
      "estimated genotyping error by flanking sites: 0.001424\n",
      "number of sites covered by at least one read: 3999, fraction covered: 0.085\n",
      "hdf5 file saved to /mnt/archgen/users/yilei/tools/hapROH/Notebooks/Vignettes/Data/SUA001.hdf5\n",
      "finished reading bam file, takes 64.967.\n",
      "number of sites covered by at least one read: 3999\n",
      "hdf5 file saved to /mnt/archgen/users/yilei/tools/hapROH/Notebooks/Vignettes/Data/SUA001.hdf5\n",
      "estimated contamination rate: 0.102190(0.076878 - 0.127501)\n"
     ]
    }
   ],
   "source": [
    "path2script=\"/mnt/archgen/users/yilei/tools/hapROH/bam\" # change this to your own path\n",
    "!python3 $path2script/hapCONX.py -m ./Data/SUA001.mpileup -r $path2ref1240k --meta $path2meta # run on pileup file\n",
    "!python3 $path2script/hapCONX.py -b ./Data/SUA001.bam -r $path2ref1240k --meta $path2meta # run on BAM file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is also a script (hapCONX_batch.py) with which you can run multiple samples in a batch. In the end a .tsv file will be made summarizing the results for each sample. For this, you need to provide a file that contains a list of pileup files or BAM files. Each line in the file should have two columns, the first column is the sample IID, and the second column is the path to pileup/BAM files. The two columns should be separted by a tab.\n",
    "\n",
    "In the final .tsv file, each row summarizes the result for one sample. The first column is the sample IID, the second column is of the form \"MLE for contamination(low 95% CI - high 95% CI)\", and the third column is the number of sites covered by at least one read. The third column essentailly says how many sites are utilized in contamination estimates (i.e, the length of the HMM chain). A rule of thumb is that we need at least 2000 sites to have confident contamination estimates. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# what's more\n",
    "More detailed documentation (including the full list of user-adjustable parameters) about hapCon can be seen at https://haproh.readthedocs.io/en/latest/hapCON.html"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
