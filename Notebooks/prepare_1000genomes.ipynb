{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook to Prepare 1000 Genomes Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import allel\n",
    "import h5py  # Python Package to do the HDF5.\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#path_vcf = \"../Data/1000Genomes/ALL.chrX.phase3_shapeit2_mvncall_integrated_v1b.20130502.genotypes.vcf.gz\" # Path of VCF\n",
    "path_vcf = \"../Data/1000Genomes/1000gX1240kEur.vcf\"\n",
    "#h5_path = \"../Data/1000Genomes/1000Genomes_X.hdf5\"\n",
    "h5_path = \"../Data/1000Genomes/1000gX1240kEur.hdf5\"\n",
    "ind_path = \"../Data/1000Genomes/integrated_call_samples_v2.20130502.ALL.ped\"  # Family Relationships\n",
    "pop_path = \"../Data/1000Genomes/integrated_call_samples_v3.20130502.ALL.panel\" # Population Information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do the Conversion to HDF 5 \n",
    "Comment out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Do the conversion to hdf5 (if not done already)\n",
    "# geno = allel.read_vcf(path_vcf) # Load the VCF # Load the VCF # Needs too much Memory for my laptop\n",
    "allel.vcf_to_hdf5(input=path_vcf, output=h5_path, compression=\"gzip\") # Do the conversion to hdf5. Takes 10 Minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 47486 variants\n",
      "Loaded 503 individuals\n",
      "['GT']\n",
      "['ALT', 'CHROM', 'FILTER_PASS', 'ID', 'POS', 'QUAL', 'REF']\n"
     ]
    }
   ],
   "source": [
    "## Load HDF5\n",
    "f = h5py.File(h5_path, \"r\") # Load for Sanity Check. See below!\n",
    "print(\"Loaded %i variants\" % np.shape(f[\"calldata/GT\"])[0])\n",
    "print(\"Loaded %i individuals\" % np.shape(f[\"calldata/GT\"])[1])\n",
    "print(list(f[\"calldata\"].keys()))\n",
    "print(list(f[\"variants\"].keys()))\n",
    "#print(list(f[\"samples\"].keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Load the ID of the Individuals\n",
    "ids = np.array(f[\"variants/ID\"])\n",
    "df_s_empirical = pd.DataFrame({'Individual ID' : list(f[\"samples\"])})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_s_empirical[\"Individual ID\"] = df_s_empirical[\"Individual ID\"].str.split(\"_\").str[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and Merge Individual Meta Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 3691 Individuals\n",
      "Loaded 3691 Population Data\n",
      "Merged from 2504 to 2504 individuals\n"
     ]
    }
   ],
   "source": [
    "df_i = pd.read_csv(ind_path, sep=\"\\t\")\n",
    "print(f\"Loaded {np.shape(df_i)[0]} Individuals\")\n",
    "\n",
    "df_pops = pd.read_csv(pop_path, sep=\"\\t\")\n",
    "print(f\"Loaded {np.shape(df_i)[0]} Population Data\")\n",
    "\n",
    "### Merge with IDs in Genotype File\n",
    "df = pd.merge(df_s_empirical, df_i, on='Individual ID', how='inner')\n",
    "df = pd.merge(df, df_pops, left_on=\"Individual ID\", right_on=\"sample\", how=\"inner\")\n",
    "print(f\"Merged from {len(df_s_empirical)} to {len(df)} individuals\")\n",
    "assert(len(df_s_empirical) == len(df))  # Sanity Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GWD    113\n",
       "YRI    108\n",
       "TSI    107\n",
       "IBS    107\n",
       "CHS    105\n",
       "JPT    104\n",
       "PUR    104\n",
       "CHB    103\n",
       "GIH    103\n",
       "ITU    102\n",
       "STU    102\n",
       "FIN     99\n",
       "KHV     99\n",
       "CEU     99\n",
       "ESN     99\n",
       "LWK     99\n",
       "ACB     96\n",
       "PJL     96\n",
       "CLM     94\n",
       "CDX     93\n",
       "GBR     91\n",
       "BEB     86\n",
       "MSL     85\n",
       "PEL     85\n",
       "MXL     64\n",
       "ASW     61\n",
       "Name: Population, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"Population\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Individual List\n",
    "Save table with Individual and Family IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nr of European Samples: 503\n",
      "Saved to ../Data/1000Genomes/EUR.csv. Nr Individuals: 503\n",
      "Nr of European Samples: 503\n",
      "Saved to ../Data/1000Genomes/EUR_fam.csv. Nr Individuals: 503\n"
     ]
    }
   ],
   "source": [
    "save_path = \"../Data/1000Genomes/EUR.csv\"\n",
    "\n",
    "eur_inds = df[\"super_pop\"]==\"EUR\"\n",
    "print(f\"Nr of European Samples: {np.sum(eur_inds)}\")\n",
    "\n",
    "df_save = df[eur_inds]\n",
    "df_save = df_save[[\"Family ID\", \"Individual ID\"]] # Extract column of Individual and Family ID\n",
    "df_save.to_csv(save_path, sep=\"\\t\", header=None, index=False)\n",
    "print(f\"Saved to {save_path}. Nr Individuals: {len(df_save)}\")\n",
    "\n",
    "\n",
    "### Do the same, but for families\n",
    "save_path = \"../Data/1000Genomes/EUR_fam.csv\"\n",
    "\n",
    "eur_inds = df[\"super_pop\"]==\"EUR\"\n",
    "print(f\"Nr of European Samples: {np.sum(eur_inds)}\")\n",
    "\n",
    "df_save = df[eur_inds]\n",
    "df_save = df_save[\"Individual ID\"] # Extract column of Individual and Family ID\n",
    "df_save.to_csv(save_path, sep=\"\\t\", header=None, index=False)\n",
    "print(f\"Saved to {save_path}. Nr Individuals: {len(df_save)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check against Sardinian X data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py:2230: FutureWarning: split() requires a non-empty pattern match.\n",
      "  yield pat.split(line.strip())\n",
      "/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py:2232: FutureWarning: split() requires a non-empty pattern match.\n",
      "  yield pat.split(line.strip())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 49711 X SNPs.\n"
     ]
    }
   ],
   "source": [
    "path_snp = \"../../ancient-sardinia/data/bed/full230.snp\" # All SNPs found in the 1240k Ancient Panel\n",
    "\n",
    "df_snp = pd.read_csv(path_snp, header=None, sep=r\"\\s*\", engine=\"python\")\n",
    "df_snp.columns = [\"SNP\", \"chr\", \"map\", \"pos\", \"ref\", \"alt\"]\n",
    "df_snp = df_snp[df_snp[\"chr\"]==23]\n",
    "\n",
    "print(f\"Loaded {len(df_snp)} X SNPs.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save  interesection with 1240k Marker \n",
    "Prepare txt List for Plink filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intersection: 48298 out of 3468093 SNPS\n",
      "Found 812 unnamed SNPs\n",
      "Successfully saved to ../Data/1000Genomes/variants1240k. Length: 47486\n"
     ]
    }
   ],
   "source": [
    "save_path = \"../Data/1000Genomes/variants1240k\"\n",
    "\n",
    "found = np.isin(f[\"variants/POS\"], df_snp[\"pos\"])\n",
    "print(f\"Intersection: {np.sum(found)} out of {len(found)} SNPS\")\n",
    "variants = f[\"variants/ID\"][found]\n",
    "\n",
    "dots = np.where(variants == \".\")[0]\n",
    "print(f\"Found {len(dots)} unnamed SNPs\")\n",
    "variants = np.delete(variants, dots)\n",
    "\n",
    "np.savetxt(save_path, variants, fmt=\"%s\")\n",
    "print(f\"Successfully saved to {save_path}. Length: {len(variants)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare downsampled Autosomal hdf5s\n",
    "Prepare a 1000 Genome autosomal hdf5 file. Include Recombination Map\n",
    "Input: 1000 Genome vcf file, Recombination Map from a 1240k Eigenstrat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Important Parameters and paths\n",
    "ch = 3 # Which Chromosome to use:\n",
    "\n",
    "# Path of the 1000 Genome VCF:\n",
    "p1, p2 = \"\", \"\"\n",
    "file_vcf100g, path_vcf100g = \"\", \"\"\n",
    "out_vcf_path0, out_vcf_path = \"\", \"\"\n",
    "path_hdf5temp, path_hdf5final = \"\", \"\"\n",
    "\n",
    "snp1240k_path, ind_path = \"\", \"\"   # Where to find the 1240k SNPs\n",
    "snp_filter_path = \"\"\n",
    "\n",
    "def prepare_paths(ch = 3):\n",
    "    \"\"\"Prepares all the Paths need for processing Steps.\n",
    "    ch: Which Chromosomes to use\"\"\"\n",
    "    global p1, p2, file_vcf100g, path_vcf100g, out_vcf_path0, out_vcf_path, path_hdf5temp, path_hdf5final, snp1240k_path, ind_path, snp_filter_path\n",
    "    \n",
    "    # Path of the 1000 Genome VCF:\n",
    "    p1 = \"../Data/1000Genomes/AutosomeVCF/\"\n",
    "    p2 = \".phase3_shapeit2_mvncall_integrated_v5a.20130502.genotypes.vcf.gz\"\n",
    "    file_vcf100g = \"ALL.chr\" + str(ch) + p2\n",
    "    path_vcf100g = p1 + file_vcf100g\n",
    "    print(f\"Full Input path:\\n{path_vcf100g}\")\n",
    "    out_vcf_path0 = \"../Data/1000Genomes/AutosomeVCF/Subset/\" + \"1240EURchr\" + str(ch) # needs no .vcf\n",
    "    out_vcf_path = out_vcf_path0 + \".vcf\"\n",
    "    path_hdf5temp = \"../Data/1000Genomes/HDF5/FULLHDF5/cr\" + str(ch) + \".hdf5\"\n",
    "    path_hdf5final = \"../Data/1000Genomes/HDF5/1240kHDF5/Eur1240chr\" + str(ch) + \".hdf5\"\n",
    "    snp1240k_path = \"../Data/1000Genomes/Markers/MinMyc.snp\"   # Where to find the 1240k SNPs\n",
    "    ind_path = \"../Data/1000Genomes/Individuals/EUR_fam.csv\"   # Where to find the individual lists\n",
    "\n",
    "    # Path of SNP Filter\n",
    "    snp_filter_path = \"../Data/1000Genomes/Markers/variants1240k\" + str(ch) + \".txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 0: Download the Data\n",
    "### Step 1: Produce hdf5 file for all markers\n",
    "### Step 2: Extract Positions. Match with Eigenstrat File Positions\n",
    "### Step 3: Create new vcf based on subset of Individuals and Markers\n",
    "### Step 4: Transfer to hdf5. \n",
    "### Step 5: Merge in Linkage Map\n",
    "### Step 6: Quality Check? (Control ref/alt against hdf5 we have for Sardinians)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Step 0: Download the Data\n",
    "def download_1kg():\n",
    "    path_cl = \"/project/jnovembre/data/external_public/1kg_phase3/haps/\"\n",
    "    path_cluster = \"hringbauer@midway.rcc.uchicago.edu:\" + path_cl + file_vcf100g\n",
    "    !scp $path_cluster $path_vcf100g # Only Download the .vcf (not the .tbi)\n",
    "    \n",
    "### Step 1: Produce hdf5 file for all markers\n",
    "def vcf_to_hdf5(in_path, out_path):\n",
    "    allel.vcf_to_hdf5(input=in_path, output=out_path, compression=\"gzip\") # Takes 10 Minutes\n",
    "    \n",
    "### Step 2: Extract Positions. Match with Eigenstrat File Positions\n",
    "### Load HDF5\n",
    "\n",
    "def merge_positions():\n",
    "    f_full = h5py.File(path_hdf5temp, \"r\") # Load for Sanity Check. See below!\n",
    "    print(\"Loaded %i variants\" % np.shape(f_full[\"calldata/GT\"])[0])\n",
    "    print(\"Loaded %i individuals\" % np.shape(f_full[\"calldata/GT\"])[1])\n",
    "    print(list(f_full[\"calldata\"].keys()))\n",
    "    print(list(f_full[\"variants\"].keys()))\n",
    "    #print(list(f[\"samples\"].keys()))\n",
    "\n",
    "    ### Load Eigenstrat\n",
    "    df_snp = pd.read_csv(snp1240k_path, header=None, sep=r\"\\s*\", engine=\"python\")\n",
    "    df_snp.columns = [\"SNP\", \"chr\", \"map\", \"pos\", \"ref\", \"alt\"]\n",
    "    df_snp = df_snp[df_snp[\"chr\"] == ch]\n",
    "    print(f\"Loaded {len(df_snp)} Chr.{ch} SNPs.\")\n",
    "\n",
    "    ### Prepare SNP File for Eigenstrat filtering \n",
    "    found = np.isin(f_full[\"variants/POS\"], df_snp[\"pos\"])\n",
    "    print(f\"Intersection: {np.sum(found)} out of {len(found)} SNPS\")\n",
    "    variants = f_full[\"variants/ID\"][found]\n",
    "\n",
    "    dots = np.where(variants == \".\")[0]\n",
    "    print(f\"Found {len(dots)} unnamed SNPs\")\n",
    "    variants = np.delete(variants, dots)\n",
    "\n",
    "    np.savetxt(snp_filter_path, variants, fmt=\"%s\")\n",
    "    print(f\"Successfully saved to {snp_filter_path}. Length: {len(variants)}\")\n",
    "    \n",
    "### Step 3: Create new vcf based on subset of Individuals and Markers\n",
    "def plink_new_vcf():\n",
    "    !plink --vcf $path_vcf100g --extract $snp_filter_path --keep-fam $ind_path --recode vcf --out $out_vcf_path0 --biallelic-only strict --keep-allele-order\n",
    "    \n",
    "### Step 4: Transfer to hdf5.\n",
    "#allel.vcf_to_hdf5(input=out_vcf_path, output=path_hdf5final, compression=\"gzip\") # Takes 1s\n",
    " \n",
    "### Step 5: Merge in Linkage Map\n",
    "### Load HDF5\n",
    "def merge_in_ld_map():\n",
    "    \"\"\"Merge in ld_map into HDF5!\"\"\"\n",
    "    f = h5py.File(path_hdf5final, \"r\") # Load for Sanity Check. See below!\n",
    "    print(\"Loaded %i variants\" % np.shape(f[\"calldata/GT\"])[0])\n",
    "    print(\"Loaded %i individuals\" % np.shape(f[\"calldata/GT\"])[1])\n",
    "    print(list(f[\"calldata\"].keys()))\n",
    "    print(list(f[\"variants\"].keys()))\n",
    "    #print(list(f[\"samples\"].keys()))\n",
    "\n",
    "    ### Load Eigenstrat\n",
    "    df_snp = pd.read_csv(snp1240k_path, header=None, sep=r\"\\s*\", engine=\"python\")\n",
    "    df_snp.columns = [\"SNP\", \"chr\", \"map\", \"pos\", \"ref\", \"alt\"]\n",
    "    df_snp = df_snp[df_snp[\"chr\"] == ch]\n",
    "    print(f\"Loaded {len(df_snp)} Chr.{ch} SNPs.\")\n",
    "\n",
    "    ### Intersect SNP positions\n",
    "    its, i1, i2 = np.intersect1d(f[\"variants/POS\"], df_snp[\"pos\"], return_indices=True)\n",
    "\n",
    "    l = len(f[\"variants/POS\"])\n",
    "    print(f\"Intersection {len(i2)} out of {l}\")\n",
    "\n",
    "    ### Extract \n",
    "    rec = np.zeros(len(f[\"variants/POS\"]))\n",
    "    rec[i1] = df_snp[\"map\"].values[i2]  # Fill in the values in Recombination map\n",
    "\n",
    "    ids0 = np.where(rec == 0)[0] # The 0 Values\n",
    "    rec[ids0] = (rec[ids0-1] + rec[ids0+1]) / 2.0 # Interpolate\n",
    "\n",
    "    ### Make sure that sorted\n",
    "    assert(np.all(np.diff(rec)>=0))  # Assert the Recombination Map is sorted! (no 0 left and no funky stuff)\n",
    "\n",
    "    f.close()\n",
    "    with h5py.File(path_hdf5final, 'a') as f0:\n",
    "        group = f0[\"variants\"]\n",
    "        group.create_dataset('MAP', (l,), dtype='f')   \n",
    "        f0[\"variants/MAP\"][:] = rec[:]\n",
    "\n",
    "    print(f\"Finished Chromosome {ch}\")\n",
    "    \n",
    "### Step 6: Delete the Data:\n",
    "def del_temp_data():\n",
    "    !rm $path_vcf100g # Remove the full .vcf\n",
    "    !rm $out_vcf_path\n",
    "    !rm $path_hdf5temp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# All in one Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_1000genomes_full(ch):\n",
    "    \"\"\"ch: Which Chromosome to prepare\"\"\"\n",
    "    prepare_paths(ch = ch)\n",
    "    download_1kg()\n",
    "    print(\"Download Complete\")\n",
    "    vcf_to_hdf5(in_path=path_vcf100g, out_path=path_hdf5temp) # Takes 10 Minutes\n",
    "    print(\"Transformation to HDF5 Complete.\")\n",
    "    merge_positions()\n",
    "    plink_new_vcf()\n",
    "    vcf_to_hdf5(in_path=out_vcf_path, out_path=path_hdf5final)\n",
    "    merge_in_ld_map()\n",
    "    del_temp_data()\n",
    "    print(\"Finished Preparing HDF5. GZ!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full Input path:\n",
      "../Data/1000Genomes/AutosomeVCF/ALL.chr2.phase3_shapeit2_mvncall_integrated_v5a.20130502.genotypes.vcf.gz\n",
      "ALL.chr2.phase3_shapeit2_mvncall_integrated_v 100% 1252MB  11.2MB/s   01:51    \n",
      "Download Complete\n",
      "Transformation to HDF5 Complete.\n",
      "Loaded 7081600 variants\n",
      "Loaded 2504 individuals\n",
      "['GT']\n",
      "['ALT', 'CHROM', 'FILTER_PASS', 'ID', 'POS', 'QUAL', 'REF']\n",
      "Loaded 98657 Chr.2 SNPs.\n",
      "Intersection: 94814 out of 7081600 SNPS\n",
      "Found 0 unnamed SNPs\n",
      "Successfully saved to ../Data/1000Genomes/Markers/variants1240k2.txt. Length: 94814\n",
      "PLINK v1.90b6.9 64-bit (4 Mar 2019)            www.cog-genomics.org/plink/1.9/\n",
      "(C) 2005-2019 Shaun Purcell, Christopher Chang   GNU General Public License v3\n",
      "Logging to ../Data/1000Genomes/AutosomeVCF/Subset/1240EURchr2.log.\n",
      "Options in effect:\n",
      "  --biallelic-only strict\n",
      "  --extract ../Data/1000Genomes/Markers/variants1240k2.txt\n",
      "  --keep-allele-order\n",
      "  --keep-fam ../Data/1000Genomes/Individuals/EUR_fam.csv\n",
      "  --out ../Data/1000Genomes/AutosomeVCF/Subset/1240EURchr2\n",
      "  --recode vcf\n",
      "  --vcf ../Data/1000Genomes/AutosomeVCF/ALL.chr2.phase3_shapeit2_mvncall_integrated_v5a.20130502.genotypes.vcf.gz\n",
      "\n",
      "7860 MB RAM detected; reserving 3930 MB for main workspace.\n",
      "--vcf: ../Data/1000Genomes/AutosomeVCF/Subset/1240EURchr2-temporary.bed +\n",
      "../Data/1000Genomes/AutosomeVCF/Subset/1240EURchr2-temporary.bim +\n",
      "../Data/1000Genomes/AutosomeVCF/Subset/1240EURchr2-temporary.fam written.\n",
      "(34459 variants skipped.)\n",
      "7047141 variants loaded from .bim file.\n",
      "2504 people (0 males, 0 females, 2504 ambiguous) loaded from .fam.\n",
      "Ambiguous sex IDs written to\n",
      "../Data/1000Genomes/AutosomeVCF/Subset/1240EURchr2.nosex .\n",
      "--extract: 94251 variants remaining.\n",
      "--keep-fam: 503 people remaining.\n",
      "Using 1 thread (no multithreaded calculations invoked).\n",
      "Before main variant filters, 503 founders and 0 nonfounders present.\n",
      "Calculating allele frequencies... 10111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394959697989 done.\n",
      "94251 variants and 503 people pass filters and QC.\n",
      "Note: No phenotypes present.\n",
      "--recode vcf to ../Data/1000Genomes/AutosomeVCF/Subset/1240EURchr2.vcf ...\n",
      "101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899done.\n",
      "Loaded 94251 variants\n",
      "Loaded 503 individuals\n",
      "['GT']\n",
      "['ALT', 'CHROM', 'FILTER_PASS', 'ID', 'POS', 'QUAL', 'REF']\n",
      "Loaded 98657 Chr.2 SNPs.\n",
      "Intersection 94249 out of 94251\n",
      "Finished Chromosome 2\n",
      "Finished Preparing HDF5. GZ!\n"
     ]
    }
   ],
   "source": [
    "ch=2  # Which Chromosome to prepare\n",
    "\n",
    "prep_1000genomes_full(ch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
