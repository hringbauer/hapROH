{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python wrapper for bcftools\n",
    "Contains additional functions to mimic output of HAPSBURG for downstream analysis\n",
    "@Harald Ringbauer, October 2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "midway2-0401.rcc.local\n",
      "Midway jnovmbre partition detected.\n",
      "/project2/jnovembre/hringbauer/HAPSBURG\n",
      "CPU Count: 28\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import socket\n",
    "import os as os\n",
    "import sys as sys\n",
    "import multiprocessing as mp\n",
    "import h5py\n",
    "\n",
    "socket_name = socket.gethostname()\n",
    "print(socket_name)\n",
    "if socket_name == \"VioletQueen\":\n",
    "    path = \"/home/harald/git/HAPSBURG/\"   # The Path on Harald's machine\n",
    "elif socket_name.startswith(\"midway2\"):\n",
    "    print(\"Midway jnovmbre partition detected.\")\n",
    "    path = \"/project2/jnovembre/hringbauer/HAPSBURG/\"  # The Path on Midway Cluster\n",
    "else: \n",
    "    raise RuntimeWarning(\"Not compatible machine. Check!!\")\n",
    "    \n",
    "os.chdir(path)  # Set the right Path (in line with Atom default)\n",
    "\n",
    "sys.path.append(\"./PackagesSupport/h5_python/\")\n",
    "from h5_functions import hdf5_to_vcf, load_h5   # Import Function to convert hdf5 to vcf\n",
    "sys.path.append(\"./PackagesSupport/parallel_runs/\")\n",
    "from helper_functions import prepare_path, create_folders, postprocess_iid  # To split up ground truth ROH\n",
    "\n",
    "print(os.getcwd()) # Show the current working directory. Should be HAPSBURG/Notebooks/ParallelRuns\n",
    "print(f\"CPU Count: {mp.cpu_count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions to run VCF Tools for a single sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def post_process_bcftools(outfile):\n",
    "    \"\"\"Post Process bcftools output\"\"\"\n",
    "    outtxt = outfile + \".txt\"\n",
    "    outST = outfile + \"ST.txt\"\n",
    "    outRG = outfile + \"RG.txt\"\n",
    "    \n",
    "    !grep ^ST $outtxt > $outST\n",
    "    !grep ^RG $outtxt > $outRG\n",
    "    #!rm $outtxt # Clean up the now redundant original output\n",
    "    \n",
    "    df_pos = pd.read_csv(outST, sep='\\t', header=None, usecols=range(1,6))\n",
    "    df_pos.columns = [\"iid\", \"ch\", \"pos\", \"state\", \"qual\"]\n",
    "    df_rohs = pd.read_csv(outRG, sep='\\t', header=None, usecols=range(1,8))\n",
    "    df_rohs.columns = [\"iid\", \"ch\", \"Start\", \"End\", \"length\", \"markers\", \"qual\"]\n",
    "    return df_pos, df_rohs\n",
    "\n",
    "def run_bcftools_roh(vcf_file, outfile, mp=\"./Data/1000Genomes/Markers/rec_map_bcf.chr3.txt\",\n",
    "                    af=\"./Data/1000Genomes/Markers/af_1000G_EUR_bcf.chr3.txt.gz\"):\n",
    "    \"\"\"Run PLINK ROH Caller on path_vcf, and save results in outfile.txt.\n",
    "    Uses Map File mp and Allele Frequency File AF (prepared in prep_map_af_bcftools.ipynb)\n",
    "    Return 2 Dataframes (per site,  total roh blocks )\"\"\"\n",
    "    outtxt = outfile + \".txt\"\n",
    "    !bcftools roh -G30 --AF-file $af -m $mp $vcf_file > $outtxt\n",
    "    # -V 1e-10   ### Command to do Viterbi Training\n",
    "    \n",
    "def create_hapsburg_df(df_t, map_dct):\n",
    "    \"\"\"Modify bcftools output to HAPSBURG format.\n",
    "    Return right Dataframe\"\"\"\n",
    "    df_t[\"StartM\"] = df_t[\"Start\"].map(map_dct)\n",
    "    df_t[\"EndM\"] = df_t[\"End\"].map(map_dct)\n",
    "    df_t[\"lengthM\"] = df_t[\"EndM\"] - df_t[\"StartM\"]\n",
    "\n",
    "    # Add all fields for roh.csv\n",
    "    df_t = df_t[[\"Start\", \"End\", \"StartM\", \"EndM\", \"length\", \"lengthM\", \"iid\", \"ch\"]]\n",
    "    return df_t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick Single Example Run on one VCF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "vcf_file = \"./Simulated/1000G_Mosaic/TSI5/ch3_6cm/data.vcf\"\n",
    "outfile  = \"./Diverse/bcfroh_out\"\n",
    "\n",
    "run_bcftools_roh(vcf_file, outfile)\n",
    "df_pos, df_rohs = post_process_bcftools(outfile)\n",
    "#df_rohs = create_hapsburg_df(df_rohs, map_dct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################\n",
    "### Combine all subfunctions\n",
    "\n",
    "def full_bcftools_mosaic(input_base_folder, ch=3, prefix_out=\"bcftools/\", convert_h5=True):\n",
    "    \"\"\"Run PLINK on Mosaic Data Set in ./Simulated\"\"\"\n",
    "    \n",
    "    print(\"Converting HDF5 into VCF...\")\n",
    "    input_h5, input_vcf, bcf_folder = create_folders(input_base_folder, outfolder=\"bcf_out/\")\n",
    "    if convert_h5:\n",
    "        hdf5_to_vcf(input_h5, input_vcf, chrom=ch) # Convert to VCF\n",
    "    \n",
    "    print(\"Running bcftools/ROH...\")\n",
    "    outfile = bcf_folder + \"bcfroh_out\"\n",
    "    run_bcftools_roh(input_vcf, outfile)   # Run BCF tools on VCF\n",
    "    df_pos, df_rohs = post_process_bcftools(outfile) # Load the output Data\n",
    "    \n",
    "    ### Create the Mapping Dictionary\n",
    "    print(\"Creating Map Dict...\")\n",
    "    f = load_h5(path=input_h5, output=False)\n",
    "    map_dct = dict(zip(f[\"variants/POS\"], f[\"variants/MAP\"]))\n",
    "    iids = f[\"samples\"][:] # Get the IIDs\n",
    "    \n",
    "    print(\"Splitting up BCF results and GT...\")\n",
    "    df_rohs = create_hapsburg_df(df_rohs, map_dct)\n",
    "    postprocess_iid(df_rohs, input_base_folder, iids, ch, prefix_out)\n",
    "    print(f\"Finished {len(iids)} Individuals!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run bcftools on all Individuals for Mosaic Folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting HDF5 into VCF...\n",
      "Running bcftools/ROH...\n",
      "Number of target samples: 100\n",
      "Number of --estimate-AF samples: 0\n",
      "Number of sites in the buffer/overlap: unlimited\n",
      "Number of lines total/processed: 77652/70453\n",
      "Creating Map Dict...\n",
      "Splitting up BCF results and GT...\n",
      "Finished 100 Individuals!\n",
      "CPU times: user 18.2 s, sys: 750 ms, total: 19 s\n",
      "Wall time: 28.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "full_bcftools_mosaic(input_base_folder = \"./Simulated/1000G_Mosaic/TSI5/ch3_6cm\",\n",
    "                     ch=3, prefix_out=\"bcftools/\", convert_h5=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run multiple lengths of copied in Chromosomes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path =  \"./Simulated/1000G_Mosaic/TSI5/\"\n",
    "\n",
    "for l in [0, 2, 4, 6, 8, 10]:\n",
    "    input_base_folder = base_path + f\"ch3_{l}cm\"\n",
    "    print(f\"\\nDoing ROH bcftools on {input_base_folder}...\")\n",
    "    full_bcftools_mosaic(input_base_folder, ch=3, prefix_out=\"bcftools/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split up the Posterior Output for Mosaic Folders\n",
    "Run to split map.csv and posterior0.csv into bcftools/ output folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def post_process_postbcf(basepath, map_dct):\n",
    "    \"\"\"Post Process the Posterior of bcftools\"\"\"\n",
    "    df_t = pd.read_csv(basepath + \"bcf_out/bcfroh_outST.txt\", header=None, sep=\"\\t\")\n",
    "    df_t.drop(columns=0, inplace=True) # Get rid of the first component\n",
    "    df_t.columns = [\"iid\", \"chr\", \"pos\", \"state\", \"post\"]\n",
    "\n",
    "    ### Transfrom it from PHRED scale to posterior\n",
    "    p = 10**(-df_t[\"post\"]/10)  # prob for alternative state\n",
    "    post = p * df_t[\"state\"] + (1-p) * (1 - df_t[\"state\"])   # 1 ROH State 0 HW\n",
    "    df_t[\"post\"] = np.log(post.values + 1e-10)\n",
    "    df_t[\"map\"] = df_t[\"pos\"].map(map_dct)\n",
    "    return df_t\n",
    "\n",
    "def split_up_bcftools_post(basepath, df_bcf, iid, ch, prefix_out=\"bcftools/\"):\n",
    "    \"\"\"Split up postprocessed Dataframe of bcftools/ROH output into\n",
    "    Mosaic folders. Save map and posterior file\"\"\"\n",
    "    output_base_folder = os.path.join(basepath, \"output/\")\n",
    "    pathout = prepare_path(output_base_folder, iid, ch=3, prefix_out=prefix_out, logfile=False)\n",
    "    df_t = df_bcf[df_bcf[\"iid\"] == iid]\n",
    "    \n",
    "    df_map = df_t[\"map\"]\n",
    "    mappath = os.path.join(pathout,\"map.csv\")\n",
    "    df_map.to_csv(mappath, sep=\",\", index=None, header=None)\n",
    "    \n",
    "    df_pos = df_t[\"post\"]\n",
    "    postpath = os.path.join(pathout,\"posterior0.csv\")\n",
    "    df_pos.to_csv(postpath, sep=\",\", index=None, header=None)\n",
    "    print(f\"Saved Posterior to {postpath}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Merge in Map(Takes about 10s)\n",
    "### Create the Mapping Dictionary (Run in Wrapper Function)\n",
    "input_h5 =  \"./Simulated/1000G_Mosaic/TSI5/ch3_6cm/data.h5\"\n",
    "\n",
    "print(\"Creating Map Dict...\")\n",
    "f = load_h5(path = input_h5, output=False)\n",
    "map_dct = dict(zip(f[\"variants/POS\"], f[\"variants/MAP\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved Posterior to ./Simulated/1000G_Mosaic/TSI5/ch3_6cm/output/iid1/chr3/bcftools/posterior0.csv\n"
     ]
    }
   ],
   "source": [
    "basepath = \"./Simulated/1000G_Mosaic/TSI5/ch3_6cm/\"\n",
    "iid=\"iid1\"\n",
    "\n",
    "df_t = post_process_postbcf(basepath, map_dct)\n",
    "split_up_bcftools_post(basepath, df_t, iid=iid, ch=3, prefix_out=\"bcftools/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Area 51\n",
    "Area to test code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split up the posterior from bcftools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert to VCF, storing the Genotype Likelihood!\n",
    "Move this piece of code eventually to packages support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_vcf(chrom, pos, ref, alt, gt, iids, vcf_path, header=[], pl=[]):\n",
    "    \"\"\"Saves VCF. If Genotype Likelihoods given (pl), save them too.\"\"\"\n",
    "    ### Hard-Coded Default Header\n",
    "    if len(header)==0:\n",
    "        header = \"\"\"##fileformat=VCFv4.3\\n##FILTER=<ID=PASS,Description=\"All filters passed\">\\n##fileDate=20191010\\n##source=1000GenomesPhase3Pipeline\\n##reference=ftp://ftp.1000genomes.ebi.ac.uk//vol1/ftp/technical/reference/phase2_reference_assembly_sequence/hs37d5.fa.gz\\n##contig=<ID=3,assembly=b37,length=198022430>\\n##FORMAT=<ID=GT,Number=1,Type=String,Description=\"Genotype\">\\n\"\"\"     \n",
    "        \n",
    "    #last_line_h =  \"\\n#CHROM POS ID REF ALT QUAL FILTER INFO\"\n",
    "    dct = {'#CHROM':chrom, 'POS':pos, 'REF':ref, 'ALT':alt}\n",
    "    df = pd.DataFrame(dct)\n",
    "    df['ID'] = \"\"\n",
    "    df['QUAL'] = 40\n",
    "    df['FILTER'] = \"PASS\"\n",
    "    df['INFO'] = \"\"\n",
    "    df[\"FORMAT\"] = \"GT\"  # GT:AD if allele depth given\n",
    "\n",
    "    df = df[['#CHROM', 'POS', 'ID', 'REF', 'ALT', 'QUAL', 'FILTER', 'INFO', \"FORMAT\"]] \n",
    "\n",
    "    ### Add the Genotype Data\n",
    "    add_gt_data(df, gt, iids=iids)\n",
    "    \n",
    "    ### Write the Header\n",
    "    with open(vcf_path, 'w') as vcf:\n",
    "        vcf.write(header)\n",
    "        #vcf.write(last_line_h)\n",
    "\n",
    "    #### Write the tab seperated data\n",
    "    df.to_csv(vcf_path, sep=\"\\t\", mode='a', index=False)  # Append\n",
    "    print(f\"Successfully saved VCF to {vcf_path}\")\n",
    "    \n",
    "def add_gt_data(df, gt, ad=[], iids=[], m_sym=\".\"):\n",
    "    \"\"\"Add Genotype and Allele Depth Fields [l,n,2] for iids to pandas dataframe df.\n",
    "    Return modified Data Frame\"\"\"\n",
    "    assert(np.shape(gt)[1]==len(iids)) # Sanity Check\n",
    "    \n",
    "    ### Replace missing Data with dot again\n",
    "    missing = gt<0  # Missing Data\n",
    "    gt = gt.astype(\"str\") ## Convert To String\n",
    "    gt[missing] = m_sym\n",
    "    \n",
    "    gt_vcf = np.core.defchararray.add(gt[:,:,0], \"/\")\n",
    "    gt_vcf = np.core.defchararray.add(gt_vcf, gt[:,:,1])\n",
    "        \n",
    "    for i, iid in enumerate(iids):\n",
    "        #data = map('/'.join, zip(gt[:,i,0], gt[:,i,1]))\n",
    "        df[iid] = gt_vcf[:,i]\n",
    "        \n",
    "        #if len(ad)>0:   # Add Allele Depth Data if needed\n",
    "        #    print(\"Implement this\") \n",
    "    return df\n",
    "    \n",
    "def hdf5_to_vcf(path_h5, path_vcf, iids=[], markers=[], chrom=0, pl_field=False):\n",
    "    \"\"\"Load HDF5 from path_h5, extract iids and\n",
    "    (if given) markers by position and save vcf to path_vcf.\n",
    "    pl: If True, also save Genotype Likelihoods!\n",
    "    iids: Which Individuals to match and save. If none given: Save all!\"\"\"\n",
    "    \n",
    "    f = load_h5(path=path_h5)\n",
    "    \n",
    "    if len(iids)==0:\n",
    "        iids = f[\"samples\"][:]\n",
    "        \n",
    "    if chrom==0:\n",
    "        chrom = f[\"variants/CHROM\"][:]\n",
    "        \n",
    "    pos = f[\"variants/POS\"][:]\n",
    "    ref = f[\"variants/REF\"][:] \n",
    "    alt = f[\"variants/ALT\"][:] \n",
    "    \n",
    "    idx = np.isin(f[\"samples\"], iids)\n",
    "    gt = f[\"calldata/GT\"][:,idx,:]\n",
    "    \n",
    "    ### Get Genotype Likelihoods from AD field\n",
    "    pl=[]  # Default\n",
    "    if pl_field:  \n",
    "        ad = f[\"calldata/AD\"][:]\n",
    "        gl = ad_to_genotypeL(ad)  # Convert Allele Depths to Genotype Likelihood\n",
    "        pl = gl_to_pl(gl)         # Convert Genotype Likelihood to PHRED scale \n",
    "        \n",
    "    to_vcf(chrom, pos, ref, alt, gt, iids, path_vcf, pl=pl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import binom  # Binomial Likelihood\n",
    "\n",
    "def ad_to_gentoypeL(ad, error=0.001):\n",
    "    \"\"\"Convert Allele Depth Fields to Genotype Likelihoods.\n",
    "    ad: [l,n,2] contains allele contains readcounts (integers)\n",
    "    error: Flip Error for Read\n",
    "    return: Genotype Probabilities (Pr(G|RC)) [l,n,3] for 00/01/11\"\"\"\n",
    "    rc_tot = np.sum(ad, axis=2)\n",
    "    rc_der = ad[:,:,1]\n",
    "\n",
    "    p_read = np.array([error, 0.5, 1-error])  # Probabilities for the 3 Genotypes\n",
    "    prob_binom = binom.pmf(rc_der[:, :, None], rc_tot[:, :, None], p_read[None, None, :])\n",
    "    return prob_binom\n",
    "\n",
    "def gl_to_pl(gl):\n",
    "    \"\"\"Convert Genotype Probabilities to normalized PHRED scores\n",
    "    gl: [l,n,3] Probabilities Pr(G|RC) (not logscale)\n",
    "    return: [l,n,3] vector\"\"\"\n",
    "    gl = -10 * np.log10(gl)  # Convert to PHRED scale\n",
    "    assert(np.min(gl)>=0)\n",
    "    pl = gl - np.min(gl, axis=2)[:,:,None] # Normalize\n",
    "    pl = np.round(pl).astype(\"int16\")  # Round to Integers\n",
    "    assert(np.min(pl)>=0) # Sanity Check\n",
    "    pl = np.clip(pl, a_min=0, a_max=99)  # Clip to 99    \n",
    "    return pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3 0]\n",
      "[9.97002999e-01 1.25000000e-01 1.00000000e-09]\n",
      "[ 0  9 90]\n"
     ]
    }
   ],
   "source": [
    "##################################################################\n",
    "### Example Case, test whether ad_to_genotypeL does what it should\n",
    "ad = np.array([[[1,0], [1,1]],  [[3,3], [3,0]], [[10,10], [0,10]]])\n",
    "\n",
    "gl = ad_to_gentoypeL(ad)\n",
    "pl = gl_to_pl(gl)\n",
    "\n",
    "i,j=1,1\n",
    "print(ad[i, j, :])\n",
    "print(gl[i, j, :])\n",
    "print(pl[i, j, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
