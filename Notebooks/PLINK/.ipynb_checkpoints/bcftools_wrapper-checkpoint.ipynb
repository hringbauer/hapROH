{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python wrapper for bcftools\n",
    "Contains additional functions to mimic output of HAPSBURG for downstream analysis\n",
    "@Harald Ringbauer, October 2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VioletQueen\n",
      "/home/harald/git/HAPSBURG\n",
      "CPU Count: 4\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import socket\n",
    "import os as os\n",
    "import sys as sys\n",
    "import multiprocessing as mp\n",
    "import h5py\n",
    "\n",
    "socket_name = socket.gethostname()\n",
    "print(socket_name)\n",
    "if socket_name == \"VioletQueen\":\n",
    "    path = \"/home/harald/git/HAPSBURG/\"   # The Path on Harald's machine\n",
    "elif socket_name.startswith(\"midway2\"):\n",
    "    print(\"Midway jnovmbre partition detected.\")\n",
    "    path = \"/project2/jnovembre/hringbauer/HAPSBURG/\"  # The Path on Midway Cluster\n",
    "else: \n",
    "    raise RuntimeWarning(\"Not compatible machine. Check!!\")\n",
    "    \n",
    "os.chdir(path)  # Set the right Path (in line with Atom default)\n",
    "\n",
    "sys.path.append(\"./PackagesSupport/h5_python/\")\n",
    "from h5_functions import hdf5_to_vcf, load_h5   # Import Function to convert hdf5 to vcf\n",
    "sys.path.append(\"./PackagesSupport/parallel_runs/\")\n",
    "from helper_functions import prepare_path, create_folders, postprocess_iid  # To split up ground truth ROH\n",
    "\n",
    "print(os.getcwd()) # Show the current working directory. Should be HAPSBURG/Notebooks/ParallelRuns\n",
    "print(f\"CPU Count: {mp.cpu_count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions to run VCF Tools for a single sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def post_process_bcftools(outfile):\n",
    "    \"\"\"Post Process bcftools output\"\"\"\n",
    "    outtxt = outfile + \".txt\"\n",
    "    outST = outfile + \"ST.txt\"\n",
    "    outRG = outfile + \"RG.txt\"\n",
    "    \n",
    "    !grep ^ST $outtxt > $outST\n",
    "    !grep ^RG $outtxt > $outRG\n",
    "    #!rm $outtxt # Clean up the now redundant original output\n",
    "    \n",
    "    df_pos = pd.read_csv(outST, sep='\\t', header=None, usecols=range(1,6))\n",
    "    df_pos.columns = [\"iid\", \"ch\", \"pos\", \"state\", \"qual\"]\n",
    "    df_rohs = pd.read_csv(outRG, sep='\\t', header=None, usecols=range(1,8))\n",
    "    df_rohs.columns = [\"iid\", \"ch\", \"Start\", \"End\", \"length\", \"markers\", \"qual\"]\n",
    "    return df_pos, df_rohs\n",
    "\n",
    "def run_bcftools_roh(vcf_file, outfile, mp=\"./Data/1000Genomes/Markers/rec_map_bcf.chr3.txt\",\n",
    "                    af=\"./Data/1000Genomes/Markers/af_1000G_EUR_bcf.chr3.txt.gz\", pl=False):\n",
    "    \"\"\"Run PLINK ROH Caller on path_vcf, and save results in outfile.txt.\n",
    "    Uses Map File mp and Allele Frequency File AF (prepared in prep_map_af_bcftools.ipynb)\n",
    "    Return 2 Dataframes (per site,  total roh blocks )\n",
    "    pl: Whether to use Genotype Likelihoods\"\"\"\n",
    "    outtxt = outfile + \".txt\"\n",
    "    if pl==False:\n",
    "        !bcftools roh -G30 --AF-file $af -m $mp $vcf_file > $outtxt\n",
    "        \n",
    "    elif pl==True:\n",
    "        !bcftools roh --AF-file $af -m $mp $vcf_file > $outtxt\n",
    "        \n",
    "    # -V 1e-10   ### Command to do Viterbi Training\n",
    "    \n",
    "def create_hapsburg_df(df_t, map_dct):\n",
    "    \"\"\"Modify bcftools output to HAPSBURG format.\n",
    "    Return right Dataframe\"\"\"\n",
    "    df_t[\"StartM\"] = df_t[\"Start\"].map(map_dct)\n",
    "    df_t[\"EndM\"] = df_t[\"End\"].map(map_dct)\n",
    "    df_t[\"lengthM\"] = df_t[\"EndM\"] - df_t[\"StartM\"]\n",
    "\n",
    "    # Add all fields for roh.csv\n",
    "    df_t = df_t[[\"Start\", \"End\", \"StartM\", \"EndM\", \"length\", \"lengthM\", \"iid\", \"ch\"]]\n",
    "    return df_t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick Single Example Run on one VCF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of target samples: 100\n",
      "Number of --estimate-AF samples: 0\n",
      "Number of sites in the buffer/overlap: unlimited\n",
      "Number of lines overlapping with --AF-file/processed: 77652/0\n",
      "Number of lines filtered/no AF/not biallelic/dup: 0/77651/0/1\n",
      "No usable sites were found.\n",
      "CPU times: user 14.9 ms, sys: 24.6 ms, total: 39.5 ms\n",
      "Wall time: 789 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "vcf_file = \"./Simulated/1000G_Mosaic/TSI5/ch3_6cm/data.vcf\"\n",
    "outfile  = \"./Diverse/nosync/bcfroh_out\"\n",
    "\n",
    "run_bcftools_roh(vcf_file, outfile, pl=False)\n",
    "#df_pos, df_rohs = post_process_bcftools(outfile)\n",
    "\n",
    "#df_rohs = create_hapsburg_df(df_rohs, map_dct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################\n",
    "### Combine all subfunctions\n",
    "\n",
    "def full_bcftools_mosaic(input_base_folder, ch=3, prefix_out=\"bcftools/\", convert_h5=True):\n",
    "    \"\"\"Run PLINK on Mosaic Data Set in ./Simulated\"\"\"\n",
    "    \n",
    "    print(\"Converting HDF5 into VCF...\")\n",
    "    input_h5, input_vcf, bcf_folder = create_folders(input_base_folder, outfolder=\"bcf_out/\")\n",
    "    if convert_h5:\n",
    "        hdf5_to_vcf(input_h5, input_vcf, chrom=ch) # Convert to VCF\n",
    "    \n",
    "    print(\"Running bcftools/ROH...\")\n",
    "    outfile = bcf_folder + \"bcfroh_out\"\n",
    "    run_bcftools_roh(input_vcf, outfile)   # Run BCF tools on VCF\n",
    "    df_pos, df_rohs = post_process_bcftools(outfile) # Load the output Data\n",
    "    \n",
    "    ### Create the Mapping Dictionary\n",
    "    print(\"Creating Map Dict...\")\n",
    "    f = load_h5(path=input_h5, output=False)\n",
    "    map_dct = dict(zip(f[\"variants/POS\"], f[\"variants/MAP\"]))\n",
    "    iids = f[\"samples\"][:] # Get the IIDs\n",
    "    \n",
    "    print(\"Splitting up BCF results and GT...\")\n",
    "    df_rohs = create_hapsburg_df(df_rohs, map_dct)\n",
    "    postprocess_iid(df_rohs, input_base_folder, iids, ch, prefix_out)\n",
    "    print(f\"Finished {len(iids)} Individuals!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run bcftools on all Individuals for Mosaic Folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting HDF5 into VCF...\n",
      "Running bcftools/ROH...\n",
      "Number of target samples: 100\n",
      "Number of --estimate-AF samples: 0\n",
      "Number of sites in the buffer/overlap: unlimited\n",
      "Number of lines total/processed: 77652/70453\n",
      "Creating Map Dict...\n",
      "Splitting up BCF results and GT...\n",
      "Finished 100 Individuals!\n",
      "CPU times: user 18.2 s, sys: 750 ms, total: 19 s\n",
      "Wall time: 28.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "full_bcftools_mosaic(input_base_folder = \"./Simulated/1000G_Mosaic/TSI5/ch3_6cm\",\n",
    "                     ch=3, prefix_out=\"bcftools/\", convert_h5=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run multiple lengths of copied in Chromosomes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path =  \"./Simulated/1000G_Mosaic/TSI5/\"\n",
    "\n",
    "for l in [0, 2, 4, 6, 8, 10]:\n",
    "    input_base_folder = base_path + f\"ch3_{l}cm\"\n",
    "    print(f\"\\nDoing ROH bcftools on {input_base_folder}...\")\n",
    "    full_bcftools_mosaic(input_base_folder, ch=3, prefix_out=\"bcftools/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split up the Posterior Output for Mosaic Folders\n",
    "Run to split map.csv and posterior0.csv into bcftools/ output folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def post_process_postbcf(basepath, map_dct):\n",
    "    \"\"\"Post Process the Posterior of bcftools\"\"\"\n",
    "    df_t = pd.read_csv(basepath + \"bcf_out/bcfroh_outST.txt\", header=None, sep=\"\\t\")\n",
    "    df_t.drop(columns=0, inplace=True) # Get rid of the first component\n",
    "    df_t.columns = [\"iid\", \"chr\", \"pos\", \"state\", \"post\"]\n",
    "\n",
    "    ### Transfrom it from PHRED scale to posterior\n",
    "    p = 10**(-df_t[\"post\"]/10)  # prob for alternative state\n",
    "    post = p * df_t[\"state\"] + (1-p) * (1 - df_t[\"state\"])   # 1 ROH State 0 HW\n",
    "    df_t[\"post\"] = np.log(post.values + 1e-10)\n",
    "    df_t[\"map\"] = df_t[\"pos\"].map(map_dct)\n",
    "    return df_t\n",
    "\n",
    "def split_up_bcftools_post(basepath, df_bcf, iid, ch, prefix_out=\"bcftools/\"):\n",
    "    \"\"\"Split up postprocessed Dataframe of bcftools/ROH output into\n",
    "    Mosaic folders. Save map and posterior file\"\"\"\n",
    "    output_base_folder = os.path.join(basepath, \"output/\")\n",
    "    pathout = prepare_path(output_base_folder, iid, ch=3, prefix_out=prefix_out, logfile=False)\n",
    "    df_t = df_bcf[df_bcf[\"iid\"] == iid]\n",
    "    \n",
    "    df_map = df_t[\"map\"]\n",
    "    mappath = os.path.join(pathout,\"map.csv\")\n",
    "    df_map.to_csv(mappath, sep=\",\", index=None, header=None)\n",
    "    \n",
    "    df_pos = df_t[\"post\"]\n",
    "    postpath = os.path.join(pathout,\"posterior0.csv\")\n",
    "    df_pos.to_csv(postpath, sep=\",\", index=None, header=None)\n",
    "    print(f\"Saved Posterior to {postpath}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Merge in Map(Takes about 10s)\n",
    "### Create the Mapping Dictionary (Run in Wrapper Function)\n",
    "input_h5 =  \"./Simulated/1000G_Mosaic/TSI5/ch3_6cm/data.h5\"\n",
    "\n",
    "print(\"Creating Map Dict...\")\n",
    "f = load_h5(path = input_h5, output=False)\n",
    "map_dct = dict(zip(f[\"variants/POS\"], f[\"variants/MAP\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run with Genotype Likelihoods (PL field)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick test on single file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of target samples: 100\n",
      "Number of --estimate-AF samples: 0\n",
      "Number of sites in the buffer/overlap: unlimited\n",
      "Number of lines overlapping with --AF-file/processed: 77650/0\n",
      "Number of lines filtered/no AF/not biallelic/dup: 0/77649/0/1\n",
      "No usable sites were found.\n",
      "CPU times: user 17.8 ms, sys: 32.2 ms, total: 50 ms\n",
      "Wall time: 1.42 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "path_vcf = \"./Simulated/1000G_Mosaic/TSI5/lambda_rc1.0/ch3_4cm/data.vcf\"\n",
    "outfile  = \"./Diverse/bcfroh_out\"\n",
    "\n",
    "run_bcftools_roh(path_vcf, outfile, pl=False)\n",
    "#df_pos, df_rohs = post_process_bcftools(outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Area 51\n",
    "Area to test code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split up the posterior from bcftools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert to VCF, storing the Genotype Likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded HDF5\n",
      "Loaded 77650 variants\n",
      "Loaded 100 individuals\n",
      "['AD', 'GT']\n",
      "['ALT', 'MAP', 'POS', 'REF']\n",
      "Calculating Genotype Likelihoods...\n",
      "Saving to VCF...\n",
      "Successfully saved VCF to ./Simulated/1000G_Mosaic/TSI5/lambda_rc1.0/ch3_4cm/data.vcf\n",
      "CPU times: user 1min 5s, sys: 4.83 s, total: 1min 10s\n",
      "Wall time: 1min 10s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "### Test for transformation to vcf with PL\n",
    "path_h5 = \"./Simulated/1000G_Mosaic/TSI5/lambda_rc1.0/ch3_4cm/data.h5\"\n",
    "path_vcf = \"./Simulated/1000G_Mosaic/TSI5/lambda_rc1.0/ch3_4cm/data.vcf\"\n",
    "\n",
    "hdf5_to_vcf(path_h5, path_vcf, iids=[], markers=[], chrom=3, pl_field=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded HDF5\n",
      "Loaded 77650 variants\n",
      "Loaded 100 individuals\n",
      "['AD', 'GT']\n",
      "['ALT', 'MAP', 'POS', 'REF']\n"
     ]
    }
   ],
   "source": [
    " f= load_h5(path_h5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0],\n",
       "       [1, 0],\n",
       "       [1, 1],\n",
       "       [0, 0],\n",
       "       [1, 1],\n",
       "       [0, 0],\n",
       "       [0, 0],\n",
       "       [1, 1],\n",
       "       [1, 1],\n",
       "       [0, 0]], dtype=int32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f[\"calldata/GT\"][0,:10,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0],\n",
       "       [0, 0],\n",
       "       [0, 2],\n",
       "       [1, 0],\n",
       "       [0, 0],\n",
       "       [1, 0],\n",
       "       [0, 0],\n",
       "       [0, 0],\n",
       "       [0, 0],\n",
       "       [1, 0]], dtype=int32)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f[\"calldata/AD\"][0,:10,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
