{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python wrapper for bcftools\n",
    "Contains additional functions to mimic output of HAPSBURG for downstream analysis\n",
    "@Harald Ringbauer, October 2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VioletQueen\n",
      "/home/harald/git/HAPSBURG\n",
      "CPU Count: 4\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import socket\n",
    "import os as os\n",
    "import sys as sys\n",
    "import multiprocessing as mp\n",
    "import h5py\n",
    "\n",
    "socket_name = socket.gethostname()\n",
    "print(socket_name)\n",
    "if socket_name == \"VioletQueen\":\n",
    "    path = \"/home/harald/git/HAPSBURG/\"   # The Path on Harald's machine\n",
    "elif socket_name.startswith(\"midway2\"):\n",
    "    print(\"Midway jnovmbre partition detected.\")\n",
    "    path = \"/project2/jnovembre/hringbauer/HAPSBURG/\"  # The Path on Midway Cluster\n",
    "else: \n",
    "    raise RuntimeWarning(\"Not compatible machine. Check!!\")\n",
    "    \n",
    "os.chdir(path)  # Set the right Path (in line with Atom default)\n",
    "\n",
    "sys.path.append(\"./PackagesSupport/h5_python/\")\n",
    "from h5_functions import hdf5_to_vcf, load_h5   # Import Function to convert hdf5 to vcf\n",
    "sys.path.append(\"./PackagesSupport/parallel_runs/\")\n",
    "from helper_functions import prepare_path, create_folders, postprocess_iid  # To split up ground truth ROH\n",
    "\n",
    "print(os.getcwd()) # Show the current working directory. Should be HAPSBURG/Notebooks/ParallelRuns\n",
    "print(f\"CPU Count: {mp.cpu_count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions to run VCF Tools for a single sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def post_process_bcftools(outfile):\n",
    "    \"\"\"Post Process bcftools output\"\"\"\n",
    "    outtxt = outfile + \".txt\"\n",
    "    outST = outfile + \"ST.txt\"\n",
    "    outRG = outfile + \"RG.txt\"\n",
    "    \n",
    "    !grep ^ST $outtxt > $outST\n",
    "    !grep ^RG $outtxt > $outRG\n",
    "    \n",
    "    df_pos = pd.read_csv(outST, sep='\\t', header=None, usecols=range(1,6))\n",
    "    df_pos.columns = [\"iid\", \"ch\", \"pos\", \"state\", \"qual\"]\n",
    "    df_rohs = pd.read_csv(outRG, sep='\\t', header=None, usecols=range(1,8))\n",
    "    df_rohs.columns = [\"iid\", \"ch\", \"Start\", \"End\", \"length\", \"markers\", \"qual\"]\n",
    "    return df_pos, df_rohs\n",
    "\n",
    "def run_bcftools_roh(vcf_file, outfile, mp=\"./Data/1000Genomes/Markers/rec_map_bcf.chr3.txt\",\n",
    "                    af=\"./Data/1000Genomes/Markers/af_1000G_EUR_bcf.chr3.txt.gz\"):\n",
    "    \"\"\"Run PLINK ROH Caller on path_vcf, and save results in outfile.txt.\n",
    "    Uses Map File mp and Allele Frequency File AF (prepared in prep_map_af_bcftools.ipynb)\n",
    "    Return 2 Dataframes (per site,  total roh blocks )\"\"\"\n",
    "    outtxt = outfile + \".txt\"\n",
    "    !bcftools roh -G30 --AF-dflt 0.3 -m $mp --AF-file $af $vcf_file > $outtxt\n",
    "    \n",
    "def create_hapsburg_df(df_t, map_dct):\n",
    "    \"\"\"Modify bcftools output to HAPSBURG format.\n",
    "    Return right Dataframe\"\"\"\n",
    "    df_t[\"StartM\"] = df_t[\"Start\"].map(map_dct)\n",
    "    df_t[\"EndM\"] = df_t[\"End\"].map(map_dct)\n",
    "    df_t[\"lengthM\"] = df_t[\"EndM\"] - df_t[\"StartM\"]\n",
    "\n",
    "    # Add all fields for roh.csv\n",
    "    df_t = df_t[[\"Start\", \"End\", \"StartM\", \"EndM\", \"length\", \"lengthM\", \"iid\", \"ch\"]]\n",
    "    return df_t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick Single Example Run on one VCF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of target samples: 100\n",
      "Number of --estimate-AF samples: 0\n",
      "Number of sites in the buffer/overlap: unlimited\n",
      "Number of lines total/processed: 77652/77651\n",
      "Number of lines filtered/no AF/not biallelic/dup: 0/0/0/1\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'map_dct' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'map_dct' is not defined"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "vcf_file = \"./Simulated/1000G_Mosaic/TSI5/ch3_6cm/data.vcf\"\n",
    "outfile  = \"./Diverse/bcfroh_out\"\n",
    "\n",
    "run_bcftools_roh(vcf_file, outfile)\n",
    "df_pos, df_rohs = post_process_bcftools(outfile)\n",
    "#df_rohs = create_hapsburg_df(df_rohs, map_dct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################\n",
    "### Combine all subfunctions\n",
    "\n",
    "def full_bcftools_mosaic(input_base_folder, ch=3, prefix_out=\"bcftools/\"):\n",
    "    \"\"\"Run PLINK on Mosaic Data Set in ./Simulated\"\"\"\n",
    "    \n",
    "    print(\"Converting HDF5 into VCF...\")\n",
    "    input_h5, input_vcf, bcf_folder = create_folders(input_base_folder, outfolder=\"bcf_out/\")\n",
    "    hdf5_to_vcf(input_h5, input_vcf, chrom=ch) # Convert to VCF\n",
    "    \n",
    "    print(\"Running bcftools/ROH...\")\n",
    "    outfile = bcf_folder + \"bcfroh_out\"\n",
    "    run_bcftools_roh(input_vcf, outfile)   # Run BCF tools on VCF\n",
    "    df_pos, df_rohs = post_process_bcftools(outfile) # Load the output Data\n",
    "    \n",
    "    ### Create the Mapping Dictionary\n",
    "    print(\"Creating Map Dict...\")\n",
    "    f = load_h5(path=input_h5, output=False)\n",
    "    map_dct = dict(zip(f[\"variants/POS\"], f[\"variants/MAP\"]))\n",
    "    iids = f[\"samples\"][:] # Get the IIDs\n",
    "    \n",
    "    print(\"Splitting up BCF results and GT...\")\n",
    "    df_rohs = create_hapsburg_df(df_rohs, map_dct)\n",
    "    postprocess_iid(df_rohs, input_base_folder, iids, ch, prefix_out)\n",
    "    print(f\"Finished {len(iids)} Individuals!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run bcftools on all Individuals for Mosaic Folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting HDF5 into VCF...\n",
      "Loaded HDF5\n",
      "Loaded 77652 variants\n",
      "Loaded 100 individuals\n",
      "['AD', 'GT']\n",
      "['ALT', 'MAP', 'POS', 'REF']\n",
      "Successfully saved VCF to ./Simulated/1000G_Mosaic/TSI5/ch3_6cm/data.vcf\n",
      "Running bcftools/ROH...\n",
      "Number of target samples: 100\n",
      "Number of --estimate-AF samples: 0\n",
      "Number of sites in the buffer/overlap: unlimited\n",
      "Number of lines overlapping with --AF-file/processed: 77652/77651\n",
      "Number of lines filtered/no AF/not biallelic/dup: 0/0/0/1\n",
      "Creating Map Dict...\n",
      "Splitting up BCF results and GT...\n",
      "Finished 100 Individuals!\n",
      "CPU times: user 36.7 s, sys: 1.52 s, total: 38.2 s\n",
      "Wall time: 46.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "full_bcftools_mosaic(input_base_folder = \"./Simulated/1000G_Mosaic/TSI5/ch3_6cm\",\n",
    "                     ch=3, prefix_out=\"bcftools/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Area 51"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
